Is the cure worse than the disease? A large-scale analysis of overfitting in automated program repair
Recent research in search-based automated program repair techniques has shown promise for reducing the significant manual effort required for debugging. This paper addresses a deficit of earlier evaluations of automated repair techniques caused by repairing programs and evaluating generated patchesâ€™ correctness using the same set of tests. Since tests are an imperfect metric of program correctness, evaluations of this type do not discriminate between correct patches, and patches that overfit the available tests and break untested but desired functionality. This paper evaluates two wellstudied repair tools, GenProg and TSPRepair, on a 956-bug dataset, each with a human-written patch. By evaluating patches on tests independent from those used during repair, we find that the tools are unlikely to improve the proportion of independent tests passed, and that the quality of the patches is proportional to the coverage of the test suite used during repair. For programs with fewer bugs, the tools are as likely to break tests as to fix them. In addition to overfitting, we measure the effects of test suite coverage, test suite provenance, starting program quality, and the difference in quality between novice-developer-written and tool-generated patches when quality is assessed with an independent test suite from patch generation. We have released the 956-bug dataset to allow future evaluations of new repair tools