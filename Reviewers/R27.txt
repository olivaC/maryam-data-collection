Mining Sandboxes
A technique to confine a computer program to computing resources accessed during automatic testing. Sandbox mining first explores software behavior by means of automatic test generation, and extracts the set of resources accessed during these tests. This set is then used as a sandbox, blocking access to resources not used during testing. The mined sandbox thus protects against behavior changes such as the activation of latent malware, infections, targeted attacks, or malicious updates. The use of test generation makes sandbox mining a fully automatic process that can be run by vendors and end users alike.
Wo ist der Fehler und wie wird er behoben? Ein Experiment mit Softwareentwicklern.
Dieser Beitrag ist eine gekürzte, deutsche Version unseres Artikels "Where is the Bug and How is it Fixed? An Experiment with Practitioners" publiziert in dem Berichtsband des elften gemeinsamen Treffens der European Software Engineering Conference und des ACM SIGSOFT Symposium on the Foundations of Software Engineering.
Practical Test Dependency Detection
—Regression tests should consistently produce the same outcome when executed against the same version of the system under test. Recent studies, however, show a different picture: in many cases simply changing the order in which tests execute is enough to produce different test outcomes. These studies also identify the presence of dependencies between tests as one likely cause of this behavior. Test dependencies affect the quality of tests and of the correlated development activities, like regression test selection, prioritization, and parallelization, which assume that tests are independent. Therefore, developers must promptly identify and resolve problematic test dependencies. This paper presents PRADET, a novel approach for detecting problematic dependencies that is both effective and efficient. PRADET uses a systematic, data-driven process to detect problematic test dependencies significantly faster and more precisely than prior work. PRADET scales to analyze large projects with thousands of tests that existing tools cannot analyze in reasonable amount of time, and found 27 previously unknown dependencies.
Testen mit automatisch bestimmten Eingabegrammatiken
Abstract not available.
Guest editorial: emerging areas in automated software engineering research
As everything becomes programmable, the significance of research on automation of software engineering tasks keeps increasing. This special section on Emerging Areas in Automated Software Engineering Research highlights the developments in automated debugging support and software analytics for incident management. The article titled “Lightweight Control-Flow Instrumentation and Postmortem Analysis in Support of Debugging” co-authored by Peter Ohmann and Ben Liblit presents automated techniques that assist programmers in debugging by providing information about program activity before failure. Authors show that latent information in postmortem core dumps can be augmented with lightweight, tunable runtime-tracing, resulting in significant slice reductions. Experimental evaluation provided by the authors indicate that presented techniques provide significant debugging support for programmers in realistic scenarios with low overhead. The article titled “Experience Report on Applying Software Analytics in Incident Management of Online Service” co-authored by Jian-Guang Lou, Qingwei Lin, Rui Ding, Qiang Fu, Dongmei Zhang, and Tao Xie focuses on application of data-driven techniques to large-scale online service incident management. The authors share their experiences in using software analytics to assist engineers in incident management, development of novel data-driven techniques for large-scale online services, and the lessons learned from research development and technology transfer in this area. Both articles demonstrate the significance of automation in software engineering and identify directions for future research. We hope that you enjoy reading these contributions.
Detecting information flow by mutating input data
Analyzing information flow is central in assessing the security of applications. However, static and dynamic analyses of information flow are easily challenged by non-available or obscure code. We present a lightweight mutation-based analysis that systematically mutates dynamic values returned by sensitive sources to assess whether the mutation changes the values passed to sensitive sinks. If so, we found a flow between source and sink. In contrast to existing techniques, mutation-based flow analysis does not attempt to identify the specific path of the flow and is thus resilient to obfuscation. In its evaluation, our MUTAFLOW prototype for Android programs showed that mutation-based flow analysis is a lightweight yet effective complement to existing tools. Compared to the popular FLOWDROID static analysis tool, MUTAFLOW requires less than 10% of source code lines but has similar accuracy; on 20 tested real-world apps, it is able to detect 75 flows that FLOWDROID misses.
Quantifying the information leak in cache attacks via symbolic execution
Cache timing attacks allow attackers to infer the properties of a secret execution by observing cache hits and misses. But how much information can actually leak through such attacks? For a given program, a cache model, and an input, our CHALICE framework leverages symbolic execution to compute the amount of information that can possibly leak through cache attacks. At the core of CHALICE is a novel approach to quantify information leak that can highlight critical cache side-channel leaks on arbitrary binary code. In our evaluation on real-world programs from OpenSSL and Linux GDK libraries, CHALICE effectively quantifies information leaks: For an AES-128 implementation on Linux, for instance, CHALICE finds that a cache attack can leak as much as 127 out of 128 bits of the encryption key.
Active Learning of Input Grammars
Abstract: Knowing the precise format of a program's input is a necessary prerequisite for systematic testing. Given a program and a small set of sample inputs, we (1) track the data flow of inputs to aggregate input fragments that share the same data flow through program execution into lexical and syntactic entities;(2) assign these entities names that are based on the associated variable and function identifiers; and (3) systematically generalize production rules by means of membership queries. As a result, we need only a minimal set of sample inputs to obtain human-readable context-free grammars that reflect valid input structure. In our evaluation on inputs like URLs, spreadsheets, or configuration files, our AUTOGRAM prototype obtains input grammars that are both accurate and very readable-and that can be directly fed into test generators for comprehensive automated testing.
Where is the bug and how is it fixed? an experiment with practitioners
Research has produced many approaches to automatically locate, explain, and repair software bugs. But do these approaches relate to the way practitioners actually locate, understand, and fix bugs? To help answer this question, we have collected a dataset named DBGBENCH --- the correct fault locations, bug diagnoses, and software patches of 27 real errors in open-source C projects that were consolidated from hundreds of debugging sessions of professional software engineers. Moreover, we shed light on the entire debugging process, from constructing a hypothesis to submitting a patch, and how debugging time, difficulty, and strategies vary across practitioners and types of errors. Most notably, DBGBENCH can serve as reality check for novel automated debugging and repair techniques.
Cut: Automatic unit testing in the cloud
Unit tests can be significantly sped up by running them in parallel over distributed execution environments, such as the cloud. However, manually setting up such environments and configuring the testing frameworks to effectively use them is cumbersome and requires specialized expertise that developers might lack. We present Cloud Unit Testing (CUT), a tool for automatically executing unit tests in distributed execution environments. Given a set of unit tests, CUT allocates appropriate computational resources, i.e., virtual machines or containers, and schedules the execution of tests over them. Developers do not need to change existing unit test code, and can easily control relevant aspects of test execution, including resource allocation and test scheduling. Additionally, during the execution CUT monitors and publishes events about the running tests which enables stream analytics. CUT and videos showcasing its main features are freely available at: https://www.st.cs.uni-saarland.de/testing/cut/
Search-based testing and system testing: a marriage in heaven
Software test generation can take place at the function level or the system level, and be driven by random, constraint-based, and/or search-based techniques. In this paper, we argue that the best way to generate tests is at the system level, as it avoids false failures due to implicit preconditions, and that for testing at the system level, search-based techniques are best suited as they avoid the combinatorial explosion of conditions due to long paths.
How Developers Debug Software—The DBGBENCH Dataset
How do professional software engineers debug computer programs? In an experiment with 27 real bugs that existed in several widely used programs, we invited 12 professional software engineers, who together spent one month on localizing, explaining, and fixing these bugs. This did not only allow us to study the various tools and strategies used to debug the same set of errors. We could also determine exactly which statements a developer would localize as faults, how a developer would diagnose and explain an error, and how a developer would fix an error - all of which software engineering researchers seek to automate. Until now, it has been difficult to evaluate the effectiveness and utility of automated debugging techniques without a user study. We publish the collected data, called DBGBENCH, to facilitate the effective evaluation of automated fault localization, diagnosis, and repair techniques w.r.t. the judgement of human experts.
Generating unit tests with structured system interactions
There is a large body of work in the literature about automatic unit tests generation, and many successful results have been reported so far. However, current approaches target library classes, but not full applications. A major obstacle for testing full applications is that they interact with the environment. For example, they establish connections to remote servers. Thoroughly testing such applications requires tests that completely control the interactions between the application and its environment. Recent techniques based on mocking enable the generation of tests which include environment interactions; however, generating the right type of interactions is still an open problem. In this paper, we describe a novel approach which addresses this problem by enhancing search-based testing with complex test data generation. Experiments on an artificial system show that the proposed approach can generate effective unit tests. Compared with current techniques based on mocking, we generate more robust unit tests which achieve higher coverage and are, arguably, easier to read and understand.
Mining input grammars with AUTOGRAM
Knowledge about how a program processes its inputs can help to understand the structure of the input as well as the structure of the program. In a JSON value like [1, true, "Alice"], for instance the integer value 1, the boolean value true and the string value "Alice" would be handled by different functions or stored in different variables. Our AUTOGRAM tool uses dynamic tainting to trace the data flow of each input character for a set of sample inputs and identifies syntactical entities by grouping input fragments that are handled by the same functions. The resulting context-free grammar reflects the structure of valid inputs and can be used for reverse engineering of formats and can serve as direct input for test generators. A video demonstrating AUTOGRAM is available at https://youtu.be/Iqym60iWBBk.
Detecting behavior anomalies in graphical user interfaces
When interacting with user interfaces, do users always get what they expect? For each user interface element in thousands of Android apps, we extracted the Android APIs they invoke as well as the text shown on their screen. This association allows us to detect outliers: User interface elements whose text, context or icon suggests one action, but which actually are tied to other actions. In our evaluation of tens of thousands of UI elements, our BACKSTAGE prototype discovered misleading random UI elements with an accuracy of 73%
Model-based testing of end-user collaboration intensive systems
Collaboration intensive systems like social networks support the interaction of multiple end-users playing different roles such as "friend" or "post owner". To ensure that end-users achieve the intended type of collaboration, systematic testing can be an effective means. However, manually creating effective test cases is cumbersome and error prone as the amount of end-users interactions to test grows exponentially with the number of involved end-users and roles. In this paper, we present a novel approach for automatic test case generation via modeling user collaborations as synchronized, non-deterministic Finite State Machines. As our preliminary evaluation shows, such collaboration models are effective and efficient: compared to collaboration-unaware alternatives, we generated test cases which achieve higher code coverage and trigger more errors, up to 10X faster.
O! Snap: Cost-Efficient Testing in the Cloud
Porting a testing environment to a cloud infrastructure is not straightforward. This paper presents O!Snap, an approach to generate test plans to cost-efficiently execute tests in the cloud. O!Snap automatically maximizes reuse of existing virtual machines, and interleaves the creation of updated test images with the execution of tests to minimize overall test execution time and/or cost. In an evaluation involving 2,600+ packages and 24,900+ test jobs of the Debian continuous integration environment, O!Snap reduces test setup time by up to 88% and test execution time by up to 43.3% without additional costs.
Mining Sandboxes for Security–Automatisches Sandboxing fü̈r Software-Sicherheit
We present sandbox mining, a technique to confine an application to resources accessed during automatic testing. Sandbox mining first explores software behavior by means of automatic test generation, and extracts the set of resources accessed during these tests. This set is then used as a sandbox, blocking access to resources not used during testing. The mined sandbox thus protects against behavior changes such as the activation of latent malware, infections, targeted attacks, or malicious updates. The use of test generation makes sandbox mining a fully automatic process that can be run by vendors and end users alike. Our BOXMATE prototype requires less than one hour to extract a sandbox from an Android app, with few to no confirmations required for frequently used functionality.
AppMining
A fundamental question of security analysis is: When is a behavior normal, and when is it not? We present techniques that extract behavior patterns from thousands of apps—patters that represent normal behavior, such as “A travel app normally does not access stored text messages”. Combining data flow analysis with app descriptions and GUI data from both apps and their stores allows for massive machine learning, which then also allows to detect yet unknown malware by classifying it as abnormal.
Quantifying the information leak in cache attacks through symbolic execution
Cache timing attacks allow attackers to infer the properties of a secret execution by observing cache hits and misses. But how much information can actually leak through such attacks? For a given program, a cache model, and an input, our CHALICE framework leverages symbolic execution to compute the amount of information that can possibly leak through cache attacks. At the core of CHALICE is a novel approach to quantify information leak that can highlight critical cache side-channel leaks on arbitrary binary code. In our evaluation on real-world programs from OpenSSL and Linux GDK libraries, CHALICE effectively quantifies information leaks: For an AES-128 implementation on Linux, for instance, CHALICE finds that a cache attack can leak as much as 127 out of 128 bits of the encryption key.
CALAPPA: a toolchain for mining Android applications
Software engineering researchers and practitioners working on the Android ecosystem frequently have to do the same tasks over and over: retrieve data from the Google Play store to analyze it, decompile the Dalvik bytecode to understand the behavior of the app, and analyze applications metadata and user reviews. In this paper we present CALAPPA, a highly reusable and customizable toolchain that allows researchers to easily run common analysis tasks on large Android application datasets. CALAPPA includes components to retrieve the data from different Android stores, and comes with a predefined, but extensible, set of modules that can analyze apps metadata and code.
Checking app user interfaces against app descriptions
Does the advertised behavior of apps correlate with what a user sees on a screen? In this paper, we introduce a technique to statically extract the text from the user interface definitions of an Android app. We use this technique to compare the natural language topics of an app’s user interface against the topics from its app store description. A mismatch indicates that some feature is exposed by the user interface, but is not present in the description, or vice versa. The popular Twitter app, for instance, spots UI elements that al- low to make purchases; however, this feature is not mentioned in its description. Likewise, we identified a number of apps whose user interface asks users to access or supply sensitive data; but this “feature” is not mentioned in the description. In the long run, analyzing user interface topics and comparing them against external descriptions opens the way for checking general mismatches between requirements and implementation.
The truth, the whole truth, and nothing but the truth: a pragmatic guide to assessing empirical evaluations
An unsound claim can misdirect a field, encouraging the pursuit of unworthy ideas and the abandonment of promising ideas. An inadequate description of a claim can make it difficult to reason about the claim, for example, to determine whether the claim is sound. Many practitioners will acknowledge the threat of unsound claims or inadequate descriptions of claims to their field. We believe that this situation is exacerbated, and even encouraged, by the lack of a systematic approach to exploring, exposing, and addressing the source of unsound claims and poor exposition. This article proposes a framework that identifies three sins of reasoning that lead to unsound claims and two sins of exposition that lead to poorly described claims and evaluations. Sins of exposition obfuscate the objective of determining whether or not a claim is sound, while sins of reasoning lead directly to unsound claims. Our framework provides practitioners with a principled way of critiquing the integrity of their own work and the work of others. We hope that this will help individuals conduct better science and encourage a cultural shift in our research community to identify and promulgate sound claims.
Mining input grammars from dynamic taints
Knowing which part of a program processes which parts of an input can reveal the structure of the input as well as the structure of the program. In a URL <pre>http://www.example.com/path/</pre>, for instance, the protocol <pre>http</pre>, the host <pre>www.example.com</pre>, and the path <pre>path</pre> would be handled by different functions and stored in different variables. Given a set of sample inputs, we use dynamic tainting to trace the data flow of each input character, and aggregate those input fragments that would be handled by the same function into lexical and syntactical entities. The result is a context-free grammar that reflects valid input structure. In its evaluation, our AUTOGRAM prototype automatically produced readable and structurally accurate grammars for inputs like URLs, spreadsheets or configuration files. The resulting grammars not only allow simple reverse engineering of input formats, but can also directly serve as input for test generators.
Automatisches Sicherheitstesten
Moderne Testgeneratoren finden Schwachstellen in Eingabeschnittstellen von Programmen, indem sie in Sekunden tausende Eingaben zufällig erzeugen. Die Werkzeuge lassen sich leicht von jedermann einsetzen–zum Angriff oder zur Verteidigung.
Predicting defects in code
A system is described herein that predicts defects in a portion of code of an application that is configured to execute on a computing device. Versions of code are analyzed to locate change bursts, which are alterations to at least one portion of code over time-related events. If a change burst is identified, defects are predicted with respect to the code based at least in part upon the identified change burst.
Droidmate: A robust and extensible test generator for android
DroidMate is a fully automated GUI execution generator for Android apps. DroidMate explores an app, i.e. (a) repeatedly reads at runtime the device GUI and monitored calls to Android APIs methods and (b) makes a decision what next GUI action (click, long-click, text entry, etc.) to execute, based on that data and provided exploration strategy. The process continues until some termination criterion is met.DroidMate is:- fully automatic: after it has been set up, the exploration itself does not require human presence. - extensible: without recompiling DroidMate, anybody can run it with their own exploration strategy, termination criterion or set of monitored methods.- robust: tested on 126 apps being in top 5 in all Google Play categories except Games, it ran successfully on 123 of them. - easy to setup: it works on Android devices and emulators out-of-the-box, without root or OS modifications. - easy to modify: documented sources, built and tested with continuous integration server, are publicly available.
Mining sandboxes
We present sandbox mining, a technique to confine an application to resources accessed during automatic testing. Sandbox mining first explores software behavior by means of automatic test generation, and extracts the set of resources accessed during these tests. This set is then used as a sandbox, blocking access to resources not used during testing. The mined sandbox thus protects against behavior changes such as the activation of latent malware, infections, targeted attacks, or malicious updates. The use of test generation makes sandbox mining a fully automatic process that can be run by vendors and end users alike. Our BOXMATE prototype requires less than one hour to extract a sandbox from an Android app, with few to no confirmations required for frequently used functionality.
Dagstuhl Reports, Vol. 5, Issue 11 ISSN 2192-5283
Abstract not available.
The impact of tangled code changes on defect prediction models
When interacting with source control management system, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing version histories, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source Java projects, we found between 7 % and 20 % of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6 % of all source files are incorrectly associated with bug reports. These incorrect bug file associations seem to not significantly impact models classifying source files to have at least one bug or no bugs. But our experiments show that untangling tangled code changes can result in more accurate regression bug prediction models when compared to models trained and tested on tangled bug datasets—in our experiments, the statistically significant accuracy improvements lies between 5 % and 200 %. We recommend better change organization to limit the impact of tangled changes.
Thread-level speculation with kernel support
Runtime systems for speculative parallelization can be substantially sped up by implementing them with kernel support. We describe a novel implementation of a thread-level speculation (TLS) system using virtual memory to isolate speculative state, implemented in a Linux kernel module. This design choice not only maximizes performance, but also allows to guarantee soundness in the presence of system calls, such as I/O. Its ability to maintain speedups even on programs with frequent mis-speculation, significantly extends its usability, for instance in speculative parallelization. We demonstrate the advantage of kernel-based TLS on a number of programs from the Cilk suite, where this approach is superior to the state of the art in each single case (7.28x on average). All systems described in this paper are made available as open source.
Abnormal Sensitive Data Usage in Android Apps
Abstract not available.
Mining android apps for anomalies
How do we know a program does what it claims to do? Our CHABADA prototype can cluster Android™ apps by their description topics and identify outliers in each cluster with respect to their API usage. A “weather” app that sends messages thus becomes an anomaly; likewise, a “messaging” app would typically not be expected to access the current location and would also be identified. In this paper we present a new approach for anomaly detection that improves the classification results of our original CHABADA paper [1]. Applied on a set of 22,500+ Android applications, our CHABADA prototype can now predict 74% of novel malware and as such, without requiring any known malware patterns, maintains a false positive rate close to 10%.
Programmers should still use slices when debugging
What is the best technique for fault localization? In a study of 37 real bugs (and 37 injected faults) in more than a dozen open source C programs, we compare the effectiveness of statistical debugging against dynamic slicing—the first study ever to compare the techniques. On average, dynamic slicing is more effective than statistical debugging, requiring programmers to examine only 14% (42 lines) of the code before finding the defect, less than half the effort required by statistical debugging (30% or 157 lines). Best results are obtained by a hybrid approach: If programmers first examine the top five most suspicious locations from statistical debugging, and then switch to dynamic slices, they will need to examine only 11% (35 lines) of the code.
Software verstehen, zerstören, schützen mit automatischen Software-Modellen
Modelle und Spezifikationen sind die Grundlage jeder vernünftigen SoftwareEntwicklung. Was aber, wenn man Modelle und Spezifikationen aus bestehenden Programmen ableiten könnte? Unsere aktuellen Arbeiten nehmen ein Programm und einige Beispieleingaben und leiten automatisch Grammatiken ab, die Eingabe- und Ausgabeformat beschreiben. So kann unser AUTOGRAM-System etwa für die JavaURL-Klasse aus den folgenden Beispiel-URLs http://user:password@www.google.com:80/command? foo=bar&lorem=ipsum#fragment http://www.guardian.co.uk/sports/worldcup#results ftp://bob:12345@ftp.example.com/oss/debian7.iso diese Eingabegrammatik ableiten, die recht anschaulich die Struktur der URL erfasst: URL ::= PROTOCOL ’://’ AUTHORITY PATH [’?’ QUERY] [’#’ REF] AUTHORITY ::= [USERINFO ’@’] HOST [’:’ PORT] PROTOCOL ::= ’http’ | ’ftp’ USERINFO ::= /[a-z]+/ ’:’ /[a-z]+/ HOST ::= /[a-z.]+/ PORT ::= ’80’• PATH ::= /\/[a-z0-9.]*/• QUERY ::= ’foo=bar&lorem=ipsum’ REF ::= /[a-z]+/ Wir erzeugen diese Grammatiken, indem wir den dynamischen Datenfluss eines jeden Zeichens verfolgen, und Zeichen, die in die gleiche Funktion oder Variable fließen, zu syntaktischen Einheiten zusammenfassen – die wir auch nach dem Funktions- oder Variablennamen benennen können. Da etwa „http“ und „ftp“ in eine Variable namens „protocol“ fließen, können wir direkt die entsprechende Regel ableiten. Dies sorgt für sehr lesbare Grammatiken, wie die obige unbearbeitete Rohausgabe unseres AUTOGRAM-Werkzeugs demonstriert. Grammatiken wie diese helfen unmittelbar, Datenformate zu verstehen. Sie ermöglichen vollautomatisches massives Sicherheitstesten auf einer Vielzahl von Systemen, indem sie die Produktion syntaktisch korrekter Eingaben unterstützen. Diese können so tief ins Programm vordringen, um dort Fehler zu finden – insbesondere Sicherheitslücken. Da 1 Universität des Saarlandes, Saarbrücken, Lehrstuhl für Softwaretechnik, zeller@cs.uni-saarland.de 42 Andreas Zeller sowohl das Lernen der Grammatik wie auch ihre Anwendung in der Testgenerierung vollautomatisch sind, ergeben sich hier viele neue Möglichkeiten – für Entwickler wie für Angreifer. Auf der anderen Seite ermöglichen erlernte Grammatiken aber auch Schutz vor illegalen Eingaben, indem nicht konforme Ein- oder Ausgaben abgeblockt werden – Techniken, die ich anhand konkreter Beispiele demonstriere. Besuchen Sie unsere Webseite, um mehr über unsere aktuellen Projekte zu erfahren: http://www.st.cs.uni-saarland.de/ Andreas Zeller ist seit 2001 Professor für Softwaretechnik an der Universität des Saarlandes in Saarbrücken. Seine Forschung beschäftigt sich mit der Analyse großer Software-Systeme und ihre Entwicklungsgeschichte. In 2010 wurde Zeller zum Fellow der ACM ernannt für seine Beiträge zur automatischen Fehlersuche und der Analyse von Software-Archiven, für die er auch jeweils mit einem 10-Jahres-Impact Award der ACM SIGSOFT und der ICSE ausgezeichnet wurde. Bereits 2012 hatten die Saarbrücker Holler, Herzig und Zeller das LANGFUZZ-System entwickelt, das Grammatik-basiert syntaktisch gültige Javascript-Eingaben für Webbrowser erzeugt. Bis heute hat LANGFUZZ als Teil der Firefox-Entwicklung mehr als 4000 Sicherheitslücken in Firefox aufgedeckt. Die aktuellen Arbeiten sind mit Matthias Höschele entstanden und werden aus einem ERC Advanced Grant für Arbeiten über Specification Mining und Testerzeugung gefördert.
Mining apps for anomalies
Does a program do what it is supposed to do? Once answer to this question could come from app mining – that is, the knowledge encoded into the hundreds of thousands of apps available in app stores. App mining can help to determine what would be normal and abnormal behavior, and thus guide programmers and users toward better security and usability.
Artifact Evaluation for Publications (Dagstuhl Perspectives Workshop 15452)
This report documents the program and the outcomes of Dagstuhl Perspectives Workshop 15452 "Artifact Evaluation for Publications". This Perspectives Workshop conveyed several stakeholders in artifact evaluation from different communities to assess how artifact evaluation is working and make recommendations to the computer systems research community about several issues with the process.
Mining workflow models from web applications
Modern business applications predominantly rely on web technology, enabling software vendors to efficiently provide them as a service, removing some of the complexity of the traditional release and update process. While this facilitates shorter, more efficient and frequent release cycles, it requires continuous testing. Having insight into application behavior through explicit models can largely support development, testing and maintenance. Model-based testing allows efficient test creation based on a description of the states the application can be in and the transitions between these states. As specifying behavior models that are precise enough to be executable by a test automation tool is a hard task, an alternative is to extract them from running applications. However, mining such models is a challenge, in particular because one needs to know when two states are equivalent, as well as how to reach that state. We present Process Crawler (ProCrawl), a tool to mine behavior models from web applications that support multi-user workflows. ProCrawl incrementally learns a model by generating program runs and observing the application behavior through the user interface. In our evaluation on several real-world web applications, ProCrawl extracted models that concisely describe the implemented workflows and can be directly used for model-based testing.
Inferring loop invariants by mutation, dynamic analysis, and static checking
Verifiers that can prove programs correct against their full functional specification require, for programs with loops, additional annotations in the form of loop invariants-properties that hold for every iteration of a loop. We show that significant loop invariant candidates can be generated by systematically mutating postconditions; then, dynamic checking (based on automatically generated tests) weeds out invalid candidates, and static checking selects provably valid ones. We present a framework that automatically applies these techniques to support a program prover, paving the way for fully automatic verification without manually written loop invariants: Applied to 28 methods (including 39 different loops) from various java.util classes (occasionally modified to avoid using Java features not fully supported by the static checker), our DYNAMATE prototype automatically discharged 97 percent of all proof obligations, resulting in automatic complete correctness proofs of 25 out of the 28 methods-outperforming several state-of-the-art tools for fully automatic verification.
Test complement exclusion: Guarantees from dynamic analysis
Modern test generation techniques allow to generate as many executions as needed, combined with dynamic analysis, they allow for understanding program behavior in situations where static analysis is challenged or impossible. However, all these dynamic techniques would still suffer from the incompleteness of testing: If some behavior has not been observed so far, there is no guarantee that it may not occur in the future. In this talk, I introduce a method called Test Complement Exclusion that combines test generation and sand boxing to provide such a guarantee. Test Complement Exclusion will have significant impact in the security domain, as it effectively detects and protects against unexpected changes of program behavior, however, guarantees would also strengthen findings in dynamic software comprehension. First experiments on real-world ANDROID programs demonstrate the feasibility of the approach.
Using dynamic symbolic execution to generate inputs in search-based GUI testing
Search-based testing has been successfully applied to generate complex sequences of events for graphical user interfaces (GUIs), but typically relies on simple heuristics or random values for data widgets like text boxes. This may greatly reduce the effectiveness of test generation for applications which expect specific input values to be entered in their GUI by users. Generating such specific input values is one of the virtues of dynamic symbolic execution (DSE), but DSE is less suitable to generate sequences of events. Therefore, this paper describes a hybrid approach that uses search-based testing to generate sequences of events, and DSE to build input data for text boxes. This is achieved by replacing standard widgets in a system under test with symbolic ones, allowing us to execute GUIs symbolically. In this paper, we demonstrate an extension of the search-based GUI testing tool EXSYST, which uses DSE to successfully increase the obtained code coverage on two case study applications.
Mining apps for abnormal usage of sensitive data
What is it that makes an app malicious? One important factor is that malicious apps treat sensitive data differently from benign apps. To capture such differences, we mined 2,866 benign Android applications for their data flow from sensitive sources, and compare these flows against those found in malicious apps. We find that (a) for every sensitive source, the data ends up in a small number of typical sinks; (b) these sinks differ considerably between benign and malicious apps; (c) these differences can be used to flag malicious apps due to their abnormal data flow; and (d) malicious apps can be identified by their abnormal data flow alone, without requiring known malware samples. In our evaluation, our mudflow prototype correctly identified 86.4% of all novel malware, and 90.1% of novel malware leaking sensitive data.
Generalized task parallelism
Existing approaches to automatic parallelization produce good results in specific domains. Yet, it is unclear how to integrate their individual strengths to match the demands and opportunities of complex software. This lack of integration has both practical reasons, as integrating those largely differing approaches into one compiler would impose an engineering hell, as well as theoretical reasons, as no joint cost model exists that would drive the choice between parallelization methods. By reducing the problem of generating parallel code from a program dependence graph to integer linear programming, <i>generalized task parallelization</i> integrates central aspects of existing parallelization approaches into a single unified framework. Implemented on top of LLVM, the framework seamlessly integrates enabling technologies such as speculation, privatization, and the realization of reductions. Evaluating our implementation on various C programs from different domains, we demonstrate the effectiveness and generality of generalized task parallelization. On a quad-core machine with hyperthreading we achieve speedups of up to 4.6 ×.
It's Not a Bug, It's a Feature: How Misclassification Impacts Bug Prediction
In a manual examination of more than 7,000 issue reports from the bug databases of five open-source projects, we found 33.8% of all bug reports to be misclassified---that is, rather than referring to a code fix, they resulted in a new feature, an update to documentation, or an internal refactoring. This misclassification introduces bias in bug prediction models, confusing bugs and features: On average, 39% of files marked as defective actually never had a bug. We discuss the impact of this misclassification on earlier studies and recommend manual data validation for future studies.
Dynamate: Dynamically inferring loop invariants for automatic full functional verification
DYNAMATE is a tool that automatically infers loop invariants and uses them to prove Java programs correct with respect to a given JML functional specification. DYNAMATE improves the flexibility of loop invariant inference by integrating static (proving) and dynamic (testing) techniques with the goal of combining their complementary strengths. In an experimental evaluation involving 26 Java methods of java.util annotated with JML pre- and postconditions, it automatically discharged over 97% of all proof obligations, resulting in automatic complete correctness proofs of 23 out of the 26 methods.
XMLMate: evolutionary XML test generation
Generating system inputs satisfying complex constraints is still a challenge for modern test generators. We present XMLMATE, a search-based test generator specially aimed at XML-based systems. XMLMATE leverages program structure, existing XML schemas, and XML inputs to generate, mutate, recombine, and evolve valid XML inputs. Over a set of seven XML-based systems, XMLMATE detected 31 new unique failures in production code, all triggered by system inputs and thus true alarms.
Procrawl: mining test models from multi-user web applications
Today's web applications demand very high release cycles--and consequently, frequent tests. Automating these tests typically requires a behavior model: A description of the states the application can be in, the transitions between these states, and the expected results. Furthermore one needs scripts to make the abstract actions (transitions) in the model executable. However, specifying such behavior models and writing the necessary scripts manually is a hard task. We present ProCrawl (Process Crawler), a tool that automatically mines (extended) finite-state machines from (multi-user) web applications and generates executable test scripts. ProCrawl explores the behavior of the application by systematically generating program runs and observing changes on the application's user interface. The resulting models can be directly used for effective model-based testing, in particular regression testing.
JTACO: Test Execution for Faster Bounded Verification.
This book constitutes the refereed proceedings of the 11th International Conference on Tests and Proofs, TAP 2017, held as part of STAF 2017, in Marburg, Germany, in July 2017. The 9 full papers and 1 invited paper presented in this volume were carefully reviewed and selected from 16 submissions. The TAP conference promotes research in verification and formal methods that targets the interplay of proofs and testing: the advancement of techniques of each kind and their combination, with the ultimate goal of improving software and system dependability.
Test generation across multiple layers
Complex software systems frequently come in many layers, each realized in a different programming language. This is a challenge for test generation, as the semantics of each layer have to be determined and integrated. An automatic test generator for Java, for instance, is typically unable to deal with the internals of lower level code (such as C-code), which results in lower coverage and fewer test cases of interest. In this paper, we sketch a novel approach to help search-based test generators for Java to achieve better coverage of underlying native code layers. The key idea is to apply test generation to the native layer first, and then to use the inputs to the native test cases as targets for search-based testing of the higher Java layers. We demonstrate our approach on a case study combining KLEE and EVOSUITE.
Search-based security testing of web applications
SQL injections are still the most exploited web application vulnerabilities. We present a technique to automatically detect such vulnerabilities through targeted test generation. Our approach uses search-based testing to systematically evolve inputs to maximize their potential to expose vulnerabilities. Starting from an entry URL, our BIOFUZZ prototype systematically crawls a web application and generates inputs whose effects on the SQL interaction are assessed at the interface between Web server and database. By evolving those inputs whose resulting SQL interactions show best potential, BIOFUZZ exposes vulnerabilities on real-world Web applications within minutes. As a black-box approach, BIOFUZZ requires neither analysis nor instrumentation of server code; however, it even outperforms state-of-the-art white-box vulnerability scanners.
Extrinsic influence factors in software reliability: A study of 200,000 windows machines
Reliability of software depends not only on intrinsic factors such as its code properties, but also on extrinsic factors—that is, the properties of the environment it operates in. In an empirical study of more than 200,000 Windows users, we found that the reliability of individual applications is related to whether and which other applications are in-stalled: While games and file-sharing applications tend to decrease the reliability of other applications, security applications tend to increase it. Furthermore, application reliability is related to the usage profiles of these applications; generally, the more an application is used, the more likely it is to have negative impact on reliability of others. As a conse-quence, software testers must be careful to investigate and control these factors.
Checking app behavior against app descriptions
How do we know a program does what it claims to do? After clustering Android apps by their description topics, we identify outliers in each cluster with respect to their API usage. A "weather" app that sends messages thus becomes an anomaly; likewise, a "messaging" app would typically not be expected to access the current location. Applied on a set of 22,500+ Android applications, our CHABADA prototype identified several anomalies; additionally, it flagged 56% of novel malware as such, without requiring any known malware patterns.
Automated fixing of programs with contracts
This paper describes AutoFix, an automatic debugging technique that can fix faults in general-purpose software. To provide high-quality fix suggestions and to enable automation of the whole debugging process, AutoFix relies on the presence of simple specification elements in the form of contracts (such as pre- and postconditions). Using contracts enhances the precision of dynamic analysis techniques for fault detection and localization, and for validating fixes. The only required user input to the AutoFix supporting tool is then a faulty program annotated with contracts; the tool produces a collection of validated fixes for the fault ranked according to an estimate of their suitability. In an extensive experimental evaluation, we applied AutoFix to over 200 faults in four code bases of different maturity and quality (of implementation and of contracts). AutoFix successfully fixed 42 percent of the faults, producing, in the majority of cases, corrections of quality comparable to those competent programmers would write; the used computational resources were modest, with an average time per fix below 20 minutes on commodity hardware. These figures compare favorably to the state of the art in automated program fixing, and demonstrate that the AutoFix approach is successfully applicable to reduce the debugging burden in real-world scenarios.
WebMate: Web application test generation in the real world
We present Web Mate, a tool for automatically generating test cases for Web applications. Given only the URL of the starting page, Web Mate automatically explores the functionality of a Web application, detecting differences across multiple browsers or operating systems, as well as across different revisions of the same Web application. Web Mate can handle full Web 2.0 functionality and explore sites as complex as Facebook. In addition to autonomously exploring the application, Web Mate can also leverage existing written or recorded test cases, and use these as an exploration base, this combination allows for quick expansion of the existing test base. Originating from research in generating test cases for specification mining, Web Mate is now the core product of a startup specializing in automated Web testing - a transfer that took us two years to complete. We report central lessons learned from this transfer, reflecting how robust, versatile, and pragmatic an innovative tool must be to be a success in the marketplace.
Inter-Application Communication Testing of Android Applications Using Intent Fuzzing
Testing is a crucial stage in the software development process that is used to uncover bugs and potential security threats. If not conducted thoroughly, buggy software may cause erroneous, malicious and even harmful behavior. Unfortunately in most software systems, testing is either completely neglected or not thoroughly conducted. One such example is Google's popular mobile platform, Android OS, where inter-application communication is not properly tested. This is because of the difficulty which it possesses in the development overhead and the manual labour required by developers in setting up the testing environment. Consequently, the lack of Android application testing continues to cause Android users to experience erroneous behavior and sudden crashes, impacting user experience and potentially resulting in financial losses. When a caller application attempts to communicate with a potentially buggy application, the caller application will suffer functional errors or it may even potentially crash. Incidentally, the user will complain that the caller application is not providing the promised functionality, resulting in a devaluation of the application's user rating. Successive failures will no longer be considered as isolated events, potentially crippling developer credibility of the calling application. In this thesis we present an automated tester for inter-application communication in Android applications. The approach used for testing is called Intent based Testing. Android applications are typically divided into multiple components that communicate via intents: messages passed through Android OS to coordinate operations between the different components. Intents are also used for inter-application communication, rendering them relevant for security. In this work, we designed and built a fully automated tool called IntentFuzzer, to test the stability of inter-application communication of Android applications using intents. Firstly, it statically analyzes the application to generate intents. Next, it tests the inter-application communication by fuzzing them, that is, injecting random input values that uncover unwanted behavior. In this way, we are able to expose several new defects including potential security issues which we discuss briefly in the Evaluation section.
Loop Invariants by Mutation, Dynamic Validation, and Static Checking
Verifiers that can prove programs correct against their full functional specification require, for programs with loops, additional annotations in the form of loop invariants—properties that hold for every iteration of a loop. We show that significant loop invariant candidates can be generated by systematically mutating postconditions; then, dynamic checking (based on automatically generated tests) weeds out invalid candidates, and static checking selects provably valid ones. We present a framework that automatically applies these techniques to support a program prover, paving the way for fully automatic verification without manually written loop invariants: Applied to 28 methods (including 39 different loops) from various java.util classes (occasionally modified to avoid using Java features not fully supported by the static checker), our DYNAMATE prototype automatically discharged 97% of all proof obligations, resulting in automatic complete correctness proofs of 25 out of the 28 methods—outperforming several state-of-the-art tools for fully automatic verification.
Automating full functional verification of programs with loops
Verifiers that can prove programs correct against their full functional specification require, for programs with loops, additional annotations in the form of loop invariants—properties that hold for every iteration of a loop. We show that significant loop invariant candidates can be generated by systematically mutating postconditions; then, dynamic checking (based on automatically generated tests) weeds out invalid candidates, and static checking selects provably valid ones. We present a framework that automatically applies these techniques to support a program prover, paving the way for fully automatic verification without manually written loop invariants: Applied to 28 methods (including 39 different loops) from various java.util classes, our DYNAMATE prototype automatically discharged 97% of all proof obligations, resulting in automatic complete correctness proofs of 25 out of the 28 methods—outperforming several state-of-the-art tools for fully automatic verification.
Mining bug data
Although software systems control many aspects of our daily life world, no system is perfect. Many of our day-to-day experiences with computer programs are related to software bugs. Although software bugs are very unpopular, empirical software engineers and software repository analysts rely on bugs or at least on those bugs that get reported to issue management systems. So what makes data software repository analysts appreciate bug reports? Bug reports are development artifacts that relate to code quality and thus allow us to reason about code quality, and quality is key to reliability, end-users, success, and finally profit. This chapter serves as a hand-on tutorial on how to mine bug reports, relate them to source code, and use the knowledge of bug fix locations to model, estimate, or even predict source code quality. This chapter also discusses risks that should be addressed before one can achieve reliable recommendation systems.
28th IEEE/ACM International Conference on Automated Software Engineering (ASE)
Abstract:
Conference proceedings front matter may contain various advertisements, welcome messages, committee or program information, and other miscellaneous conference information. This may in some cases also include the cover art, table of contents, copyright statements, title-page or half title-pages, blank pages, venue maps or other general information relating to the conference that was part of the original conference proceedings.
Predicting defects using change genealogies
When analyzing version histories, researchers traditionally focused on single events: e.g. the change that causes a bug, the fix that resolves an issue. Sometimes however, there are indirect effects that count: Changing a module may lead to plenty of follow-up modifications in other places, making the initial change having an impact on those later changes. To this end, we group changes into change genealogies, graphs of changes reflecting their mutual dependencies and influences and develop new metrics to capture the spatial and temporal influence of changes. In this paper, we show that change genealogies offer good classification models when identifying defective source files: With a median precision of 73% and a median recall of 76%, change genealogy defect prediction models not only show better classification accuracies as models based on code complexity, but can also outperform classification models based on code dependency network metrics.
Where should we fix this bug? a two-phase recommendation model
To support developers in debugging and locating bugs, we propose a two-phase prediction model that uses bug reports' contents to suggest the files likely to be fixed. In the first phase, our model checks whether the given bug report contains sufficient information for prediction. If so, the model proceeds to predict files to be fixed, based on the content of the bug report. In other words, our two-phase model "speaks up" only if it is confident of making a suggestion for the given bug report; otherwise, it remains silent. In the evaluation on the Mozilla "Firefox" and "Core" packages, the two-phase model was able to make predictions for almost half of all bug reports; on average, 70 percent of these predictions pointed to the correct files. In addition, we compared the two-phase model with three other prediction models: the Usual Suspects, the one-phase model, and BugScout. The two-phase model manifests the best prediction performance.
Checked coverage: an indicator for oracle quality
A known problem of traditional coverage metrics is that they do not assess oracle quality—that is, whether the computation result is actually checked against expectations. In this paper, we introduce the concept of checked coverage—the dynamic slice of covered statements that actually influence an oracle. Our experiments on seven open‐source projects show that checked coverage is a sure indicator for oracle quality and even more sensitive than mutation testing. Copyright © 2013 John Wiley & Sons, Ltd.
Mining behavior models from enterprise web applications
Today's enterprise web applications demand very high release cycles---and consequently, frequent tests. Automating these tests typically requires a behavior model: A description of the states the application can be in, the transitions between these states, and the expected results. Furthermore one needs scripts to make the abstract actions (transitions) in the model executable. As specifying such behavior models and writing the necessary scripts manually is a hard task, a possible alternative could be to extract them from existing applications. However, mining such models can be a challenge, in particular because one needs to know when two states are equivalent, as well as how to reach that state. We present ProCrawl (PROcess CRAWLer), a generic approach to mine behavior models from (multi-user) enterprise web applications. ProCrawl observes the behavior of the application through its user interface, generates and executes tests to explore unobserved behavior. In our evaluation of three non-trivial web applications (an open-source shop system, an SAP product compliance application, and an open-source conference manager), ProCrawl produces models that precisely abstract application behavior and which can be directly used for effective model-based regression testing.
Covering and uncovering equivalent mutants
Mutation testing measures the adequacy of a test suite by seeding artificial defects (mutations) into a program. If a test suite fails to detect a mutation, it may also fail to detect real defects—and hence should be improved. However, there are also mutations that keep the program semantics unchanged and thus cannot be detected by any test suite. Such equivalent mutants must be weeded out manually, which is a tedious task. In this paper, we examine whether changes in coverage can be used to detect non‐equivalent mutants: If a mutant changes the coverage of a run, it is more likely to be non‐equivalent. In a sample of 140 manually classified mutations of seven Java programs with 5000 to 100 000 lines of code, we found that (i) the problem is serious and widespread—about 45% of all undetected mutants turned out to be equivalent; (ii) manual classification takes time—about 15 min per mutation; (iii) coverage is a simple, efficient and effective means to identify equivalent mutants—with a classification precision of 75% and a recall of 56%; and (iv) coverage as an equivalence detector is superior to the state of the art, in particular violations of dynamic invariants. Our detectors have been released as part of the open‐source JAVALANCHE framework; the data set is publicly available for replication and extension of experiments. Copyright © 2012 John Wiley & Sons, Ltd.
3.9 Males and Females Debugging: Are the Tools Getting in the Way?
No abstract available.
Dagstuhl Reports, Vol. 3, Issue 2 ISSN 2192-5283
No abstract available.
Machine Learning and Testing
This report documents the program and the outcomes of Dagstuhl Seminar 13021 “Symbolic Methods in Testing”. The aim of the seminar was to bring together leading researchers of this field; the seminary ended up with 38 participants from 10 countries: France, The Netherlands, The Unites States, Germany, Switzerland, United Kingdom, Brazil, Norway, Estonia and Italy. Through a series of presentations, discussions, and working group meetings, the seminar attempted to get a coherent picture of the field, which transcends the borders of applications and disciplines, of existing approaches and problems in formal testing. The seminar brought together, on the one hand, researchers from the different camps and various tools. The main outcome of the seminar is the exchange of information between different groups and the discussion of new trends (parallelization, cloud-computing).
The impact of tangled code changes
When interacting with version control systems, developers often commit unrelated or loosely related code changes in a single transaction. When analyzing the version history, such tangled changes will make all changes to all modules appear related, possibly compromising the resulting analyses through noise and bias. In an investigation of five open-source JAVA projects, we found up to 15% of all bug fixes to consist of multiple tangled changes. Using a multi-predictor approach to untangle changes, we show that on average at least 16.6% of all source files are incorrectly associated with bug reports. We recommend better change organization to limit the impact of tangled changes.
It's not a bug, it's a feature: how misclassification impacts bug prediction
In a manual examination of more than 7,000 issue reports from the bug databases of five open-source projects, we found 33.8% of all bug reports to be misclassified---that is, rather than referring to a code fix, they resulted in a new feature, an update to documentation, or an internal refactoring. This misclassification introduces bias in bug prediction models, confusing bugs and features: On average, 39% of files marked as defective actually never had a bug. We discuss the impact of this misclassification on earlier studies and recommend manual data validation for future studies.
Reconstructing core dumps
When a software failure occurs in the field, it is often difficult to reproduce. Guided by a memory dump at the moment of failure (a “core dump”), our RECORE test case generator searches for a series of events that precisely reconstruct the failure from primitive data. Applied on seven non-trivial Java bugs, RECORE reconstructs the exact failure in five cases without any runtime overhead in production code.
Wissensmanagement: Schritte zum intelligenten Unternehmen
No abstract available.
Sambamba: runtime adaptive parallel execution
How can we exploit a microprocessor as efficiently as possible? The "classic" approach is static optimization at compile-time, conservatively optimizing a program while keeping all possible uses in mind. Further optimization can only be achieved by anticipating the actual usage profile: If we know, for instance, that two computations will be independent, we can run them in parallel. However, brute force parallelization may slow down execution due to its large overhead. But as this depends on runtime features, such as structure and size of input data, parallel execution needs to dynamically adapt to the runtime situation at hand. Our SAMBAMBA framework implements such a dynamic adaptation for regular sequential C programs through adaptive dispatch between sequential and parallel function instances. In an evaluation of 14 programs, we show that automatic parallelization in combination with adaptive dispatch can lead to speed-ups of up to 5.2 fold on a quad-core machine with hyperthreading. At this point, we rely on programmer annotations but will get rid of this requirement as the platform evolves to support efficient speculative optimizations.
WebMate: Generating test cases for web 2.0
Web applications are everywhere—well tested web applications however are in short supply. The mixture of JavaScript, HTML and CSS in a variety of different browsers makes it virtually impossible to apply static analysis techniques. In this setting, systematic testing becomes a real challenge. We present a technique to automatically generate tests for Web 2.0 applications. Our approach systematically explores and tests all distinct functions of a web application. Our prototype implementation WEBMATE handles interfaces as complex as Facebook and is able to cover up to 7 times as much code as existing tools. The only requirements to use WEBMATE are the address of the application and, if necessary, user name and password.
Entwicklung sicherer Software durch Security by Design
Dieser Trend- und Strategiebericht vertritt die These, dass die Entwicklung und Integration sicherer Software nach dem Prinzip Security by Design ausgestaltet werden muss und benennt entsprechende Herausforderungen für eine praxisorientierte Forschungsagenda. Software ist heute wie auch zukünftig der wichtigste Treiber von Innovationen in vielen Anwendungsbereichen und Branchen. Viele Schwachstellen und Angriffe lassen sich auf Sicherheitslücken in Anwendungssoftware zurückführen. Sicherheitsfragen werden bei der heutigen Entwicklung oder Integration von Anwendungssoftware entweder überhaupt nicht oder nur unzureichend betrachtet, so dass durch Anwendungssoftware immer wieder neue Ansatzpunkte für Angriffe entstehen. So wird die Sicherheit von Software neben der Funktionalität für Anwender und Hersteller immer wichtiger. Die Anwendung neuer praktischer Methoden und das systematische Befolgen von Sicherheitsprozessen sollen Hersteller und Integratoren von Software bei der Vermeidung von Sicherheitslücken unterstützen. Die Verbesserung von Entwicklungs- und Sicherheitsprozessen bietet Herstellern auch die Möglichkeit, bei verbesserten Sicherheitseigenschaften Kosten und Entwicklungszeiten von Software zu reduzieren. Für Unternehmen hat dieser Schritt eine große strategische Bedeutung mit großer Relevanz für deren mittel- bis langfristige Wettbewerbsfähigkeit. Da Softwareprodukte und Softwareentwicklungsprozesse heute sehr komplex sein können, ist es für Hersteller nicht klar, wie Security by Design und die hierfür erforderlichen Sicherheitsprozesse nutzbringend und wirtschaftlich umgesetzt werden können. Es ist die Aufgabe der angewandten Forschung, die Herausforderungen in diesem Zusammenhang anzugehen, zu bewältigen und verwertbare Lösungen in die Praxis zu transferieren.
Die Saarbrücker Graduiertenschule der Informatik.
No abstract available.
Can We Trust Software Repositories?
To acquire data for empirical studies, software engineering researchers frequently leverage software repositories as data sources. Indeed, version and bug databases contain a wealth of data on how a product came to be. To turn this data into knowledge, though, requires deep insights into the specific process and product; and it requires careful scrutiny of the techniques used to obtain the data. The central challenge of the future will thus be to combine both automatic and manual empirical analysis.
Fault Prediction, Localization, and Repair (Dagstuhl Seminar 13061)
Software debugging, which involves localizing, understanding, and removing the cause of a failure, is a notoriously difficult, extremely time consuming, and human-intensive activity. For this reason, researchers have invested a great deal of effort in developing automated techniques and tools for supporting various debugging tasks. In this seminar, we discussed several different tools and techniques that aid in the task of Fault Prediction, Localization and Repair. The talks encompassed a wide variety of methodologies for fault prediction and localizing, such as - statistical fault localization, - core dump analysis, - taint analysis, - program slicing techniques, - dynamic fault-comprehension techniques, - visualization techniques, - combining hardware and software instrumentation for fault detection and failure prediction, - and verification techniques for checking safety properties of programs. For automatically (or semi-automatically) repairing faulty programs, the talks covered approaches such as - automated repair based on symbolic execution, constraint solving and program synthesis, - combining past fix patterns, machine learning and semantic patch generation - a technique that exploits the intrinsic redundancy of reusable components, - a technique based on memory-access patterns and a coverage matrix, - a technique that determines a combination of mutual-exclusion and order relationships that, once enforced, can prevent buggy interleaving. in addition, this seminar also explored some unusual topics such as Teaching Debugging, using Online Courses. Another interesting topic covered was the low representation of females in computing, and how programming and debugging tools interact with gender differences.
Muster der Softwaretechnik-Lehre.
Das Organisieren von Softwaretechnik-Projekten ist für neue Dozenten stets eine Herausforderung: Man muss Kunden finden, Projekte definieren, Tutoren schulen, Anforderungen definieren, Terminpläne machen, Regeln aufstellen. . . und zum Schluss das Projektergebnis bewerten. Eine ungenügende Planung und Umsetzung kann schnell das gesamte Projekt gefährden. Daher empfiehlt es sich, für die häufigsten Fragen und Probleme Lösungen explizit aufzuschreiben. Dies können zunächst immer wiederkehrende Dinge sein, wie die Frage, wann welche Vorlesungsinhalte benötigt werden, oder wie die Begutachtung von Praktikums-Dokumenten organisiert werden muss. Andere Fragen mögen zunächst banal erscheinen, machen aber schnell deutlich, warum es sich empfiehlt, Wissen explizit zu dokumentieren: • Ich möchte eine Grillfeier für 160 Teilnehmer organisieren. Wieviele Getränke und Speisen muss ich pro Person ansetzen? Woher bekomme ich Grills, Würstchen, Besteck, Salate? • Ich möchte eine „Messe“ organisieren, auf dem meine Praktikums-Teilnehmer ihre Projekte vorstellen können. Was brauche ich, und woher bekomme ich es? Wann muss ich wen verständigen? Um das Wissen über Softwaretechnik-Lehre zu verwalten, haben wir ein Wiki eingerichtet, um das Wissen über Softwaretechnik-Lehre zu organisieren und zu dokumentieren – und zwar in einer Form, die sowohl lokales als auch standortübergreifendes Wissen dokumentieren kann. Die Artikel nutzen das Format der Muster – analog zu Entwurfsmustern als Lösungsschablonen für immer wiederkehrende Probleme: Name. Jedes Muster hat einen Namen, damit es unter diesem Namen einfach referenziert und nachgeschlagen werden kann. Ziel. Jedes Muster löst ein bestimmtes Problem, das hier allgemein charakterisiert werden kann. Motivation. Das Problem sollte relevant sein – also häufig oder in verschiedenen Ausprägungen immer und immer wieder auftreten. Anwendung. Dies ist eine Beschreibung, wie das Muster umgesetzt wird – sowohl in abstrakter (standortunabhängiger) Form als auch illustriert an konkreten Beispielen. Die Anwendung sollte zeigen, wie das Muster das Problem löst. Zusammenspiel mit anderen Mustern. Voraussetzungen in Form von anderen Mustern sowie Konflikte mit anderen Mustern sollten ebenfalls dokumentiert werden. Folgen. Was sind die Folgen dieses Musters? Hier sind insbesondere unerwartete und negative Folgen gemeint, die dem Leser bewusst sein sollten. Tipps und Tricks. In diesem Abschnitt können Hilfestellungen zur Umsetzung des Musters, sowie gemachte Erfahrungen dokumentiert werden. Bekannte Einsätze. Dieser Abschnitt verweist auf die Veranstaltungen, in denen das Muster umgesetzt wurde. Die Wiki-Einträge zu den Veranstaltungen wiederum beschreiben die dort eingesetzten Muster, sowie ggf. Kontaktpersonen und Webseiten mit weiterführenden Informationen. Abbildung 1 zeigt eins der derzeit gut 50 Muster
Classifying code changes and predicting defects using changegenealogies
Identifying bug fixes and using them to estimate or even predict software quality is a frequent task when mining version archives. The number of applied bug fixes serves as code quality metric identifying defect-prone and non-defect-prone code artifacts. But when is a set of applied code changes, we call it change set, considered a bug fix and which metrics should be used to building high quality defect prediction models? Most commonly, bug fixes are identified by analyzing commit messages—short, mostly unstructured pieces of plain text. Commit message containing keywords such as “fix” or “issue” followed by a bug report identifier, are considered to fix the corresponding bug report. Similar, most defect prediction models use metrics describing the structure, complexity, or dependencies of source code artifacts. Complex or central code is considered to be more defect-prone. But commit messages and code metrics describe the state of software artifacts and code changes at a particular point in time, disregarding their genealogies that describe how the current state description came to be. There are approaches measuring historic properties of code artifacts [1]–[5] and using code dependency graphs [6], [7] but non of these approaches tracks the structural dependency paths of code changes to measure the centrality and impact of change sets, although change sets are those development events that make the source code look as it does. Herzig et al. [8] used so called change genealogy graphs to model structural dependencies between change sets. The authors used these change genealogy graphs to measure and analyze the impact of change sets on other, later applied change sets. In this paper, we make use of change genealogy graphs to define a set of change genealogy network metrics describing the structural dependencies of change sets. We further investigate whether change genealogy metrics can be used to identify bug fixing change sets (without using commit messages and bug databases) and whether change genealogy metrics are expressive enough to build effective defect prediction models classifying source files to be defect-prone or not. Regarding the identification of bug fixing change sets, our assumption is that change sets applying bug fixes show significant dependency differences when compared to change sets applying new feature implementations. We suspect that implementing and adding a new feature implies adding new method definitions that impact a large set of later applied code changes, which add code fragments adding method calls to these newly defined methods. In contrast, we suspect bug fixes to be relatively small rarely defining new methods but modifying existing features and thus to have a small impact on later applied code changes. The impact of bug fixes is to modify the runtime behavior of the software system rather than causing future change sets to use different functionality. Similar, we suspect more central change sets—depending on a large set of earlier change sets and causing many later applied change sets to be dependent on itself—to be crucial to the software development process. Consequently, we suspect code artifacts that got many crucial and central code changes applied to be more defect prone than others. More specifically, we seek to answer the following research questions in our study: RQ1 How do bug fix classification models based on change genealogy metrics compare to classification models based on code complexity metrics (Section V)? RQ2 How do defect prediction models compare with defect prediction models based on code complexity or code dependency network metrics (Section VI)? We tested the classification and prediction abilities of our approaches on four open source projects. The results show that change genealogy metrics can be used to separate bug fixing from feature implementing change sets with an average precision of 72% and an average recall of 89%. Our results also show that defect prediction models based on change genealogy metrics can predict defect-prone source files with precision and recall values of up to 80%. On average the precision for change genealogy models lies at 69% and the average recall at 81%. Compared to prediction models based on code dependency network metrics, change genealogy based prediction models achieve better precision and comparable recall values.
Fuzzing with Code Fragments.
Fuzz testing is an automated technique providing random data as input to a software system in the hope to expose a vulnerability. In order to be effective, the fuzzed input must be common enough to pass elementary consistency checks; a JavaScript interpreter, for instance, would only accept a semantically valid program. On the other hand, the fuzzed input must be uncommon enough to trigger exceptional behavior, such as a crash of the interpreter. The LangFuzz approach resolves this conflict by using a grammar to randomly generate valid programs; the code fragments, however, partially stem from programs known to have caused invalid behavior before. LangFuzz is an effective tool for security testing: Applied on the Mozilla JavaScript interpreter, it discovered a total of 105 new severe vulnerabilities within three months of operation (and thus became one of the top security bug bounty collectors within this period); applied on the PHP interpreter, it discovered 18 new defects causing crashes.
Proving programs continuous: technical perspective
An abstract is not available.
Mining models
Modern Model Checking techniques can easily verify advanced properties in complex software systems. Specifying these models and properties is as hard as ever, though. I present techniques to extract models from legacy systems—models that are precise and complete enough to serve as specifications, and which open the door to modular verification.
Search-based system testing: high coverage, no false alarms
Modern test case generation techniques can automatically achieve high code coverage. If they operate on the unit level, they run the risk of generating inputs infeasible in reality, which, when causing failures, are painful to identify and eliminate. Running a unit test generator on five open source Java programs, we found that all of the 181 reported failures were false failures—that is, indicating a problem in the generated test case rather than the program. By generating test cases at the GUI level, our EXSYST prototype can avoid such false alarms by construction. In our evaluation, it achieves higher coverage than search-based test generators at the unit level; yet, every failure can be shown to be caused by a real sequence of input events. Whenever a system interface is available, we recommend considering search-based system testing as an alternative to avoid false failures.
Isolating failure causes through test case generation
Manual debugging is driven by experiments—test runs that narrow down failure causes by systematically confirming or excluding individual factors. The BUGEX approach leverages test case generation to systematically isolate such causes from a single failing test run—causes such as properties of execution states or branches taken that correlate with the failure. Identifying these causes allows for deriving conclusions as: “The failure occurs whenever the daylight savings time starts at midnight local time.” In our evaluation, a prototype of BUGEX precisely pinpointed important failure explaining facts for six out of seven real-life bugs.
Webmate: a tool for testing web 2.0 applications
applications based on JavaScript pose new challenges for testing, as a simple crawling through links covers only a small part of the functionality. The WEBMATE approach automatically explores and navigates through arbitrary Web 2.0 applications. WEBMATE addresses challenges such as interactive elements, state abstraction, and non-determinism in large applications; we demonstrate its usage for regular application testing as well as for cross-browser testing.
EXSYST: search-based GUI testing
Test generation tools commonly aim to cover structural artefacts of software, such as either the source code or the user interface. However, focusing only on source code can lead to unrealistic or irrelevant test cases, while only exploring a user interface often misses much of the underlying program behavior. Our EXSYST prototype takes a new approach by exploring user interfaces while aiming to maximize code coverage, thus combining the best of both worlds. Experiments show that such an approach can achieve high code coverage matching and exceeding the code coverage of traditional unit-based test generators; yet, by construction every test case is realistic and relevant, and every detected failure can be shown to be caused by a real sequence of input events.
Sambamba: A runtime system for online adaptive parallelization
How can we exploit a microprocessor as efficiently as possible? The “classic” approach is static optimization at compile-time, optimizing a program for all possible uses. Further optimization can only be achieved by anticipating the actual usage profile: If we know, for instance, that two computations will be independent, we can run them in parallel. In the Sambamba project, we replace anticipation by adaptation. Our runtime system provides the infrastructure for implementing runtime adaptive and speculative transformations. We demonstrate our framework in the context of adaptive parallelization. We show the fully automatic parallelization of a small irregular C program in combination with our adaptive runtime system. The result is a parallel execution which adapts to the availability of idle system resources. In our example, this enables a 1.92 fold speedup on two cores while still preventing oversubscription of the system.
SPECIAL SECTION ON THE INTERNATIONAL SYMPOSIUM ON SOFTWARE TESTING AND ANALYSIS
Presents the front cover/table of contents for this issue of the periodical.
Automatically generating test cases for specification mining
Dynamic specification mining observes program executions to infer models of normal program behavior. What makes us believe that we have seen sufficiently many executions? The TAUTOKO (“Tautoko” is the Mãori word for “enhance, enrich.”) typestate miner generates test cases that cover previously unobserved behavior, systematically extending the execution space, and enriching the specification. To our knowledge, this is the first combination of systematic test case generation and typestate mining-a combination with clear benefits: On a sample of 800 defects seeded into six Java subjects, a static typestate verifier fed with enriched models would report significantly more true positives and significantly fewer false positives than the initial models.
Mutation-driven generation of unit tests and oracles
To assess the quality of test suites, mutation analysis seeds artificial defects (mutations) into programs; a nondetected mutation indicates a weakness in the test suite. We present an automated approach to generate unit tests that detect these mutations for object-oriented classes. This has two advantages: First, the resulting test suite is optimized toward finding defects modeled by mutation operators rather than covering code. Second, the state change caused by mutations induces oracles that precisely detect the mutants. Evaluated on 10 open source libraries, our μtest prototype generates test suites that find significantly more seeded defects than the original manually written test suites.
Technical Perspective Proving Programs Continuous
While the intuitive idea is easy to grasp, the concept of continuity so far has widely evaded formal treatment; in particular, it was not possible to automatically reason about continuity in the presence of loops. This is where the work of Swarat Chaudhuri, Sumit PrOVInG a PrOGraM’S correctness is usually an all-or-nothing game. Either a program is correct with respect to its specification or it is not. If our proof succeeds, we have 100% correctness; if our proof does not succeed, we have nothing. Formal correctness proofs are difficult, because one must symbolically cover the entire range of possible inputs—and the slightest gap in the input leaves us with a gap in the proof. But what if it turned out the risk posed by leaving a gap is actually small? This, of course, is the assumption of testing: If I tested some function with a sample of values, and it works correctly for this sample, I have reasons to assume it will also work for similar values. This is something I can assume if the behavior of my function is continuous—if it computes the correct square root for 10, 100, and 1,000, it should also do the right thing for any value in between. One may think this is a dangerous assumption: Simply because my program has worked well for these three values, why should it work for any other? A program is free to do anything; since it need not obey mathematical or physical laws, it can behave in an entirely different way for any new value. This logical view is true in principle. In real life, however, programmers prefer abstractions that are easy to understand and to reason about. The reason testing works in practice is that programmers naturally strive toward continuity. Gulwani, and Roberto Lublinerman comes into play. Their framework can formally verify programs for continuity, proving that small changes to the input only cause small changes to the output. They show that several programs such as sorting or shortest path are continuous—or even Lipschitz continuous, implying that perturbations to a function’s input cause only proportional changes to its output. Such a function would also be declared robust, meaning it will behave predictably even when inputs are uncertain or erroneous. Being able to formally reason about continuity and robustness lets us see programs as driven not only by logic, but also analytical calculus; and this view can be very helpful for understanding why programs generally tend to work well even if only coarsely tested. This work also bridges the gap between programs and control theory, allowing for ample cross-fertilization between the fields; indeed, one can think of mathematical optimizations of program code just as the adoption of programming concepts by control theory. So, should we treat programs as driven by logic, by calculus, or both? I encourage you to read the following paper to see the manifold connections between logic and calculus in computer programs.
Inferring Loop Invariants Dynamically
There is extensive literature on inferring loop invariants statically (i.e. without explicitly executing the program under analysis). We report on a new dynamic technique for inferring loop invariants based on the invariant detector Daikon [2]. Unlike InvGen [4], this new technique follows a counter example guided approach for refining candidate loop invariants. Let us consider the following annotated program for multiplying 16 bit integers in the left column: _(requires 0<=x<65535) _(requires 0<=y<65535) _(ensures \result==x*y) { mult = i = 0; while (i<y) { mult+=x; i++; } return mult; } // Candidate Loop Invariants #1 x one of { 1, 1316 } #2 y one of { 1, 131 } #3 i >= 0 ... #9 i <= y #10 i == (mult / x) #11 mult == (x * i) Our approach starts by finding new test cases using the search-based test suite generator EvoSuite [3]. Then, the dynamic invariant detector collects 11 different loop invariant candidates (excerpt shown on the right), which we feed to the static verifier VCC [1]. Since the conjunction of all candidates under-approximates the loop invariant, the static verifier fails. Then, EvoSuite guides the generation of new test inputs using the static verifier’s error model. The invariant detector synthesizes new candidates (ruling some of them out), which are fed to VCC. This refinement continues until VCC successfully verifies the program (using only candidates #9 and #11). The combination of test case generation and Daikon opens the potential for inferring loop invariants even for nontrivial programs. Current challenges include the static verification itself, as well as refining the candidate loop invariants. The main challenge, however, will be to find appropriate patterns for the most recurrent loop invariants: Daikon itself is limited to at most three related variables, and we will have to expand the search space considerably. Finally, we are also looking for benchmarks such that we can compare against other existing automatic loop invariant detectors, such as InvGen [4].
Exploring realistic program behavior
Modern test case generation techniques can automatically achieve high code coverage. If they operate on the unit level, they run the risk of generating nonsensical inputs, which, when causing failures, are painful to identify and eliminate. Running a unit test generator on five open source Java programs, we found that all of the 181 reported failures were false failures—that is, indicating a problem in the generated test case rather than the program. By generating test cases at the GUI level, our EXSYST prototype can avoid such false failures by construction. In our evaluation, it achieves higher coverage than search-based test generators at the unit level; yet, every failure can be shown to be caused by a real sequence of input events. Whenever a system interface is available, we recommend considering search-based system testing as an alternative to avoid false failures.
Mining temporal specifications from object usage
Abstract A caller must satisfy the callee's precondition—that is, reach a state in which the callee may be called. Preconditions describe the state that needs to be reached, but not how to reach it. We combine static analysis with model checking to mine Fair Computation Tree Logic (CTL F) formulas that describe the operations a parameter goes through:“In parseProperties (String xml), the parameter xml normally stems from getProperties ().” Such operational preconditions can be learned from program code, and the code can be checked for their violations. Applied to AspectJ, our Tikanga prototype found 169 violations of operational preconditions, uncovering 7 unique defects and 27 unique code smells—with 52% true positives in the 25% top-ranked violations.
Mining cause-effect-chains from version histories
Software reliability is heavily impacted by soft ware changes. How do these changes relate to each other? By analyzing the impacted method definitions and usages, we determine dependencies between changes, resulting in a change genealogy that captures how earlier changes enable and cause later ones. Model checking this genealogy reveals temporal process patterns that encode key features of the software process such as pending development activities:" Whenever class A is changed, its test case is later updated as well." Such patterns can be validated automatically: In an evaluation of four open source histories, our prototype would recommend pending activities with a precision of 60-72%.
Failure is a four-letter word: a parody in empirical research
Background: The past years have seen a surge of techniques predicting failure-prone locations based on more or less complex metrics. Few of these metrics are actionable, though. Aims: This paper explores a simple, easy-to-implement method to predict and avoid failures in software systems. The IROP method links elementary source code features to known software failures in a lightweight, easy-to-implement fashion. Method: We sampled the Eclipse data set mapping defects to files in three Eclipse releases. We used logistic regression to associate programmer actions with defects, tested the predictive power of the resulting classifier in terms of precision and recall, and isolated the most defect-prone actions. We also collected initial feedback on possible remedies. Results: In our sample set, IROP correctly predicted up to 74% of the failure-prone modules, which is on par with the most elaborate predictors available. We isolated a set of four easy-to-remember recommendations, telling programmers precisely what to do to avoid errors. Initial feedback from developers suggests that these recommendations are straightforward to follow in practice. Conclusions: With the abundance of software development data, even the simplest methods can produce "actionable" results.
Search-based program analysis
Traditionally, program analysis has been divided into two camps: Static techniques analyze code and safely determine what cannot happen; while dynamic techniques analyze executions to determine what actually has happened. While static analysis suffers from overapproximation, erring on whatever could happen, dynamic analysis suffers from underapproximation, ignoring what else could happen. In this talk, I suggest to systematically generate executions to enhance dynamic analysis, exploring and searching the space of software behavior. First results in fault localization and specification mining demonstrate the benefits of search-based analysis.
Assessing modularity via usage changes
Good program design strives towards modularity, that is, limiting the effects of changes to the code. We assess the modularity of software modules by mining change histories: The more a change to a module implementation changes its usage in client code, the lower its modularity. In an early analysis of four different releases of open-source projects, we found that changes can differ greatly in their impact on client code, and that such impact helps in assessing modularity.
Untangling changes
—When developers commit software changes to a version control system, they often commit unrelated changes in a single transaction—simply because, while, say, fixing a bug in module A, they also came across a typo in module B, and updated a deprecated call in module C. When analyzing such archives later, the changes to A, B, and C are treated as being falsely related. In an evaluation of five Java projects, we found up to 15% of all fixes to consist of multiple unrelated changes, compromising the resulting analyses through noise and bias. We present the first approach to untangle such combined changes after the fact. By taking into account data dependencies, distance measures, change couplings, test impact couplings, and distances in call graphs, our approach is able to untangle tangled changes with a mean success rate of 63–75%. Our recommendation is that such untangling be considered as a mandatory step in mining software archives.
Mining evolution of object usage
As software evolves, so does the interaction between its components. But how can we check if components are updated consistently? By abstracting object usage into temporal properties, we can learn evolution patterns that express how object usage evolves over time. Software can then be checked against these patterns, revealing code that is in need of update: “Your check for isValidWidget() is now superseded by checkWidget().” In an evaluation of seven different versions of three open source projects, our LAMARCK tool was able to detect existing code issues with a precision of 33%–64% and to prevent such issues with a precision of 90%–100%.
Minimizing reproduction of software failures
A program fails. What now? Taking a single failing run, we record and minimize the interaction between objects to the set of calls relevant for the failure. The result is a minimal unit test that faithfully reproduces the failure at will: "Out of these 14,628 calls, only 2 are required". In a study of 17 real-life bugs, our JINSI prototype reduced the search space to 13.7% of the dynamic slice or 0.22% of the source code, with only 1--12 calls left to examine.
Generating parameterized unit tests
State-of-the art techniques for automated test generation focus on generating executions that cover program behavior. As they do not generate oracles, it is up to the developer to figure out what a test does and how to check the correctness of the observed behavior. In this paper, we present an approach to generate parameterized unit tests---unit tests containing symbolic pre- and postconditions characterizing test input and test result. Starting from concrete inputs and results, we use test generation and mutation to systematically generalize pre- and postconditions while simplifying the computation steps. Evaluated on five open source libraries, the generated parameterized unit tests are (a) more expressive, characterizing general rather than concrete behavior; (b) need fewer computation steps, making them easier to understand; and (c) achieve a higher coverage than regular unit tests.
Dagstuhl Reports, Vol. 1, Issue 2 ISSN 2192-5283
No abstract available.
Mining precise specifications
Recent advances in software validation and verification make it possible to widely automate the check whether a specification is satisfied. This progress is hampered, though, by the persistent difficulty of writing specifications. Are we facing a "specification crisis"? By mining specifications from existing systems, we can alleviate this burden, reusing and extending the knowledge of 60 years of programming, and bridging the gap between formal methods and real-world software. But mining specifications has its challenges:We need good usage examples to learn expected behavior; we need to cope with the approximations of static and dynamic analysis; and we need specifications that are readable and relevant to users. In this talk, I present the state of the art in specification mining, its challenges, and its potential, up to a vision of seamless integration of specification and programming.
Lightweight Mining of Object Usage
Whenever we want to use a third-party package, we need to understand the services it provides as well as the services it requires. Such a description typically comes as part of the documentation of the programmer application interface (API), providing a specification for each function provided. Correct usage of these individual functions is also easily checked. Statically typed signatures, for instance, are part of most programming languages; violations can be discovered right within the editor.
ICSE 2011 technical briefings
The better we meet the interest of our community, the better we can help bringing ourselves up-to-date with the latest and greatest in and around software engineering. To this purpose, ICSE 2011 for the first time featured technical briefings, an all-day venue for communicating the state of topics related to software engineering, thus providing an exchange of ideas as well as an introduction to the main conference itself.
Which crashes should i fix first?: Predicting top crashes at an early stage to prioritize debugging efforts
Many popular software systems automatically report failures back to the vendors, allowing developers to focus on the most pressing problems. However, it takes a certain period of time to assess which failures occur most frequently. In an empirical investigation of the Firefox and Thunderbird crash report databases, we found that only 10 to 20 crashes account for the large majority of crash reports; predicting these “top crashes” thus could dramatically increase software quality. By training a machine learner on the features of top crashes of past releases, we can effectively predict the top crashes well before a new release. This allows for quick resolution of the most important crashes, leading to improved user experience and better allocation of maintenance efforts.
Specifications for free
Recent advances in software validation and verification make it possible to widely automate the check whether a specification is satisfied. This progress is hampered, though, by the persistent difficulty of writing specifications. Are we facing a “specification crisis”? By mining specifications from existing systems, we can alleviate this burden, reusing and extending the knowledge of 60 years of programming, and bridging the gap between formal methods and real-world software. In this NFM 2011 invited keynote, I present the state of the art in specification mining, its challenges, and its potential, up to a vision of seamless integration of specification and programming.
Breeding high-impact mutations
Mutation testing was developed to measure the adequacy of a test suite by seeding artificial bugs (mutations) into a program, and checking whether the test suite detects them. An undetected mutation either indicates a insufficiency in the test suite and provides means for improvement, or it is an equivalent mutation that cannot be detected because it does not change the program's semantics. Impact metrics-that quantify the difference between a run of the original and the mutated version of a program-are one way to detectnon-equivalent mutants. In this paper we present a genetic algorithm that aims to produce a set of mutations that have a high impact, are not detected by the test suite, and at the same time are well spread all over the code. We believe that such a set is useful for improving a test suite, as a high impact of a mutation implies it caused a grave damage, which is not detected by the test suite, and that the mutation is likely to be non-equivalent. First results are promising: The number of undetected mutants in a set of evolved mutants increases from 20 to over 70 percent, the average impact of these undetected mutants grows at the same time by a factor of 5.
Calibrated mutation testing
During mutation testing, artificial defects are inserted into a program, in order to measure the quality of a test suite and to provide means for improvement. These defects are generated using predefined mutation operators-inspired by faults that programmers tend to make. As the type of faults varies between different programmers and projects, mutation testing might be improved by learning from past defects-Does a sample of mutations similar to past defects help to develop better tests than a randomly chosen sample of mutations? In this paper, we present the first approach that uses software repository mining techniques to calibrate mutation testing to the defect history of a project. Furthermore, we provide an implementation and evaluation of calibrated mutation testing for the Jaxen project. However, first results indicate that calibrated mutation testing cannot outperform random selection strategies.
Assessing oracle quality with checked coverage
A known problem of traditional coverage metrics is that they do not assess oracle quality - that is, whether the computation result is actually checked against expectations. In this paper, we introduce the concept of checked coverage - the dynamic slice of covered statements that actually influence an oracle. Our experiments on seven open-source projects show that checked coverage is a sure indicator for oracle quality - and even more sensitive than mutation testing, its much more demanding alternative.
When does my program fail?
Oops! My program fails. Which are the circumstances under which this failure occurs? Answering this question is one of the first steps in debugging -- and a crucial one, as it helps characterizing, understanding, and classifying the problem. In this paper, we propose a technique to identify failure circumstances automatically. Given a concrete failure, we first compute the path condition leading to the failure and then use a constraint solver to identify, from the constraints in the path condition, the general failure conditions: "The program fails whenever the credit card number begins with 6, 5, and a non-zero digit." A preliminary evaluation of the approach on real programs demonstrates its potential usefulness.
Exploiting common object usage in test case generation
Generated test cases are good at systematically exploring paths and conditions in software. However, generated test cases often do not make sense. We adapt test case generation to follow patterns of common object usage, as mined from code examples. Our experiments show that generated tests thus (a) reuse familiar usage patterns, making them easier to understand and (b) focus on common usage, thus respecting implicit preconditions and avoiding meaningless tests.
Hardware and Software: Verification and Testing: 5th International Haifa Verification Conference, HCV 2009, Haifa, Israel, October 19-22, 2009, Revised Selected P...
This book constitutes the thoroughly refereed post proceedings of the 5th International Haifa Verification Conference, HVC 2009, held in Haifa, Israel in October 2009. The 11 revised full papers presented together with four abstracts of invited lectures were carefully reviewed and selected from 23 submissions. The papers address all current issues, challenges and future directions of verification for hardware, software, and hybrid systems and present academic research in the verification of systems, generally divided into two paradigms - formal verification and dynamic verification (testing).
ACM Fellows honored
Forty-one men and women are inducted as 2010 ACM Fellows.
SIGSOFT/FSE'11 19th ACM SIGSOFT Symposium on the Foundations of Software Engineering (FSE− 19) and ESEC'11: 13rd European Software Engineering Conf...
The ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE) is an internationally renowned forum for researchers, practitioners, and educators to present and discuss the most recent innovations, trends, experiences, and challenges in the field of software engineering. Formerly the FSE conference in alternating years and ESEC/FSE in other years, ESEC/FSE is now the new name of this annual conference series. The ESEC/FSE conference brings together experts from academia and industry to exchange the latest research results and trends, as well as their practical application in all areas of software engineering.
Hardware and Software: Verification and Testing
This book constitutes the thoroughly refereed proceedings of the 8th International Haifa Verification Conference, HVC 2012, held in Haifa, Israel in November 2012. The 18 revised full papers presented together with 3 poster presentations were carefully reviewed and selected from 36 submissions. They focus on the future directions of testing and verification for hardware, software, and complex hybrid systems.
Failure is a four-letter word
Background: The past years have seen a surge of techniques predicting failure-prone locations based on more or less complex metrics. Few of these metrics are actionable, though. Aims: This paper explores a simple, easy-to-implement method to predict and avoid failures in software systems. The IROP method links elementary source code features to known software failures in a lightweight, easy-to-implement fashion. Method: We sampled the Eclipse data set mapping defects to files in three Eclipse releases. We used logistic regression to associate programmer actions with defects, tested the predictive power of the resulting classifier in terms of precision and recall, and isolated the most defect-prone actions. We also collected initial feedback on possible remedies. Results: In our sample set, IROP correctly predicted up to 74% of the failure-prone modules, which is on par with the most elaborate predictors available. We isolated a set of four easy-to-remember recommendations, telling programmers precisely what to do to avoid errors. Initial feedback from developers suggests that these recommendations are straightforward to follow in practice. Conclusions: With the abundance of software development data, even the simplest methods can produce “actionable” results.
Self-repairing programs (dagstuhl seminar 11062)
Dagstuhl seminar 11062 ``Self-Repairing Programs'' included 23 participants and organizers from research and industrial communities. Self-Repairing Programs are a new and emerging area, and many participants reported that they initially felt their first research home to be in another area, such as testing, program synthesis, debugging, self-healing systems, or security. Over the course of the seminar, the participants found common ground in discussions of concerns, challenges, and the state of the art.
Mining specifications: A roadmap
Recent advances in software validation and verification make it possible to widely automate whether a specification is satisfied. This progress is hampered, though, by the persistent difficulty of writing specifications. Are we facing a “specification crisis”? In this paper, I show how to alleviate the burden of writing specifications by reusing and extending specifications as mined from existing software and give an overview on the state of the art in specification mining, its origins, and its potential.
Change bursts as defect predictors
In software development, every change induces a risk. What happens if code changes again and again in some period of time? In an empirical study on Windows Vista, we found that the features of such change bursts have the highest predictive power for defect-prone components. With precision and recall values well above 90%, change bursts significantly improve upon earlier predictors such as complexity metrics, code churn, or organizational structure. As they only rely on version history and a controlled change process, change bursts are straight-forward to detect and deploy.
Learning from 6,000 Projects: Mining Models in the Large
Models - abstract and simple descriptions of some artifact - are the backbone of all software engineering activities. While writing models is hard, existing code can serve as a source for abstract descriptions of how software behaves. To infer correct usage, code analysis needs usage examples, though, the more, the better. We have built a lightweight parser that efficiently extracts API usage models from source code - models that can then be used to detect anomalies. Applied on the 200 million lines of code of the Gen too Linux distribution, we would extract more than 15 million API constraints, encoding and abstracting the "wisdom of Linux code".
Learning from 6,000 projects: lightweight cross-project anomaly detection
Real production code contains lots of knowledge - on the domain, on the architecture, and on the environment. How can we leverage this knowledge in new projects? Using a novel lightweight source code parser, we have mined more than 6,000 open source Linux projects (totaling 200,000,000 lines of code) to obtain 16,000,000 temporal properties reflecting normal interface usage. New projects can be checked against these rules to detect anomalies - that is, code that deviates from the wisdom of the crowds. In a sample of 20 projects, ~25% of the top-ranked anomalies uncovered actual code smells or defects.
Automated fixing of programs with contracts
In program debugging, finding a failing run is only the first step; what about correcting the fault? Can we automate the second task as well as the first? The AutoFix-E tool automatically generates and validates fixes for software faults. The key insights behind AutoFix-E are to rely on contracts present in the software to ensure that the proposed fixes are semantically sound, and on state diagrams using an abstract notion of state based on the boolean queries of a class. Out of 42 faults found by an automatic testing tool in two widely used Eiffel libraries, AutoFix-E proposes successful fixes for 16 faults. Submitting some of these faults to experts shows that several of the proposed fixes are identical or close to fixes proposed by humans.
Generating test cases for specification mining
Dynamic specification mining observes program executions to infer models of normal program behavior. What makes us believe that we have seen sufficiently many executions? The typestate miner generates test cases that cover previously unobserved behavior, systematically extending the execution space and enriching the specification. To our knowledge, this is the first combination of systematic test case generation and typestate mining--a combination with clear benefits: On a sample of 800 defects seeded into six Java subjects, a static typestate verifier fed with enriched models would report significantly more true positives, and significantly fewer false positives than the initial models.
Introduction: The Best Papers of ISSTA
We present the best papers of the International Symposium on Software Testing and Analysis (ISSTA) 2008.
Emerging Faculty Symposium 2010
The challenge and prospect of becoming a new teaching faculty member at a research university is one that most people accept with enthusiasm and energy, but also with some trepidation: • How do I get a position? • How do I get to develop and publish strong research results? • How will I be able to balance the many aspects of work as well as my personal life? • What is my academic path to tenure and beyond?
(Un-) covering equivalent mutants
Mutation testing measures the adequacy of a test suite by seeding artificial defects (mutations) into a program. If a test suite fails to detect a mutation, it may also fail to detect real defects-and hence should be improved. However, there also are mutations which keep the program semantics unchanged and thus cannot be detected by any test suite. Such equivalent mutants must be weeded out manually, which is a tedious task. In this paper, we examine whether changes in coverage can be used to detect non-equivalent mutants: If a mutant changes the coverage of a run, it is more likely to be non-equivalent. Ina sample of 140 manually classified mutations of seven Java programs with 5,000to 100,000 lines of code, we found that: (a) the problem is serious and widespread-about 45% of all undetected mutants turned out to be equivalent;(b) manual classification takes time-about 15 minutes per mutation; (c)coverage is a simple, efficient, and effective means to identify equivalent mutants-with a classification precision of 75% and a recall of 56%; and (d)coverage as an equivalence detector is superior to the state of the art, in particular violations of dynamic invariants. Our detectors have been released as part of the open source Javalanche framework; the data set is publicly available for replication and extension of experiments.
Mining your own evidence
Throughout this book, you will find examples of how to gather evidence—evidence on the effectiveness of testing, the quality of bug reports, the role of complexity metrics, and so on. But do these findings actually apply to your project? The definite way to find this out is to repeat the appropriate study on your data, in your environment. This way, you will not only gather lots of insight into your own project; you will also experience the joys of experimental research. Unfortunately, you may also encounter the downside: empirical studies can be very expensive, in particular if they involve experiments with developers. Fortunately, there is a relatively inexpensive way to gather lots of evidence about your project. Software archives, such as version or bug repositories, record much of the activity around your product, in terms of problems occurring, changes made, and problems fixed. By mining these archives automatically, you can obtain lots of initial evidence about your product—evidence that already is worthy in itself, but which may also pave the path toward further experiments and further insights. In this chapter, we give a hands-on tutorial into mining software archives, covering both the basic technical steps and possible pitfalls that you may encounter on the way.
Mining API popularity
When designing a piece of software, one frequently must choose between multiple external libraries that provide similar services. Which library is the best one to use? We mined hundreds of open source projects and their external dependencies in order to observe the popularity of their APIs and to give recommendations of the kind: “Projects are moving away from this API element. Consider a change.” Such wisdom of the crowds can provide valuable information to both the API users and the API producers.
Generating fixes from object behavior anomalies
Advances in recent years have made it possible in some cases to locate a bug (the source of a failure) automatically. But debugging is also about correcting bugs. Can tools do this automatically? The results reported in this paper, from the new PACHIKA tool, suggest that such a goal may be reachable. PACHIKA leverages differences in program behavior to generate program fixes directly. It automatically summarizes executions to object behavior models, determines differences between passing and failing runs, generates possible fixes, and assesses them via the regression test suite. Evaluated on the ASPECTJ bug history, PACHIKA generates a valid fix for 3 out of 18 crashing bugs; each fix pinpoints the bug location and passes the ASPECTJ test suite.
Debugging debugging: acm sigsoft impact paper award keynote
Imagine some program and a number of changes. If none of these changes is applied ("yesterday"), the program works. If all changes are applied ("today"), the program does not work. Which change is responsible for the failure? This is how the abstract of the paper "Yesterday, my program worked. Today, it does not. Why?" started; a paper which, originally published at ESEC/FSE 1999 [12], introduced the concept of delta debugging, one of the most popular automated debugging techniques. This year, this paper receives the ACM SIGSOFT Impact Paper Award, recognizing its influence in the past ten years. In my keynote, I review the state of debugging then and now, share how it can be hard to be simple, what programmers really need, and what research should do (and should not do) to explore these needs and cater to them.
Mining trends of library usage
A library is available in multiple versions. Which one should I use? Has it been widely adopted already? Was it a good decision to switch to the newest version? We have mined hundreds of open-source projects for their library dependencies, and determined global trends in library usage. This wisdom of the crowds can be helpful for developers when deciding when to use which version of a library - by helping them avoid pitfalls experienced by other developers, and by showing important emerging trends in library usage.
Javalanche: efficient mutation testing for Java
To assess the quality of a test suite, one can use mutation testing - seeding artificial defects (mutations) into the program and checking whether the test suite finds them. Javalanche is an open source framework for mutation testing Java programs with a special focus on automation, efficiency, and effectiveness. In particular, Javalanche assesses the impact of individual mutations to effectively weed out equivalent mutants; it has been demonstrated to work on programs with up to 100,000 lines of code.
Why programs fail: a guide to systematic debugging
WHY PROGRAMS FAIL is a book about bugs in computer programs, how to reproduce them, how to find them, and how to fix them such that they do not occur anymore. This book teaches a number of techniques that allow you to debug any program in a systematic, and sometimes even elegant way. Moreover, the techniques can widely be automated, which allows you to let your computer do most of the debugging.
Efficient mutation testing by checking invariant violations
Mutation testing measures the adequacy of a test suite by seeding artificial defects (mutations) into a program. If a mutation is not detected by the test suite, this usually means that the test suite is not adequate. However, it may also be that the mutant keeps the program's semantics unchanged-and thus cannot be detected by any test. Such equivalent mutants have to be eliminated manually, which is tedious. We assess the impact of mutations by checking dynamic invariants. In an evaluation of our JAVALANCHE framework on seven industrial-size programs, we found that mutations that violate invariants are significantly more likely to be detectable by a test suite. As a consequence, mutations with impact on invariants should be focused upon when improving test suites. With less than 3% of equivalent mutants, our approach provides an efficient, precise, and fully automatic measure of the adequacy of a test suite.
Profiling Java programs for parallelism
One of the biggest challenges imposed by multi-core architectures is how to exploit their potential for legacy systems not built with multiple cores in mind. By analyzing dynamic data dependences of a program run, one can identify independent computation paths that could have been handled by individual cores. Our prototype computes dynamic dependences for Java programs and recommends locations to the programmer with the highest potential for parallelization. Such measurements can also provide starting points for automatic, speculative parallelization.
Predicting defects in SAP Java code: An experience report
Which components of a large software system are the most defect-prone? In a study on a large SAP Java system, we evaluated and compared a number of defect predictors, based on code features such as complexity metrics, static error detectors, change frequency, or component imports, thus replicating a number of earlier case studies in an industrial context. We found the overall predictive power to be lower than expected; still, the resulting regression models successfully predicted 50–60% of the 20% most defect-prone components.
Mining the Jazz repository: Challenges and opportunities
By integrating various development and collaboration tools into one single platform, the Jazz environment offers several opportunities for software repository miners. In particular, Jazz offers full traceability from the initial requirements via work packages and work assignments to the final changes and tests; all these features can be easily accessed and leveraged for better prediction and recommendation systems. In this paper, we share our initial experiences from mining the Jazz repository. We also give a short overview of the retrieved data sets and discuss possible problems of the Jazz repository and the platform itself.
The impact of equivalent mutants
If a mutation is not killed by a test suite, this usuallymeans that the test suite is not adequate. However, itmay also be that the mutant keeps the program’s seman-tics unchanged—and thus cannot be detected by any test.We found such equivalent mutants to be surprisingly com-mon: In an experiment on the JAXEN XPATH query engine,8/20 = 40% of all mutations turned out to be equivalent.Worse, checking the equivalency took us 15 minutes for asingle mutation. Equivalent mutants thus make it impossi-ble to automatically assess test suites by means of mutationtesting. To identify equivalent mutants, we are currently investi-gating the impact of a mutation on the execution: the morea mutation alters the execution, the higher the chance of itbeing non-equivalent. First experiments assessing the im-pact on code coverage are promising.
Mining Software Archives introduction
No qbstract available.
07491 Abstracts Collection--Mining Programs and Processes
From 02.12. to 17.12.2007, the Dagstuhl Seminar 07491 ``Mining Programs and Processes'' was held in Schloss Dagstuhl~--~Leibniz Center for Informatics. During the seminar, several participants presented their current research, and ongoing work and open problems were discussed. Abstracts of the presentations given during the seminar as well as abstracts of seminar results and ideas are put together in this paper. The first section describes the seminar topics and goals in general. Links to extended abstracts or full papers are provided, if available.
07491 Executive Summary--Mining Programs and Processes
The main goal of the seminar "Mining Programs and Processes" was to create a synergy between researchers of three communities, namely mining software repositories, data mining and machine learning, and empirical software engineering.
Guest Editors' Introduction: Mining Software Archives
Modern programming environments automatically collect lots of data on software development, notably changes and defects. The field of mining software archives is concerned with the automated extraction, collection, and abstraction of information from this data. This is the introduction to a special issue of IEEE Software presenting a selection of the exciting research that is taking place in the field.
Localizing bugs in program executions with graphical models
We devise a graphical model that supports the process of debugging software by guiding developers to code that is likely to contain defects. The model is trained using execution traces of passing test runs; it reflects the distribution over transitional patterns of code positions. Given a failing test case, the model determines the least likely transitional pattern in the execution trace. The model is designed such that Bayesian inference has a closed-form solution. We evaluate the Bernoulli graph model on data of the software projects AspectJ and Rhino.
Project-specific deletion patterns
We apply data mining to version control data in order to detect project-specific deletion patterns---subcomponents or features of the software that were deleted on purpose. We believe that locations that are similar to earlier deletions are likely to be code smells. Future recommendation tools can warn against such smells: "People who used gets() in the past now use fgets(). Consider a change, too."
What is the long-term impact of changes?
During their life cycle, programs undergo many changes. Each of these changes may introduce new features---or new problems. While most of the impact of a change is immediate, some of the impact may become evident only in the long term. For instance, suppose we make the internals of a component accessible to its clients. In itself, this does not introduce a problem. In the long term, though, this will most likely lead to maintainability issues. We are currently exploring ways to identify this long-term impact of change. We want to show how a change eventually impacts program quality (in terms of defects), program maintainability, and development effort. Identifying those changes with the greatest impact will foster our understanding of a program's history, and help us in learning lessons for future projects. Eventually, such lessons may come as automated recommendations regarding long-term impact: "In the long run, this change will cause maintainability issues. Do you want to reconsider?"
Replaying and isolating failing multi-object interactions
When a program fails, there are typically multiple objects that contribute to the failure. Our JINSI tool automatically captures the failure-causing interaction between objects and isolates a sequence of calls that all are relevant for reproducing the failure. In contrast to existing work, JINSI also isolates relevant interaction within the observed component and thus across all layers of a complex application. In a proof of concept, JINSI has successfully isolated the interaction for a failure of the COLUMBA e-mail client, pinpointing the defect: "Out of the 187,532 interactions in the addressbook component, two incoming calls suffice to reproduce the failure."
Predicting software metrics at design time
How do problem domains impact software features? We mine software code bases to relate problem domains (characterized by imports) to code features such as complexity, size, or quality. The resulting predictors take the specific imports of a component and predict its size, complexity, and quality metrics. In an experiment involving 89 plug-ins of the ECLIPSE project, we found good prediction accuracy for most metrics. Since the predictors rely only on import relationships, and since these are available at design time, our approach allows for early estimation of crucial software metrics.
Learning from software
During software development and maintenance, programmers conduct several activities--tracking bug reports, changing the software, discussing features, or running tests. As more and more of these activities are organized using tools, they leave data behind that is automatically accessible in software archives such as change or bug databases. By data mining these archives, one can leverage the resulting patterns and rules to increase program quality and programmer productivity. Analyzing software engineering data is, of course, a standard practice in empirical software engineering. What is new, though, is that we can now automate current empirical approaches. This leads to automated assistance in all development decisions for programmers and managers alike: "For this task, you should collaborate with Joe, because it will likely require risky work on the 'Mailbox' class."
Status Report ACM DL
No abstract available.
Predicting Defects in SAP Products: A Replicated Study
Given a large body of code, how do we know where to focus our quality assurance effort? By mining the software’s defect history, we can automatically learn which code features correlated with defects in the past—and leverage these correlations for new predictions: “In the past, high inheritance depth meant high number of defects. Since this new component also has a high inheritance depth, let us test it thoroughly”. Such history-based approaches work best if the new component is similar to the components learned from. But how does learning from history perform for projects with high variability between components? We ran a study on two SAP products involving a wide spectrum of functionality. We found that learning and predicting was accurate at package level, but not at product level. These results suggest that to learn from past defects, one should separate the product into component clusters with similar functionality, and make separate predictions for each cluster. Initial approaches to form such clusters automatically, based on similarity of metrics, showed promising accuracy.
Mining operational preconditions
A procedure’s client must satisfy its precondition— that is, reach a state in which the procedure may be called. Preconditions describe the state that needs to be reached, but not how to reach it. We use static analysis to infer the sequence of operations a variable goes through before being used as a parameter: “In parseProperties(String xml), the parameter xml normally stems from getProperties().” Such operational preconditions can be learned from code examples and checked to detect anomalies. Applied to AJ, our OP-M prototype found 288 violations of operational preconditions, uncovering 9 unique defects and 48 unique code smells.
Predicting bugs from history
Version and bug databases contain a wealth of information about software failures— how the failure occurred, who was affected, and how it was fixed. Such defect information can be automatically mined from software archives; and it frequently turns out that some modules are far more defect-prone than others. How do these differences come to be? We research how code properties like (a) code complexity, (b) the problem domain, (c) past history, or (d) process quality affect software defects, and how their correlation with defects in the past can be used to predict future software properties—where the defects are, how to fix them, as well as the associated cost.
