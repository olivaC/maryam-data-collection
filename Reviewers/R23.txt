Automated test case generation as a many-objective optimisation problem with dynamic selection of the targets
The test case generation is intrinsically a multi-objective problem, since the goal is covering multiple test targets (e.g., branches). Existing search-based approaches either consider one target at a time or aggregate all targets into a single fitness function (whole-suite approach). Multi and many-objective optimisation algorithms (MOAs) have never been applied to this problem, because existing algorithms do not scale to the number of coverage objectives that are typically found in real-world software. In addition, the final goal for MOAs is to find alternative trade-off solutions in the objective space, while in test generation the interesting solutions are only those test cases covering one or more uncovered targets. In this paper, we present Dynamic Many-Objective Sorting Algorithm (DynaMOSA), a novel many-objective solver specifically designed to address the test case generation problem in the context of coverage testing. DynaMOSA extends our previous many-objective technique Many-Objective Sorting Algorithm (MOSA) with dynamic selection of the coverage targets based on the control dependency hierarchy. Such extension makes the approach more effective and efficient in case of limited search budget. We carried out an empirical study on 346 Java classes using three coverage criteria (i.e., statement, branch, and strong mutation coverage) to assess the performance of DynaMOSA with respect to the whole-suite approach (WS), its archive-based variant (WSA) and MOSA. The results show that DynaMOSA outperforms WSA in 28 percent of the classes for branch coverage (+8 percent more coverage on average) and in 27 percent of the classes for mutation coverage (+11 percent more killed mutants on average). It outperforms WS in 51 percent of the classes for statement coverage, leading to +11 percent more coverage on average. Moreover, DynaMOSA outperforms its predecessor MOSA for all the three coverage criteria in 19 percent of the classes with +8 percent more code coverage on average.
Search Based Path and Input Data Generation for Web Application Testing
Test case generation for web applications aims at ensuring full coverage of the navigation structure. Existing approaches resort to crawling and manual/random input generation, with or without a preliminary construction of the navigation model. However, crawlers might be unable to reach some parts of the web application and random input generation might not receive enough guidance to produce the inputs needed to cover a given path. In this paper, we take advantage of the navigation structure implicitly specified by developers when they write the page objects used for web testing and we define a novel set of genetic operators that support the joint generation of test inputs and feasible navigation paths. On a case study, our tool Subweb was able to achieve higher coverage of the navigation model than crawling based approaches, thanks to its intrinsic ability of generating inputs for feasible paths and of discarding likely infeasible paths.
LIPS vs MOSA: a Replicated Empirical Study on Automated Test Case Generation
Replication is a fundamental pillar in the construction of scientific knowledge. Test data generation for procedural programs can be tackled using a single-target or a many-objective approach. The proponents of LIPS, a novel single-target test generator, conducted a preliminary empirical study to compare their approach with MOSA, an alternative many-objective test generator. However, their empirical investigation suffers from several external and internal validity threats, does not consider complex programs with many branches and does not include any qualitative analysis to interpret the results. In this paper, we report the results of a replication of the original study designed to address its major limitations and threats to validity. The new findings draw a completely different picture on the pros and cons of single-target vs many-objective approaches to test case generation.
APOGEN: automatic page object generator for web testing
Modern web applications are characterized by ultra-rapid development cycles, and web testers tend to pay scant attention to the quality of their automated end-to-end test suites. Indeed, these quickly become hard to maintain, as the application under test evolves. As a result, end-to-end automated test suites are abandoned, despite their great potential for catching regressions. The use of the Page Object pattern has proven to be very effective in end-to-end web testing. Page objects are façade classes abstracting the internals of web pages into high-level business functions that can be invoked by the test cases. By decoupling test code from web page details, web test cases are more readable and maintainable. However, the manual development of such page objects requires substantial coding effort, which is paid off only later, during software evolution. In this paper, we describe a novel approach for the automatic generation of page objects for web applications. Our approach is implemented in the tool Apogen, which automatically derives a testing model by reverse engineering the target web application. It combines clustering and static analysis to identify meaningful page abstractions that are automatically turned into Java page objects for Selenium WebDriver. Our evaluation on an open-source web application shows that our approach is highly promising: Automatically generated page object methods cover most of the application functionalities and result in readable and meaningful code, which can be very useful to support the creation of more maintainable web test suites.
Minimizing the stakeholder dissatisfaction risk in requirement selection for next release planning
Context The requirements to be delivered in the next software release are selected according to the stakeholders’ perceived value, expected implementation cost, budget availability, and precedence and technical dependency constraints. Existing approaches to the requirement selection problem do not take into account the risk of stakeholders’ dissatisfaction possibly resulting from divergence in the stakeholders’ estimates of the requirement value. Objective We present a novel risk-aware, multi-objective approach to the next release problem that aims at reducing the stakeholder dissatisfaction risk in a given cost/value region of interest provided by stakeholders. Method We have devised an exact algorithm to address the risk-aware formulation of the next release problem and implemented the algorithm using two well-known SMT solvers, Yices and Z3. To allow the application of the proposed formulation to large size problems, we have also implemented an approximate algorithm based on the NSGA-II metaheuristic. Results Results show that (1) the stakeholder dissatisfaction risk can be minimised with minimum impact on cost/value, and (2) our approach is scalable when NSGA-II is used. SMT solvers scale up to problems that are not overly large in terms of the number of requirements and/or are not too sparse in terms of dependencies, but the metaheuristic can quickly find good solutions even for large size problems. Conclusion We recommend the users of our approach to apply an SMT solver and to resort to a metaheuristic algorithm only if the SMT solver does not terminate within reasonable time, due to the actual combination of number of requirements and dependency density.
How professional hackers understand protected code while performing attack tasks
Code protections aim at blocking (or at least delaying) reverse engineering and tampering attacks to critical assets within programs. Knowing the way hackers understand protected code and perform attacks is important to achieve a stronger protection of the software assets, based on realistic assumptions about the hackers' behaviour. However, building such knowledge is difficult because hackers can hardly be involved in controlled experiments and empirical studies. The FP7 European project Aspire has given the authors of this paper the unique opportunity to have access to the professional penetration testers employed by the three industrial partners. In particular, we have been able to perform a qualitative analysis of three reports of professional penetration test performed on protected industrial code. Our qualitative analysis of the reports consists of open coding, carried out by 7 annotators and resulting in 459 annotations, followed by concept extraction and model inference. We identified the main activities: understanding, building attack, choosing and customizing tools, and working around or defeating protections. We built a model of how such activities take place. We used such models to identify a set of research directions for the creation of stronger code protections.
Generating valid grammar-based test inputs by means of genetic programming and annotated grammars
Automated generation of system level tests for grammar based systems requires the generation of complex and highly structured inputs, which must typically satisfy some formal grammar. In our previous work, we showed that genetic programming combined with probabilities learned from corpora gives significantly better results over the baseline (random) strategy. In this work, we extend our previous work by introducing grammar annotations as an alternative to learned probabilities, to be used when finding and preparing the corpus required for learning is not affordable. Experimental results carried out on six grammar based systems of varying levels of complexity show that grammar annotations produce a higher number of valid sentences and achieve similar levels of coverage and fault detection as learned probabilities.
Assessment of source code obfuscation techniques
Obfuscation techniques are a general category of software protections widely adopted to prevent malicious tampering of the code by making applications more difficult to understand and thus harder to modify. Obfuscation techniques are divided in code and data obfuscation, depending on the protected asset. While preliminary empirical studies have been conducted to determine the impact of code obfuscation, our work aims at assessing the effectiveness and efficiency in preventing attacks of a specific data obfuscation technique - VarMerge. We conducted an experiment with student participants performing two attack tasks on clear and obfuscated versions of two applications written in C. The experiment showed a significant effect of data obfuscation on both the time required to complete and the successful attack efficiency. An application with VarMerge reduces by six times the number of successful attacks per unit of time. This outcome provides a practical clue that can be used when applying software protections based on data obfuscation.
Test oracle assessment and improvement
We introduce a technique for assessing and improving test oracles by reducing the incidence of both false positives and false negatives. We prove that our approach can always result in an increase in the mutual information between the actual and perfect oracles. Our technique combines test case generation to reveal false positives and mutation testing to reveal false negatives. We applied the decision support tool that implements our oracle improvement technique to five real-world subjects. The experimental results show that the fault detection rate of the oracles after improvement increases, on average, by 48.6% (86% over the implicit oracle). Three actual, exposed faults in the studied systems were subsequently confirmed and fixed by the developers.
Automatic Page Object Generation with APOGEN
Page objects are used in web test automation to decouple the test cases logic from their concrete implementation. Despite the undeniable advantages they bring, as decreasing the maintenance effort of a test suite, yet the burden of their manual development limits their wide adoption. In this demo paper, we give an overview of Apogen, a tool that leverages reverse engineering, clustering and static analysis, to automatically generate Java page objects for web applications.
Clustering-aided page object generation for web testing
To decouple test code from web page details, web testers adopt the Page Object design pattern. Page objects are facade classes abstracting the internals of web pages (e.g., form fields) into high-level business functions that can be invoked by test cases (e.g., user authentication). However, writing such page objects requires substantial effort, which is paid off only later, during software evolution. In this paper we propose a clustering-based approach for the identification of meaningful abstractions that are automatically turned into Java page objects. Our clustering approach to page object identification has been integrated into our tool for automated page object generation, Apogen. Experimental results indicate that the clustering approach provides clusters of web pages close to those manually produced by a human (with, on average, only three differences per web application). 75 % of the code generated by Apogen can be used as-is by web testers, breaking down the manual effort for page object creation. Moreover, a large portion (84 %) of the page object methods created automatically to support assertion definition corresponds to useful behavioural abstractions.
Why do record/replay tests of web applications break?
Software engineers often use record/replay tools to enable the automated testing of web applications. Tests created in this manner can then be used to regression test new versions of the web applications as they evolve. Web application tests recorded by record/replay tools, however, can be quite brittle, they can easily break as applications change. For this reason, researchers have begun to seek approaches for automatically repairing record/replay tests. To date, however, there have been no comprehensive attempts to characterize the causes of breakagesin record/replay tests for web applications. In this work, wepresent a taxonomy classifying the ways in which record/replay tests for web applications break, based on an analysis of 453 versions of popular web applications for which 1065 individual test breakages were recognized. The resulting taxonomy can help direct researchers in their attempts to repair such tests. It can also help practitioners by suggesting best practices when creating tests or modifying programs, and can help researchers with other tasks such as test robustness analysis and IDE design.
Risk-aware multi-stakeholder next release planning using multi-objective optimization
[Context and motivation]: Software requirements selection is an essential task in the software development process. It consists of finding the best requirement set for each software release, considering several requirements characteristics, such as precedences and multiple conflicting objectives, such as stakeholders’ perceived value, cost and risk. [Question/Problem]: However, in this scenario, important information about the variability involved in the requirements values estimation are discarded and might expose the company to a risk when selecting a solution. [Principal ideas/results]: We propose a novel approach to the risk-aware multi-objective next release problem and implemented our approach by means of a satisfiability modulo theory solver. We aim at improving the decision quality by reducing the risk associated with the stakeholder dissatisfaction as related to the variability of the value estimation made by these stakeholders. [Contribution]: Results show that Pareto-optimal solutions exist where a major risk reduction can be achieved at the price of a minor penalty in the value-cost trade-off.
ROBULA+: An algorithm for generating robust XPath locators for web testing
Automated test scripts are used with success in many web development projects, so as to automatically verify key functionalities of the web application under test, reveal possible regressions and run a large number of tests in short time. However, the adoption of automated web testing brings advantages but also novel problems, among which the test code fragility problem. During the evolution of the web application, existing test code may easily break and testers have to correct it. In the context of automated DOM‐based web testing, one of the major costs for evolving the test code is the manual effort necessary to repair broken web page element locators – lines of source code identifying the web elements (e.g. form fields and buttons) to interact with. In this work, we present Robula+, a novel algorithm able to generate robust XPath‐based locators – locators that are likely to work correctly on new releases of the web application. We compared Robula+ with several state of the practice/art XPath locator generator tools/algorithms. Results show that XPath locators produced by Robula+ are by far the most robust. Indeed, Robula+ reduces the locators' fragility on average by 90% w.r.t. absolute locators and by 63% w.r.t. Selenium IDE locators. Copyright © 2016 John Wiley & Sons, Ltd.
Approaches and tools for automated end-to-end web testing
The importance of test automation in web engineering comes from the widespread use of web applications and the associated demand for code quality. Test automation is considered crucial for delivering the quality levels expected by users, since it can save a lot of time in testing and it helps developers to release web applications with fewer defects. The main advantage of test automation comes from fast, unattended execution of a set of tests after some changes have been made to a web application. Moreover, modern web applications adopt a multitier architecture where the implementation is scattered across different layers and run on different machines. For this reason, end-to-end testing techniques are required to test the overall behavior of web applications. In the last years, several approaches have been proposed for automated end-to-end web testing and the choice among them depends on a number of factors, including the tools used for web testing and the costs associated with their adoption. They can be classified using two main criteria: the first concerns how test cases are developed (ie, Capture-Replay and Programmable approaches), while, the second concerns how test cases localize the web elements to interact with (ie, Coordinates-based, DOM-based, and Visual approaches), that is what kind of locators are used for selecting the target GUI components. For developers and project managers it is not easy to select the most suitable automated end-to-end web testing approach for their needs among the existing ones. This chapter provides a comprehensive overview of the automated end-to-end web testing approaches and summarizes the findings of a long term research project aimed at empirically investigating their strengths and weaknesses.
Do automatically generated test cases make debugging easier? an experimental assessment of debugging effectiveness and efficiency
Several techniques and tools have been proposed for the automatic generation of test cases. Usually, these tools are evaluated in terms of fault-revealing or coverage capability, but their impact on the manual debugging activity is not considered. The question is whether automatically generated test cases are equally effective in supporting debugging as manually written tests. We conducted a family of three experiments (five replications) with humans (in total, 55 subjects) to assess whether the features of automatically generated test cases, which make them less readable and understandable (e.g., unclear test scenarios, meaningless identifiers), have an impact on the effectiveness and efficiency of debugging. The first two experiments compare different test case generation tools (Randoop vs. EvoSuite). The third experiment investigates the role of code identifiers in test cases (obfuscated vs. original identifiers), since a major difference between manual and automatically generated test cases is that the latter contain meaningless (obfuscated) identifiers. We show that automatically generated test cases are as useful for debugging as manual test cases. Furthermore, we find that, for less experienced developers, automatic tests are more useful on average due to their lower static and dynamic complexity.
Meta-heuristic generation of robust XPath locators for web testing
Test scripts used for web testing rely on DOM locators, often expressed as XPaths, to identify the active web page elements and the web page data to be used in assertions. When the web application evolves, the major cost incurred for the evolution of the test scripts is due to broken locators, which fail to locate the target element in the new version of the software. We formulate the problem of automatically generating robust XPath locators as a graph exploration problem, for which we provide an optimal, greedy algorithm. Since such an algorithm has exponential time and space complexity, we present also a genetic algorithm.
Results for EvoSuite--MOSA at the Third Unit Testing Tool Competition
Evo Suite-MOSA is a unit test data generation tool that employs a novel many-objective optimization algorithm suitably developed for branch coverage. It was implemented by extending the Evo Suite test data generation tool. In this paper we present the results achieved by Evo Suite-MOSA in the third Unit Testing Tool Competition at SBST'15. Among six participants, Evo Suite-MOSA stood third with an overall score of 189.22.
Weekly round trips from norms to requirements and tests: an industrial experience report
SEAC is a major software provider in Italy in the area of business management, with a focus on norms and human resources. SEAC is re-engineering their huge legacy system to C#/SQL Server. To minimise the risks associated with such reengineering project, SEAC has adopted an incremental and agile process model, which produces small and frequent releases of new, incremental modules that replace a portion of the legacy system at a time. Since the SEAC software handles business activities that are highly dependent on norms, such as the contracts of employees, the taxation of incomes and salaries, the pension contributions, one of the key challenges is to support a smooth transformation of norms into requirements, into code and eventually into test cases used to verify that norms have been implemented as prescribed by the law. The SE research unit at FBK has been involved to introduce a set of practices aimed at supporting such transformation, so as to improve the current process. We report the experience made during the project in this paper.
Why creating web page objects manually if it can be done automatically?
Page Object is a design pattern aimed at making web test scripts more readable, robust and maintainable. The effort to manually create the page objects needed for a web application may be substantial and unfortunately existing tools do not help web developers in such task. In this paper we present Apogen, a tool for the automatic generation of page objects for web applications. Our tool automatically derives a testing model by reverse engineering the target web application and uses a combination of dynamic and static analysis to generate Java page objects for the popular Selenium WebDriver framework. Our preliminary evaluation shows that it is possible to use around 3/4 of the automatic page object methods as they are, while the remaining 1/4 need only minor modifications.
Using multi-locators to increase the robustness of web test cases
The main reason for the fragility of web test cases is the inability of web element locators to work correctly when the web page DOM evolves. Web elements locators are used in web test cases to identify all the GUI objects to operate upon and eventually to retrieve web page content that is compared against some oracle in order to decide whether the test case has passed or not. Hence, web element locators play an extremely important role in web testing and when a web element locator gets broken developers have to spend substantial time and effort to repair it. While algorithms exist to produce robust web element locators to be used in web test scripts, no algorithm is perfect and different algorithms are exposed to different fragilities when the software evolves. Based on such observation, we propose a new type of locator, named multi-locator, which selects the best locator among a candidate set of locators produced by different algorithms. Such selection is based on a voting procedure that assigns different voting weights to different locator generation algorithms. Experimental results obtained on six web applications, for which a subsequent release was available, show that the multi-locator is more robust than the single locators (about -30% of broken locators w.r.t. the most robust kind of single locator) and that the execution overhead required by the multiple queries done with different locators is negligible (2-3% at most).
Reformulating branch coverage as a many-objective optimization problem
Test data generation has been extensively investigated as a search problem, where the search goal is to maximize the number of covered program elements (e.g., branches). Recently, the whole suite approach, which combines the fitness functions of single branches into an aggregate, test suite-level fitness, has been demonstrated to be superior to the traditional single-branch at a time approach. In this paper, we propose to consider branch coverage directly as a many-objective optimization problem, instead of aggregating multiple objectives into a single value, as in the whole suite approach. Since programs may have hundreds of branches (objectives), traditional many-objective algorithms that are designed for numerical optimization problems with less than 15 objectives are not applicable. Hence, we introduce a novel highly scalable many-objective genetic algorithm, called MOSA (Many-Objective Sorting Algorithm), suitably defined for the many- objective branch coverage problem. Results achieved on 64 Java classes indicate that the proposed many-objective algorithm is significantly more effective and more efficient than the whole suite approach. In particular, effectiveness (coverage) was significantly improved in 66% of the subjects and efficiency (search budget consumed) was improved in 62% of the subjects on which effectiveness remains the same.
Automated generation of visual web tests from DOM-based web tests
Functional test automation is increasingly adopted by web applications developers. In particular, 2nd generation tools overcome the limitations of 1st generation tools, based on screen coordinates, by providing APIs for easy selection and interaction with Document Object Model (DOM) elements. On the other hand, a new, 3rd generation of web testing tools, based on visual image recognition, brings the promise of wider applicability and simplicity. In this paper, we consider the problem of the automated creation of 3rd generation visual web tests from 2nd generation test suites. This transformation affects mostly the way in which test cases locate web page elements to interact with or to assert the expected test case outcome. Our tool PESTO determines automatically the screen position of a web element located in the DOM by a DOM-based test case. It then determines a rectangle image centred around the web element so as to ensure unique visual matching. Based on such automatically extracted images, the original, 2nd generation test suite is rewritten into a 3rd generation, visual test suite. Experimental results show that our approach is accurate, hence potentially saving substantial human effort in the creation of visual web tests from DOM-based ones.
Extraction of domain concepts from the source code
Program understanding involves mapping domain concepts to the code elements that implement them. Such mapping is often implicit and undocumented. However, identifier names contain relevant clues to rediscover the mapping and make it available to programmers. In this paper, we present two approaches that exploit structural and linguistic aspects of the source code to extract ontologies. The extracted ontologies are then compared in terms of the concepts they contain and the support they give to program understanding, specifically concept location. Such ontologies are composed of domain and implementation concepts as they come from the source code. To filter domain concepts, we have applied Information Retrieval (IR) based filtering techniques. We have assessed the resulting ontologies against a reference, manually defined, domain ontology. The experimentation was carried out using six real world open source programs. Results show that the ontologies extracted using the structural and linguistic aspects of the source code are complementary. We also observed that their union gives a better support to concept location than the individual ontologies. Filtering the ontologies gives a concise representation of the domain knowledge captured in the source code. The filtered ontologies, however, have been found to be less effective in supporting concept location than the unfiltered ontologies.
Meta-heuristic Generation of Robust XPath Locators for Web Testing
Test scripts used for web testing rely on DOM locators, often expressed as XPaths, to identify the active web page elements and the web page data to be used in assertions. When the web application evolves, the major cost incurred for the evolution of the test scripts is due to broken locators, which fail to locate the target element in the new version of the software. We formulate the problem of automatically generating robust XPath locators as a graph exploration problem, for which we provide an optimal, greedy algorithm. Since such an algorithm has exponential time and space complexity, we present also a genetic algorithm.
Why Creating Web Page Objects Manually If It Can Be Done Automatically?
Page Object is a design pattern aimed at making web test scripts more readable, robust and maintainable. The effort to manually create the page objects needed for a web application may be substantial and unfortunately existing tools do not help web developers in such task. In this paper we present APOGEN, a tool for the automatic generation of page objects for web applications. Our tool automatically derives a testing model by reverse engineering the target web application and uses a combination of dynamic and static analysis to generate Java page objects for the popular Selenium WebDriver framework. Our preliminary evaluation shows that it is possible to use around 3/4 of the automatic page object methods as they are, while the remaining 1/4 need only minor modifications.
Intrinsic software redundancy for self-healing software systems, automated oracle generation
Software systems are intrinsically redundant. We identify the sources of intrinsic software redundancy in good design practices, and suggest how to exploit intrinsic software redundancy to augment software systems with self-healing capabilities and to automatically generate test oracles.
Search-based synthesis of equivalent method sequences
Software components are usually redundant, since their interface offers different operations that are equivalent in their functional behavior. Several reliability techniques exploit this redundancy to either detect or tolerate faults in software. Metamorphic testing, for instance, executes pairs of sequences of operations that are expected to produce equivalent results, and identifies faults in case of mismatching outcomes. Some popular fault tolerance and self-healing techniques execute redundant operations in an attempt to avoid failures at runtime. The common assumption of these techniques, though, is that such redundancy is known a priori. This means that the set of operations that are supposed to be equivalent in a given component should be available in the specifications. Unfortunately, inferring this information manually can be expensive and error prone. This paper proposes a search-based technique to synthesize sequences of method invocations that are equivalent to a target method within a finite set of execution scenarios. The experimental results obtained on 47 methods from 7 classes show that the proposed approach correctly identifies equivalent method sequences in the majority of the cases where redundancy was known to exist, with very few false positives.
Reducing web test cases aging by means of robust XPath locators
In the context of web regression testing, the main aging factor for a test suite is related to the continuous evolution of the underlying web application that makes the test cases broken. This rapid decay forces the quality experts to evolve the test ware. One of the major costs of test case evolution is due to the manual effort necessary to repair broken web page element locators. Locators are lines of source code identifying the web elements the test cases interact with. Web test cases rely heavily on locators, for instance to identify and fill the input portions of a web page (e.g., The form fields), to execute some computations (e.g., By locating and clicking on buttons) and to verify the correctness of the output (by locating the web page elements showing the results). In this paper we present ROBULA (ROBUst Locator Algorithm), a novel algorithm able to partially prevent and thus reduce the aging of web test cases by automatically generating robust XPath-based locators that are likely to work also when new releases of the web application are created. Preliminary results show that XPath locators produced by ROBULA are substantially more robust than absolute and relative locators, generated by state of the practice tools such as Fire Path. Fragility of the test suites is reduced on average by 56% for absolute locators and 41% for relative locators.
Poster: A measurement framework to quantify software protections
Programs often run under strict usage conditions (e.g., license restrictions) that could be broken in case of code tampering. Possible attacks include malicious reverse engineering, tampering using static, dynamic and hybrid techniques, on standard devices as well as in labs with additional special purpose hardware equipment. ASPIRE (http://www.aspire-fp7.eu) is a European FP7 research project devoted to the elaboration of novel techniques to mitigate and prevent attacks to code integrity, to code/data confidentiality and to code lifting. This paper presents the ongoing activity to define a set of metrics aimed at quantifying the effect on code of the ASPIRE protections. The metrics have been conceived based on a measurement framework, which prescribes the identification of the relevant code features to consider and of their relationships with attacks and protections.
A Multi-objective Approach to Business Process Repair
Business process model repair aims at updating an existing model so as to accept deviant (e.g., new) behaviours, while remaining as close as possible to the initial model. In this paper, we present a multi-objective approach to process model repair, which maximizes the behaviours accepted by the repaired model while minimizing the cost associated with the repair operations. Given the repair operations for full process repair, we formulate the associated multi-objective problem in terms of a set of pseudo-Boolean constraints. In order to evaluate our approach, we have applied it to a case study from the Public Administration domain. Results indicate that it provides business analysts with a selection of good and tunable alternative solutions.
SCAN: an approach to label and relate execution trace segments
Program comprehension is a prerequisite to any maintenance and evolution task. In particular, when performing feature location, developers perform program comprehension by abstracting software features and identifying the links between high‐level abstractions (features) and program elements. We present Segment Concept AssigNer (SCAN), an approach to support developers in feature location. SCAN uses a search‐based approach to split execution traces into cohesive segments. Then, it labels the segments with relevant keywords and, finally, uses formal concept analysis to identify relations among segments. In a first study, we evaluate the performances of SCAN on six Java programs by 31 participants. We report an average precision of 69% and a recall of 63% when comparing the manual and automatic labels and a precision of 63% regarding the relations among segments identified by SCAN. After that, we evaluate the usefulness of SCAN for the purpose of feature location on two Java programs. We provide evidence that SCAN (i) identifies 69% of the gold set methods and (ii) is effective in reducing the quantity of information that developers must process to locate features—reducing the number of methods to understand by an average of 43% compared to the entire execution traces. Copyright © 2014 John Wiley & Sons, Ltd.
PESTO: A tool for migrating DOM-based to visual web tests
Automated testing of web applications reduces the effort needed in manual testing. Old 1st generation tools, based on screen coordinates, produce quite fragile test suites, tightly coupled with the specific screen resolution, window position and size experienced during test case recording. These tools have been replaced by a 2nd generation of tools, which offer easy selection and interaction with the web elements, based on DOM-oriented commands. Recently, a new 3rd generation of tools came up based on visual image recognition, bringing the promise of wider applicability and simplicity. A tester might ask if the migration towards such new technology is worthwhile, since the manual effort to rewrite a test suite might be overwhelming. In this paper, we propose PESTO, a tool facing the problem of the automated migration of 2nd generation test suites to the 3rd generation. PESTO determines automatically the screen position of each web element located on the DOM by a 2nd generation test case. It then calculates a screenshot image centred around the web element so as to ensure unique visual matching. Then, the entire source code of the DOM-based test suite is transformed into a visual test suite, based on such automatically extracted images and using specific visual commands.
Combining stochastic grammars and genetic programming for coverage testing at the system level
When tested at the system level, many programs require complex and highly structured inputs, which must typically satisfy some formal grammar. Existing techniques for grammar based testing make use of stochastic grammars that randomly derive test sentences from grammar productions, trying at the same time to avoid unbounded recursion. In this paper, we combine stochastic grammars with genetic programming, so as to take advantage of the guidance provided by a coverage oriented fitness function during the sentence derivation and evolution process. Experimental results show that the combination of stochastic grammars and genetic programming outperforms stochastic grammars alone.
RET 2014 Organization
Welcome to the 1st International Workshop on Requirements Engineering and Testing (RET'14), colocated with RE 2014, in Karlskrona (SE). The objective of RET is to explore the interaction of Requirements Engineering (RE) and Testing in research and industry, and the challenges that result from this interaction. While much work has been done in the respective fields of requirements engineering and testing, there exists much more than can be done to understand the connection between RE and Testing processes. We hope RET will provide a much needed forum for exchanging ideas and best practices for aligning RE and Testing, and in particular can help foster industry-academia collaboration on this topic, and thereby connect the communities of RE and Testing. Topics include processes, practices, artifacts, methods, techniques, tools and softer aspects like the communication between roles in the engineering process. The RET'14 workshop solicited two types of submissions: full technical papers presenting research results, and position papers introducing challenges, preliminary results, or industrial experience reports. We received 14 submissions, each of them was extensively reviewed by at least three members of the program committee. The general and program chairs were excluded from submission and review. Finally, four technical papers and six position papers were accepted for presentation and publication in the proceedings, organized by the following topics: • RET Challenges and practices • Quality requirements • Formal languages and models The workshop started with a keynote by Magnus Ohlsson entitled “The Agile Hangover – Handling testable agile requirements”, followed by three presentation and discussion sessions, as well as a conclusive interactive session to map the research area. We would like to thank all the authors, from academia and industry, for their submissions, and especially the members of the program committee who provided valuable and detailed feedback. Moreover, we thank the organizers of RE for their support. We are looking forward to an exciting workshop!
A family of experiments to assess the effectiveness and efficiency of source code obfuscation techniques
Context: code obfuscation is intended to obstruct code understanding and, eventually, to delay malicious code changes and ultimately render it uneconomical. Although code understanding cannot be completely impeded, code obfuscation makes it more laborious and troublesome, so as to discourage or retard code tampering. Despite the extensive adoption of obfuscation, its assessment has been addressed indirectly either by using internal metrics or taking the point of view of code analysis, e.g., considering the associated computational complexity. To the best of our knowledge, there is no publicly available user study that measures the cost of understanding obfuscated code from the point of view of a human attacker. Aim: this paper experimentally assesses the impact of code obfuscation on the capability of human subjects to understand and change source code. In particular, it considers code protected with two well-known code obfuscation techniques, i.e., identifier renaming and opaque predicates. Method: We have conducted a family of five controlled experiments, involving undergraduate and graduate students from four Universities. During the experiments, subjects had to perform comprehension or attack tasks on decompiled clients of two Java network-based applications, either obfuscated using one of the two techniques, or not. To assess and compare the obfuscation techniques, we measured the correctness and the efficiency of the performed task. Results: —at least for the tasks we considered—simpler techniques (i.e., identifier renaming) prove to be more effective than more complex ones (i.e., opaque predicates) in impeding subjects to complete attack tasks.
Visual vs. DOM-based web locators: An empirical study
Automation in Web testing has been successfully supported by DOM-based tools that allow testers to program the interactions of their test cases with the Web application under test. More recently a new generation of visual tools has been proposed where a test case interacts with the Web application by recognising the images of the widgets that can be actioned upon and by asserting the expected visual appearance of the result. In this paper, we first discuss the inherent robustness of the locators created by following the visual and DOM-based approaches and we then compare empirically a visual and a DOM-based tool, taking into account both the cost for initial test suite development from scratch and the cost for test suite maintenance during code evolution. Since visual tools are known to be computationally demanding, we also measure the test suite execution time. Results indicate that DOM-based locators are generally more robust than visual ones and that DOM-based test cases can be developed from scratch and evolved at lower cost. Moreover, DOM-based test cases require a lower execution time. However, depending on the specific features of the Web application under test and its expected evolution, in some cases visual locators might be the best choice (e.g., when the visual appearance is more stable than the structure).
Interpolated n-grams for model based testing
Models - in particular finite state machine models - provide an invaluable source of information for the derivation of effective test cases. However, models usually approximate part of the program semantics and capture only some of the relevant dependencies and constraints. As a consequence, some of the test cases that are derived from models are infeasible. In this paper, we propose a method, based on the computation of the N-gram statistics, to increase the likelihood of deriving feasible test cases from a model. Correspondingly, the level of model coverage is also expected to increase, because infeasible test cases do not contribute to coverage. While N-grams do improve existing test case derivation methods, they show limitations when the N-gram statistics is incomplete, which is expected to necessarily occur as N increases. Interpolated N-grams overcome such limitation and show the highest performance of all test case derivation methods compared in this work.
Reproducing field failures for programs with complex grammar-based input
To isolate and fix failures that occur in the field, after deployment, developers must be able to reproduce and investigate such failures in-house. In practice, however, bug reports rarely provide enough information to recreate field failures, thus making in-house debugging an arduous task. This task becomes even more challenging for programs whose input must adhere to a formal specification, such as a grammar. To help developers address this issue, we propose an approach for automatically generating inputs that recreate field failures in-house. Given a faulty program and a field failure for this program, our approach exploits the potential of grammar-guided genetic programming to iteratively find legal inputs that can trigger the observed failure using a limited amount of runtime data collected in the field. When applied to 11 failures of 5 real-world programs, our approach was able to reproduce all but one of the failures while imposing a limited amount of overhead.
FITTEST: A new continuous and automated testing process for future Internet applications
Since our society is becoming increasingly dependent on applications emerging on the Future Internet, quality of these applications becomes a matter that cannot be neglected. However, the complexity of the technologies involved in Future Internet applications makes testing extremely challenging. The EU FP7 FITTEST project has addressed some of these challenges by developing and evaluating a Continuous and Integrated Testing Environment that monitors a Future Internet application when it runs such that it can automatically adapt the testware to the dynamically changing behaviour of the application.
Reducing Web Test Cases Aging by means of Robust XPath Locators
In the context of web regression testing, the main aging factor for a test suite is related to the continuous evolution of the underlying web application that makes the test cases broken. This rapid decay forces the quality experts to evolve the testware. One of the major costs of test case evolution is due to the manual effort necessary to repair broken web page element locators. Locators are lines of source code identifying the web elements the test cases interact with. Web test cases rely heavily on locators, for instance to identify and fill the input portions of a web page (e.g., the form fields), to execute some computations (e.g., by locating and clicking on buttons) and to verify the correctness of the output (by locating the web page elements showing the results). In this paper we present ROBULA (ROBUst Locator Algorithm), a novel algorithm able to partially prevent and thus reduce the aging of web test cases by automatically generating robust XPath-based locators that are likely to work also when new releases of the web application are created. Preliminary results show that XPath locators produced by ROBULA are substantially more robust than absolute and relative locators, generated by state of the practice tools such as FirePath. Fragility of the test suites is reduced on average by 56% for absolute locators and 41% for relative locators.
Recent advances in web testing
Web applications have become key assets of our society, which depends on web applications for sectors like business, health-care, and public administration. Testing is the most widely used and effective approach to ensure quality and dependability of the software, including web applications. However, web applications are special as compared to traditional software, because they involve dynamic code creation and interpretation and because they implement a specific interaction mode, based on the navigation structure of the web application. Researchers have investigated approaches and techniques to automate web testing, dealing with the special features of web applications. This chapter contains a comprehensive overview of the research carried out in the last 10 years to support web testing with automated tools. We categorize the works available in the literature according to the specific web testing phase that they address. In particular, we first of all consider the works aiming at building a navigation model of the web application under test. In fact, such a model is often the starting point for test case derivation. Then, we consider the problem of input generation, because the traversal of a selected navigation path requires that appropriate input data are identified and submitted to the server during test execution. Metrics are introduced and used to assess the adequacy of the test cases constructed from the model. The last part of the chapter is devoted to very recent advancements in the area, focused on rich client web applications, which demand a specific approach to modeling and to test case derivation.
Test prioritization based on change sensitivity: an industrial case study
In the context of service-based systems, applications access software services, either home-built or third-party, to orchestrate their functionality. Since such services evolve independently from the applications, the latter need to be tested to make sure that they work properly with the updated or new services. In a previous work we have proposed a test prioritization approach that ranks test cases based on their sensitivity to external service changes. The idea is to give priority to the tests that detect the highest number of artificial changes (mutations), because they have a higher chance of detecting real changes in external services. In this paper, we apply change-sensitivity based test prioritization to an industrial system from IBM within the FITTEST European project. Results indicate that the ranked test cases achieve automatically comparable performance as manual prioritization made by an experienced team
The FITTEST tool suite for testing future internet applications
Future Internet applications are expected to be much more complex and powerful, by exploiting various dynamic capabilities For testing, this is very challenging, as it means that the range of possible behavior to test is much larger, and moreover it may at the run time change quite frequently and significantly with respect to the assumed behavior tested prior to the release of such an application. The traditional way of testing will not be able to keep up with such dynamics. The Future Internet Testing (FITTEST) project (http://crest.cs.ucl.ac.uk/fittest/), a research project funded by the European Commission (grant agreement n. 257574) from 2010 till 2013, was set to explore new testing techniques that will improve our capacity to deal with the challenges of testing Future Internet applications. Such techniques should not be seen as replacement of the traditional testing, but rather as a way to complement it. This paper gives an overview of the set of tools produced by the FITTEST project, implementing those techniques.
N-gram based test sequence generation from finite state models
Model based testing offers a powerful mechanism to test applications that change dynamically and continuously, for which only some limited black-box knowledge is available (this is typically the case of future internet applications). Models can be inferred from observations of real executions and test cases can be derived from models, according to various strategies (e.g., graph or random visits). The problem is that a relatively large proportion of the test cases obtained in this way might result to be non executable, because they involve infeasible paths. In this paper, we propose a novel test case derivation strategy, based on the computation of the N-gram statistics. Event sequences are generated for which the subsequences of size N respect the distribution of the N-tuples observed in the execution traces. In this way, generated and observed sequences share the same context (up to length N), hence increasing the likelihood for the generated ones of being actually executable. A consequence of the increased proportion of feasible test cases is that model coverage is also expected to increase.
Automated inference of classifications and dependencies for combinatorial testing
Even for small programs, the input space is huge - often unbounded. Partition testing divides the input space into disjoint equivalence classes and combinatorial testing selects a subset of all possible input class combinations, according to criteria such as pairwise coverage. The down side of this approach is that the partitioning of the input space into equivalence classes (input classification) is done manually. It is expensive and requires deep domain and implementation understanding. In this paper, we propose a novel approach to classify test inputs and their dependencies automatically. Firstly, random (or automatically generated) input vectors are sent to the system under test (SUT). For each input vector, an observed “hit vector” is produced by monitoring the execution of the SUT. Secondly, hit vectors are grouped into clusters using machine learning. Each cluster contains similar hit vectors, i.e., similar behaviors, and from them we obtain corresponding clusters of input vectors. Input classes are then extracted for each input parameter straightforwardly. Our experiments with a number of subjects show good results as the automatically generated classifications are the same or very close to the expected ones.
Sbfr: A search based approach for reproducing failures of programs with grammar based input
Reproducing field failures in-house, a step developers must perform when assigned a bug report, is an arduous task. In most cases, developers must be able to reproduce a reported failure using only a stack trace and/or some informal description of the failure. The problem becomes even harder for the large class of programs whose input is highly structured and strictly specified by a grammar. To address this problem, we present SBFR, a search-based failure-reproduction technique for programs with structured input. SBFR formulates failure reproduction as a search problem. Starting from a reported failure and a limited amount of dynamic information about the failure, SBFR exploits the potential of genetic programming to iteratively find legal inputs that can trigger the failure.
Supporting concept location through identifier parsing and ontology extraction
Identifier names play a key role in program understanding and in particular in concept location. Programmers can easily “parse” identifiers and understand the intended meaning. This, however, is not trivial for tools that try to exploit the information in the identifiers to support program understanding. To address this problem, we resort to natural language analyzers, which parse tokenized identifier names and provide the syntactic relationships (dependencies) among the terms composing the identifiers. Such relationships are then mapped to semantic relationships. In this study, we have evaluated the use of off-the-shelf and trained natural language analyzers to parse identifier names, extract an ontology and use it to support concept location. In the evaluation, we assessed whether the concepts taken from the ontology can be used to improve the efficiency of queries used in concept location. We have also investigated if the use of different natural language analyzers has an impact on the ontology extracted and the support it provides to concept location. Results show that using the concepts from the ontology significantly improves the efficiency of concept location queries (e.g., in some cases, an improvement of 127% is observed). The results also indicate that the efficiency of concept location queries is not affected by the differences in the ontologies produced by different analyzers.
Special section on ICSM 2011
The IEEE International Conference on Software Maintenance (ICSM) is the premiere international venue in software maintenance and evolution, where participants from academia, government, and industry meet and share ideas and experiences for solving critical software maintenance problems. In 2011, the 27th edition of ICSM was held at the Williamsburg Lodge, in the heart of Williamsburg, VA, USA. Williamsburg, once the capital of England's oldest, wealthiest, and most populous North American colony, is one of the most important American historical landmarks. ICSM 2011 was held from September 25 to 30, 2011. The conference was preceded by two co‐located events, the 11th IEEE International Working Conference on Source Code Analysis and Manipulation (SCAM) and the International Workshop on the Maintenance and Evolution of Service‐Oriented and Cloud‐Based Systems (MESOCA). It was followed by two other co‐located events, the 13th IEEE International Symposium on Web Systems Evolution (WSE) and the 6th IEEE International Workshop on Visualizing Software for Understanding and Analysis (VISSOFT). ICSM 2011 attracted 127 submissions, each of which was reviewed by at least three members of the ICSM Program Committee. Following a week and a half of online discussion supported by EasyChair, 36 papers were selected for publication and presentation in the Technical Program of the conference. Consensus was aimed for and reached in almost all cases. The topics of accepted papers ranged from Reverse Engineering and Program Comprehension to Impact Analysis, Traceability, Migration and Evolution, Refactoring, Software Clones, Linguistic Analysis, and Regression Testing. The conference program included the ICSM Doctoral Symposium, Tutorials, Tool Demonstrations, Posters, the Industry track, and the Early Research Achievements track. This special section of the Journal of Software: Evolution and Process contains extended versions of the papers selected by the ICSM 2011 Program Committee among the best papers presented at the conference. These extended versions have additionally undergone the rigorous JSEP journal review process: they were reviewed by three anonymous referees under the supervision and coordination of the guest editors. We are proud to present you the three excellent papers that are the results of this process. Paper 1, ‘An empirical study of faults in late propagation clone genealogies’, by Liliane Barbour, Foutse Khomh, and Ying Zou, investigates the history of clone evolution over time, focusing on the phenomenon of late propagation. Late propagation occurs whenever one of two clones diverges from the other, as a result of code evolution, while later such changes are reconciled, so as to remove the divergence. The authors have defined eight types of late propagation, and they have evaluated empirically the relationship between fault proneness and late propagation of various type. They have also investigated whether reconciling changes are fault‐fixing changes. This paper contributes to our knowledge of the risks associated with clones and with their evolution over time. Paper 2, ‘Evaluating test‐to‐code traceability recovery methods through controlled experiments’, by Abdallah Qusef, Gabriele Bavota, Rocco Oliveto, Andrea De Lucia, and David Binkley, presents a combination of two controlled experiments comparing the effectiveness of four different approaches to recovering traceability links between test cases and code. Traceability addresses the problem of identifying links between software artifacts of different types, and test‐to‐code traceability addresses the issue of identifying precisely which sections of code are actually tested by a given test case. In the experiments presented in this paper, the effectiveness of the four approaches on three large Java systems is compared using a manual consensus of three PhD student programmers as a reference answer. Both accuracy and practical usefulness to programmers are explored, demonstrating that the authors' own SCOTCH method can be significantly more effective than existing techniques. Paper 3, ‘How developers perform feature location tasks: a human‐centric and process‐oriented exploratory study’, by Jinshui Wang, Xin Peng, Zhenchang Xing, and Wenyun Zhao, describes an empirical study of feature location in code by software developers. Feature location addresses the problem of identifying which parts of the source code of a system are concerned with implementing a specific business or technical concern. Automation in feature location has been a popular topic in the software maintenance research community for some time. In this paper, the authors choose instead to seek insight from how programmers actually perform the task in practice, concluding that the process can be understood at three levels of granularity, each of which is influenced by a number of external factors. The empirical insights reported here can be used to inform future work in supporting and automating this important aspect of software maintenance. We wish to thank the authors for having contributed to this special section, the ICSM 2011 Program Committee for their indications on which papers to include in the special section, and the journal referees, for their detailed and constructive comments, which greatly helped the authors to improve their papers.
Capture-replay vs. programmable web testing: An empirical assessment during test case evolution
There are several approaches for automated functional web testing and the choice among them depends on a number of factors, including the tools used for web testing and the costs associated with their adoption. In this paper, we present an empirical cost/benefit analysis of two different categories of automated functional web testing approaches: (1) capture-replay web testing (in particular, using Selenium IDE); and, (2) programmable web testing (using Selenium WebDriver). On a set of six web applications, we evaluated the costs of applying these testing approaches both when developing the initial test suites from scratch and when the test suites are maintained, upon the release of a new software version. Results indicate that, on the one hand, the development of the test suites is more expensive in terms of time required (between 32% and 112%) when the programmable web testing approach is adopted, but on the other hand, test suite maintenance is less expensive when this approach is used (with a saving between 16% and 51%). We found that, in the majority of the cases, after a small number of releases (from one to three), the cumulative cost of programmable web testing becomes lower than the cost involved with capture-replay web testing and the cost saving gets amplified over the successive releases.
Web testware evolution
Web applications evolve at a very fast rate, to accommodate new functionalities, presentation styles and interaction modes. The test artefacts developed during web testing must be evolved accordingly. Among the other causes, one critical reason why test cases need maintenance during web evolution is that the locators used to uniquely identify the page elements under test may fail or may behave incorrectly. The robustness of web page locators used in test cases is thus critical to reduce the test maintenance effort. We present an algorithm that generates robust web page locators for the elements under test and we describe the design of an empirical study that we plan to execute to validate such robust locators.
Automated oracles: An empirical study on cost and effectiveness
Software testing is an effective, yet expensive, method to improve software quality. Test automation, a potential way to reduce testing cost, has received enormous research attention recently, but the so-called “oracle problem” (how to decide the PASS/FAIL outcome of a test execution) is still a major obstacle to such cost reduction. We have extensively investigated state-of-the-art works that contribute to address this problem, from areas such as specification mining and model inference. In this paper, we compare three types of automated oracles: Data invariants, Temporal invariants, and Finite State Automata. More specifically, we study the training cost and the false positive rate; we evaluate also their fault detection capability. Seven medium to large, industrial application subjects and real faults have been used in our empirical investigation.
Orthogonal exploration of the search space in evolutionary test case generation
The effectiveness of evolutionary test case generation based on Genetic Algorithms (GAs) can be seriously impacted by genetic drift, a phenomenon that inhibits the ability of such algorithms to effectively diversify the search and look for alternative potential solutions. In such cases, the search becomes dominated by a small set of similar individuals that lead GAs to converge to a sub-optimal solution and to stagnate, without reaching the desired objective. This problem is particularly common for hard-to-cover program branches, associated with an extremely large solution space. In this paper, we propose an approach to solve this problem by integrating a mechanism for orthogonal exploration of the search space into standard GA. The diversity in the population is enriched by adding individuals in orthogonal directions, hence providing a more effective exploration of the solution space. To the best of our knowledge, no prior work has addressed explicitly the issue of evolution direction based diversification in the context of evolutionary testing. Results achieved on 17 Java classes indicate that the proposed enhancements make GA much more effective and efficient in automating the testing process. In particular, effectiveness (coverage) was significantly improved in 47% of the subjects and efficiency (search budget consumed) was improved in 85% of the subjects on which effectiveness remains the same.
Automated Generation of State Abstraction Functions using Data Invariant Inference
Model based testing relies on the availability of models that can be defined manually or by means of model inference techniques. To generate models that include meaningful state abstractions, model inference requires a set of abstraction functions as input. However, their specification is difficult and involves substantial manual effort. In this paper, we investigate a technique to automatically infer both the abstraction functions necessary to perform state abstraction and the finite state models based on such abstractions. The proposed approach uses a combination of clustering, invariant inference and genetic algorithms to optimize the abstraction functions along three quality attributes that characterize the resulting models: size, determinism and infeasibility of the admitted behaviors. Preliminary results on a small e-commerce application are extremely encouraging because the automatically produced models include the set of manually defined gold standard models.
Automated generation of state abstraction functions using data invariant inference
Model based testing relies on the availability of models that can be defined manually or by means of model inference techniques. To generate models that include meaningful state abstractions, model inference requires a set of abstraction functions as input. However, their specification is difficult and involves substantial manual effort. In this paper, we investigate a technique to automatically infer both the abstraction functions necessary to perform state abstraction and the finite state models based on such abstractions. The proposed approach uses a combination of clustering, invariant inference and genetic algorithms to optimize the abstraction functions along three quality attributes that characterize the resulting models: size, determinism and infeasibility of the admitted behaviors. Preliminary results on a small e-commerce application are extremely encouraging because the automatically produced models include the set of manually defined gold standard models.
Automated identifier completion and replacement
Various studies indicate that having concise and consistent identifiers improves the quality of the source code and hence impacts positively source code understanding and maintenance. In order to write concise and consistent identifiers, however, developers need to have some knowledge about the concepts captured in the source code and how they are named. Acquiring such knowledge from the source code might be feasible only for small systems, while it is not viable for large systems. In this paper, we propose an automated approach which exploits concepts and relations automatically extracted from the source code to suggest identifiers. The suggestion is ranked based on the context in which a new identifier is introduced and it can be used either to complete the identifier being written or to replace it with a more appropriate one. To validate the proposed approach, we have conducted a case study by simulating the activities of a developer in naming identifiers. The results of the study show that in the majority of the cases our approach provides completion suggestions which match the identifiers actually used by the developers.
Cluster‐based modularization of processes recovered from web applications
Web applications are often used to expose business processes implemented as software systems. This paper describes a technique for recovering business processes based on a dynamic analysis of the applications behavior. The technique described here does not require any access to internal software artifacts of the application, such as source code or documentation. An initial process is inferred to by means of the analysis of execution traces, in which the execution of GUI elements such as forms and links is recorded. The recovered process is then abstracted by clustering its elements according to four different criteria: structural, page‐based, dependency‐based and semantical. A case study has been conducted with the aim of evaluating understandability and readability of the reverse engineered processes as well as the clustering techniques used in refining them. Copyright © 2010 John Wiley & Sons, Ltd.
Search-Based Test Case Generation
http://crest.cs.ucl.ac.uk/fittest/downloads/TAROT-PaoloTonella.pdf
Interactive requirements prioritization using a genetic algorithm
Context The order in which requirements are implemented affects the delivery of value to the end-user, but it also depends on technical constraints and resource availability. The outcome of requirements prioritization is a total ordering of requirements that best accommodates the various kinds of constraints and priorities. During requirements prioritization, some decisions on the relative importance of requirements or the feasibility of a given implementation order must necessarily resort to a human (e.g., the requirements analyst), possessing the involved knowledge. Objective In this paper, we propose an Interactive Genetic Algorithm (IGA) that includes incremental knowledge acquisition and combines it with the existing constraints, such as dependencies and priorities. We also assess the performance of the proposed algorithm. Method The validation of IGA was conducted on a real case study, by comparing the proposed algorithm with the state of the art, interactive prioritization technique Incomplete Analytic Hierarchy Process (IAHP). Results The proposed method outperforms IAHP in terms of effectiveness, efficiency and robustness to decision maker errors. Conclusion IGA produces a good approximation of the reference requirements ranking, requiring an acceptable manual effort and tolerating a reasonable human error rate.
Testing of future internet applications running in the cloud
The cloud will be populated by Future Internet (FI) software, comprising advanced, dynamic and largely autonomic interactions among services, end-user applications, content and media. The complexity of the technologies involved in the cloud makes testing extremely challenging and demands novel approaches and major advancements in the field. This chapter describes the main challenges associated with the test of FI applications running in the cloud. We present a research agenda that was defined in order to address the FI testing challenges. The goal of the agenda is investigating the technologies for the development of a FI automated testing environment, which can monitor the FI applications under test and can react dynamically to the observed changes. Realization of this environment involves substantial research in areas such as search based testing, model inference, oracle learning and anomaly detection.
Revolution: Automatic evolution of mined specifications
Specifications mined from execution traces are largely used to support testing and analysis of software applications with little runtime variability. However, when models are mined from applications that evolve at runtime, the resulting models become quickly obsolete, and thus of little support for any testing and analysis activity. To cope with such systems, mined specifications must be consistently updated every time the software changes. In principle, models can be periodically mined from scratch, but in many cases this solution is too expensive or even impossible. In this paper we describe Revolution, an approach for the automatic evolution of specifications mined by applying state abstraction techniques. Revolution produces models that are continuously updated and thus remain aligned with the actual implementation. Empirical results show that Revolution can suitably address run-time evolving applications.
Can lexicon bad smells improve fault prediction?
In software development, early identification of fault-prone classes can save a considerable amount of resources. In the literature, source code structural metrics have been widely investigated as one of the factors that can be used to identify faulty classes. Structural metrics measure code complexity, one aspect of the source code quality. Complexity might affect program understanding and hence increase the likelihood of inserting errors in a class. Besides the structural metrics, we believe that the quality of the identifiers used in the code may also affect program understanding and thus increase the likelihood of error insertion. In this study, we measure the quality of identifiers using the number of Lexicon Bad Smells (LBS) they contain. We investigate whether using LBS in addition to structural metrics improves fault prediction. To conduct the investigation, we assess the prediction capability of a model while using i) only structural metrics, and ii) structural metrics and LBS. The results on three open source systems, ArgoUML, Rhino, and Eclipse, indicate that there is an improvement in the majority of the cases.
Evolutionary testing of autonomous software agents
A system built in terms of autonomous software agents may require even greater correctness assurance than one that is merely reacting to the immediate control of its users. Agents make substantial decisions for themselves, so thorough testing is an important consideration. However, autonomy also makes testing harder; by their nature, autonomous agents may react in different ways to the same inputs over time, because, for instance they have changeable goals and knowledge. For this reason, we argue that testing of autonomous agents requires a procedure that caters for a wide range of test case contexts, and that can search for the most demanding of these test cases, even when they are not apparent to the agents’ developers. In this paper, we address this problem, introducing and evaluating an approach to testing autonomous agents that uses evolutionary optimisation to generate demanding test cases. We propose a methodology to derive objective (fitness) functions that drive evolutionary algorithms, and evaluate the overall approach with two simulated autonomous agents. The obtained results show that our approach is effective in finding good test cases automatically.
Combining model-based and combinatorial testing for effective test case generation
Model-based testing relies on the assumption that effective adequacy criteria can be defined in terms of model coverage achieved by a set of test paths. However, such test paths are only abstract test cases and input test data must be specified to make them concrete. We propose a novel approach that combines model-based and combinatorial testing in order to generate executable and effective test cases from a model. Our approach starts from a finite state model and applies model-based testing to generate test paths that represent sequences of events to be executed against the system under test. Such paths are transformed to classification trees, enriched with domain input specifications such as data types and partitions. Finally, executable test cases are generated from those trees using t-way combinatorial criteria. While test cases that satisfy a combinatorial criterion can be generated for each individual test path obtained from the model, we introduce a post-optimization algorithm that can guarantee the combinatorial criterion of choice on the whole set of test paths extracted from the model. The resulting test suite is smaller, but it still satisfies the same adequacy criterion. We developed a tool and used it to evaluate our approach on 6 subject systems of various types and sizes, to study the effectiveness of the generated test suites, the reduction achieved by the post-optimization algorithm, as well as the effort required to produce them.
An empirical study about the effectiveness of debugging when random test cases are used
Automatically generated test cases are usually evaluated in terms of their fault revealing or coverage capability. Beside these two aspects, test cases are also the major source of information for fault localization and fixing. The impact of automatically generated test cases on the debugging activity, compared to the use of manually written test cases, has never been studied before. In this paper we report the results obtained from two controlled experiments with human subjects performing debugging tasks using automatically generated or manually written test cases. We investigate whether the features of the former type of test cases, which make them less readable and understandable (e.g., unclear test scenarios, meaningless identifiers), have an impact on accuracy and efficiency of debugging. The empirical study is aimed at investigating whether, despite the lack of readability in automatically generated test cases, subjects can still take advantage of them during debugging.
Crawlability metrics for web applications
Automated web crawlers can be used to explore and exercise portions of a web application under test. However, the possibility to achieve full exploration of a web application through automated crawling is severely limited by the choice of the input values submitted with forms. Depending on the crawler's capabilities, a larger or smaller portion of web application will be automatically explored. In this paper, we introduce web crawl ability metrics to quantify properties of application pages and forms that affect crawl ability. Moreover, we show that our metrics can be used to identify the boundaries between those parts of the application that can be successfully crawled automatically and those parts that will require manual intervention or other crawl ability support. We have validated our crawl ability metrics on real web applications, for which low crawl ability was indeed associated with the existence of pages never exercised during automated crawling.
Finding the optimal balance between over and under approximation of models inferred from execution logs
Models inferred from execution traces (logs) may admit more behaviours than those possible in the real system (over-approximation) or may exclude behaviours that can indeed occur in the real system (under-approximation). Both problems negatively affect model based testing. In fact, over-approximation results in infeasible test cases, i.e., test cases that cannot be activated by any input data. Under-approximation results in missing test cases, i.e., system behaviours that are not represented in the model are also never tested. In this paper we balance over- and under-approximation of inferred models by resorting to multi-objective optimization achieved by means of two search-based algorithms: A multi-objective Genetic Algorithm (GA) and the NSGA-II. We report the results on two open-source web applications and compare the multi-objective optimization to the state-of-the-art KLFA tool. We show that it is possible to identify regions in the Pareto front that contain models which violate fewer application constraints and have a higher bug detection ratio. The Pareto fronts generated by the multi-objective GA contain a region where models violate on average 2% of an application's constraints, compared to 2.8% for NSGA-II and 28.3% for the KLFA models. Similarly, it is possible to identify a region on the Pareto front where the multi-objective GA inferred models have an average bug detection ratio of 110 : 3 and the NSGA-II inferred models have an average bug detection ratio of 101 : 6. This compares to a bug detection ratio of 310928 : 13 for the KLFA tool.
SPECIAL SECTION ON THE INTERNATIONAL SYMPOSIUM ON SOFTWARE TESTING AND ANALYSIS
Presents the front cover/table of contents for this issue of the periodical.
Guest Editorial: Special Section on the International Symposium on Software Testing and Analysis 2010
THE International Symposium on Software Testing and Analysis (ISSTA) is the premier forum for the presentation of leading edge research results on issues related to software testing and analysis. Every year, ISSTA brings together academics, industrial researchers, and practitioners to exchange new ideas, problems, and experience on how to analyze and test software systems. ISSTA 2010 was held from 12 to 16 July 2010 at the center for scientific and technologic research of the Fondazione Bruno Kessler (FBK) in Trento, a beautiful historical city in the middle of the Italian Alps. The conference was preceded by four workshops and two tutorials on various themes related to software testing and analysis. ISSTA 2010 attracted more than 100 submissions, each of which was evaluated by at least three members of the ISSTA Program Committee and discussed in the Program Committee meeting. The result was a high-quality technical program with 23 accepted research papers that cover a variety of topics, including formal verification, symbolic execution, test input generation, debugging, and concurrency testing and analysis. This special section of the IEEE Transactions on Software Engineering contains five papers selected by the ISSTA Program Committee among the best papers presented at the conference. The papers, suitably revised, enhanced, and extended by the authors, went through the standard TSE review process—they were reviewed by three anonymous referees, and the process was overseen by the guest editors. We are delighted to present you the five excellent papers that were the results of this effort. In the paper “Automatically Generating Test Cases for Specification Mining,” Valentin Dallmeier, Nikolai Knopp, Christoph Mallon, Sebastian Hack, Gordon Fraser, and Andreas Zeller present an improvement of dynamic specification mining, a technique to infer models of the normal program behavior based on observed executions. The improved technique generates test inputs that cover previously unobserved behaviors and systematically extends the execution space, thus enriching the mined specification. The empirical evaluation of the approach shows its effectiveness on a set of real-world Java programs. The paper “Random Testing: Theoretical Results and Practical Implications,” by Andrea Arcuri, Zohaib Iqbal, and Lionel Briand, provides a rigorous discussion of random testing, its benefits and drawbacks. The authors also address, both theoretically and through simulations, several general questions about the efficiency, effectiveness, scalability, and predictability of random testing techniques. Finally, the authors use their results to assess the validity of empirical analyses reported in the literature and derive guidelines for practitioners and researchers interested in using random testing. In the paper “Mutation-Driven Generation of Unit Tests and Oracles,” Gordon Fraser and Andreas Zeller present an automated approach to generating unit tests, including the associated oracles, that are specifically targeted at detecting mutations of object-oriented classes. In this way, their approach produces test suites that are optimized toward finding defects, rather than covering the code. An evaluation of the approach performed on several open source libraries shows that the approach can generate test suites that find significantly more seeded defects than manually written test suites. The paper “Automatic Detection of Unsafe Dynamic Component Loadings,” by Taeho Kwon and Zhendong Su, targets code vulnerabilities that may lead to unintended, or even malicious, components to be loaded at run time. To detect and eliminate such vulnerabilities, the authors perform an analysis based on runtime information collected through binary instrumentation and analyzed to detect vulnerable component loadings. In their evaluation of the approach, the authors show that it can detect vulnerable and unsafe component loadings in popular software running under both Microsoft Windows and Linux. In the paper “Fault Localization for Dynamic Web Applications,” Shay Artzi, Julian Dolby, Frank Tip, and Marco Pistoia present a novel approach for locating faults in modern web applications, combining test generation and fault localization. Specifically, the approach extends existing fault-localization algorithms for use on web applications written in PHP and leverages several test-generation strategies with the aim of maximizing their fault-localization effectiveness. The empirical evaluation of the approach, performed on several open-source PHP applications, shows that the test suites generated by the approach exhibit high fault-localization effectiveness.
Reajax: a reverse engineering tool for ajax web applications
In contrast to conventional multi-page Web applications, an Ajax application is often developed as a single-page application in which content and structure are changed at runtime according to user interactions, asynchronous messages received from the server and the current state of the application. These features make Ajax applications quite hard to understand for programmers. The authors propose to support Ajax comprehension through reverse engineering. In this study, the authors propose a reverse-engineering tool, ReAjax, to build GUI-based state models from Ajax applications. ReAjax applies dynamic analysis and uses execution traces to generate a finite state machine of the target application GUI. They show that GUI-based state models obtained semi-automatically are similar to those obtained manually and they can be used for program understanding purposes. Finally, the authors summarise a case study and some usage scenarios in which ReAjax has been applied to five real Ajax applications with the purpose of evaluating its viability and effectiveness in recovering models.
Model-driven development (MDD) is a software engineering discipline which suggests that software development should be done at the modelling level and that ap...
No abstract available.
Semantics-based aspect-oriented management of exceptional flows in business processes
Enriching business process models with semantic annotations that are taken from an ontology has become a crucial need in service provisioning, integration and composition, and business processes management. We represent semantically annotated business processes as part of an Web ontology lanuage knowledge base that formalizes the business process structure, the business domain, a set of criteria that describe correct semantic annotations, and a set of constraints that describe requirements on the business process itself. In this paper, we show how the Semantic Web representation and reasoning techniques can be 1) exploited by our aspect-oriented approach to modularize exception-handling (as well as other crosscutting) mechanisms and 2) effectively applied to formalize and automatically verify constraints on the management of exceptional flows (as well as other relevant flows) in business processes. The benefits of the Semantic Web and the aspect-oriented technologies are illustrated in a case study, where exceptional flows are modularized separately and managed at the semantic level due to the proposed approach.
Crosscutting concern mining in business processes
One of the most complex tasks for business analysts is the consistent management of crosscutting concerns in large business processes. By crosscutting concerns the authors mean those process features that are not assigned to a single modular unit in the process and are thus scattered and tangled with other features. For example, the adaptation of the process to the user preferences involves several scattered activities that constitute a crosscutting concern. In this study, the authors propose a semi-automatic approach, based on formal concept analysis, for the identification of crosscutting concerns in business processes at design time. A preliminary evaluation of the effectiveness of the proposed technique in detecting crosscutting concerns has been provided by applying the approach to a case study.
Symbolic search-based testing
We present an algorithm for constructing fitness functions that improve the efficiency of search-based testing when trying to generate branch adequate test data. The algorithm combines symbolic information with dynamic analysis and has two key advantages: It does not require any change in the underlying test data generation technique and it avoids many problems traditionally associated with symbolic execution, in particular the presence of loops. We have evaluated the algorithm on industrial closed source and open source systems using both local and global search-based testing techniques, demonstrating that both are statistically significantly more efficient using our approach. The test for significance was done using a one-sided, paired Wilcoxon signed rank test. On average, the local search requires 23.41% and the global search 7.78% fewer fitness evaluations when using a symbolic execution based fitness function generated by the algorithm.
Towards the extraction of domain concepts from the identifiers
Program identifiers represent an invaluable source of information for developers who are not familiar with the code to be evolved. Domain concepts and inter-concept relationships can be automatically extracted by means of natural language processing techniques applied to the program identifiers. However, the ontology produced by this approach tends to be very large and to include implementation details that reduce its usefulness for domain concept understanding. In this paper, we analyze the effectiveness of information retrieval based techniques used to filter domain concepts and relations from the implementation details, so as to obtain a smaller, more informative domain ontology. In particular, we show that fully automated techniques based on keywords or topics have quite poor performance, while a semi-automated approach, requiring limited user involvement, can highly improve the filtering of domain concepts.
The effect of lexicon bad smells on concept location in source code
Experienced programmers choose identifier names carefully, in the attempt to convey information about the role and behavior of the labeled code entity in a concise and expressive way. In fact, during program understanding the names given to code entities represent one of the major sources of information used by developers. We conjecture that lexicon bad smells, such as, extreme contractions, inconsistent term use, odd grammatical structure, etc., can hinder the execution of maintenance tasks which rely on program understanding. We propose an approach to determine the extent of this impact and instantiate it on the task of concept location. In particular, we conducted a study on two open source software systems where we investigated how lexicon bad smells affect Information Retrieval-based concept location. In this study, the classes changed in response to past modification requests are located before and after lexicon bad smells are identified and removed from the source code. The results indicate that lexicon bad smells impact concept location when using IR-based techniques.
Optimizing the trade-off between complexity and conformance in process reduction
While models are recognized to be crucial for business process management, often no model is available at all or available models are not aligned with the actual process implementation. In these contexts, an appealing possibility is recovering the process model from the existing system. Several process recovery techniques have been proposed in the literature. However, the recovered processes are often complex, intricate and thus difficult to understand for business analysts. In this paper, we propose a process reduction technique based on multi-objective optimization, which at the same time minimizes the process complexity and its non-conformances. This allows us to improve the process model understandability, while preserving its completeness with respect to the core business properties of the domain. We conducted a case study based on a real-life e-commerce system. Results indicate that by balancing complexity and conformance our technique produces understandable and meaningful reduced process models.
Using an SMT solver for interactive requirements prioritization
The prioritization of requirements is a crucial activity in the early phases of the software development process. It consists of finding an order relation among requirements, considering several requirements characteristics, such as stakeholder preferences, technical constraints, implementation costs and user perceived value. We propose an interactive approach to the problem of prioritization based on Satisfiability Modulo Theory (SMT) techniques and pairwise comparisons. Our approach resorts to interactive knowledge acquisition whenever the relative priority among requirements cannot be determined based on the available information. Synthesis of the final ranking is obtained via SMT constraint solving. The approach has been evaluated on a set of requirements from a real healthcare project. Results show that it overcomes other interactive state-of-the-art prioritization approaches in terms of effectiveness, efficiency and robustness to decision maker errors.
Test case prioritization for audit testing of evolving web services using information retrieval techniques
Web services evolve frequently to meet new business demands and opportunities. However, service changes may affect service compositions that are currently consuming the services. Hence, audit testing (a form of regression testing in charge of checking for compatibility issues) is needed. As service compositions are often in continuous operation and the external services have limited (expensive) access when invoked for testing, audit testing has severe time and resources constraints, which make test prioritization a crucial technique (only the highest priority test cases will be executed).This paper presents a novel approach to the prioritization of audit test cases using information retrieval. This approach matches a service change description with the code portions exercised by the relevant test cases. So, test cases are prioritized based on their relevance to the service change. We evaluate the proposed approach on a system that composes services from eBay and Google.
A framework for the collaborative specification of semantically annotated business processes
Semantic annotations are a way to provide a precise meaning to business process elements, which supports reasoning on properties and constraints. Among the obstacles preventing widespread adoption of semantic annotations are the technical skills required to manage the formalization of the semantics and the difficulty of reconciling the different viewpoints of different analysts working on the same business process. In this paper, we support business analysts in the collaborative annotation of business processes by means of a tool inspired to the Wiki pages model. Using this tool, analysts can concurrently work on process elements, ontology concepts, process annotation or constraint specification. The underlying formalism is not exposed in the Wiki pages, where natural language templates are used. Copyright © 2011 John Wiley & Sons, Ltd.
Towards testing future web applications
The current Web applications are in continuous evolution to provide new and more complex functionalities, which can improve the user experience by means of adaptivity and dynamic changes. Since testing is the most frequently used technique to evaluate the quality of software applications in industry, novel testing approaches will be necessary to evaluate the quality of future (and more complex) web applications. In this paper, we investigate the testing challenges of future web applications and propose a testing methodology that addresses these challenges by the integration of search-based testing, model-based testing, oracle learning, concurrency testing, combinatorial testing, regression testing, and coverage analysis. This paper also presents a testing metamodel that states testing concepts and their relationships, which are used as the theoretical basis of the proposed testing methodology.
Crawlability metrics for automated web testing
Web applications are exposed to frequent changes both in requirements and involved technologies. At the same time, there is a continuously growing demand for quality and trust and such a fast evolution and quality constraints claim for mechanisms and techniques for automated testing. Web application automated testing often involves random crawlers to navigate the application under test and automatically explore its structure. However, owing to the specific challenges of the modern Web systems, automatic crawlers may leave large portions of the application unexplored. In this paper, we propose the use of structural metrics to predict whether an automatic crawler with given crawling capabilities will be sufficient or not to achieve high coverage of the application under test. In this work, we define a taxonomy of such capabilities and we determine which combination of them is expected to give the highest reward in terms of coverage increase. Our proposal is supported by an experiment in which 19 web applications have been analyzed.
Challenges in audit testing of web services
Audit testing of Web services is a form of regression testing that applies to the integration of external services in a service composition. When such services change, the application that integrates them needs to be regressiontested, to ensure that the existing functionalities are not broken by the service changes. This means the external services must be audited for preserved compatibility with the integration. In this paper, we analyze the main research challenges behind audit testing of services. Such challenges descend from the limited testability of external services and encompass traditional regression testing problems, such as test case selection and prioritization, for which existing solutions are nonetheless inadequate. Moreover, in the paper we discuss in detail one such challenge, i.e., the problem of defining an oracle by making the integrator's assumptions explicit.
Automated detection of discontinuities in models inferred from execution traces
Modern applications (e.g., the so called Future Internet applications) exhibit properties that make them hard to model once for all. In fact, they dynamically adapt to the user's habits, to the context, to the environment, they dynamically discover new services and components to integrate, they modify themselves through reflection, automatically. Model inference techniques are based on the observation of the application behavior (trace collection) and on its generalization into a model. Model inference supports testing, understanding and evolution of the software. However, inferred models may become obsolete at run time, due to the evolution or the self-modifications of the software. We investigate an approach for the automated detection of model discontinuities, based on a trade off between delay of the detection and accuracy, measured in terms of few false negatives.
Change sensitivity based prioritization for audit testing of webservice compositions
Modern software systems have often the form of Web service compositions. They take advantage of the availability of a variety of external Web services to provide rich and complex functionalities, obtained as the integration of external services. However, Web services change at a fast pace and while syntactic changes are easily detected as interface incompatibilities, other more subtle changes are harder to detect and may give raise to faults. They occur when the interface is compatible with the composition, but the semantics of the service response has changed. This typically involves undocumented or implicit aspects of the service interface. Audit testing of services is the process by which the service integrator makes sure that the service composition continues to work properly with the new versions of the integrated services. Audit testing of services is conducted under strict (sometimes extreme) time and budget constraints. Hence, prioritizing the audit test cases so as to execute the most important ones first becomes of fundamental importance. We propose a test case prioritization method specifically tailored for audit testing of services. Our method is based on the idea that the most important test cases are those that have the highest sensitivity to changes injected into the service responses (mutations). In particular, we consider only changes that do not violate the explicit contract with the service (i.e., the WSDL), but may violate the implicit assumptions made by the service integrator.
Future internet testing with fittest
The complexity of the technologies involved in the Future Internet makes testing extremely challenging and demands for novel approaches and major advancement in the field. The overall aim of the FITTEST project is to address these testing challenges, by developing an integrated environment for automated testing, which can monitor the Future Internet application under test and adapt to the dynamic changes observed. Future Internet applications do not remain fixed after their release, services and components can be dynamically added by customers. Consequently, FITTEST testing will be continuous and post-release such that maintenance and quality assurance can cope with the changes in the intended use of an application after release. The testing environment will integrate, adapt and automate various techniques for continuous Future Internet testing (dynamic model inference, model-based testing, log-based diagnosis, oracle learning, combinatorial testing, concurrent testing, regression testing).
Codebender: Remote software protection using orthogonal replacement
CodeBender implements a novel client replacement strategy to counter the malicious host problem and address the problem of guaranteeing client-code security. CodeBender is a tool that implements a novel client-replacement strategy to counter the malicious host problem. It works by limiting the client code's validity and, when the code expires, by having the server provide a new client that replaces the former one. The complexity of analyzing frequently changing, always different (orthogonal) program code deters an adversary's reverse engineering efforts. We've implemented CodeBender and tested its practicability in two case studies.
Using search-based algorithms for Ajax event sequence generation during testing
Modern Web applications offer a rich and unique user experience by taking advantages of the so called Web 2.0 technologies, among which Ajax. Ajax supports the intensive use of asynchronous communication between client-pages and the Web server and it allows on-the-fly manipulations of client-pages content and structure to realize a rich, dynamic and interactive user interface. Correspondingly, new types of faults that cannot be easily revealed by existing Web testing techniques are associated with modern Ajax-based applications. In our previous investigations, we used state-based testing for event sequence generation and it proved to be quite effective in exposing Ajax specific faults. However, the search space of the semantically interacting event sequences is huge, as it can grow exponentially with the event sequence length. In this paper, we apply search-based algorithms, namely hill climbing and simulated annealing, to the problem of generating maximally diverse event sequences of various lengths. In this way, we control the size of the generated test suites, while keeping the included test cases as diverse as possible. We evaluate the performance of the algorithms on two open source Ajax applications.
Automated Detection of Discontinuities in Models Inferred from Execution Traces
Modern applications (e.g., the so called Future Internet applications) exhibit properties that make them hard to model once for all. In fact, they dynamically adapt to the user's habits, to the context, to the environment, they dynamically discover new services and components to integrate, they modify themselves through reflection, automatically. Model inference techniques are based on the observation of the application behavior (trace collection) and on its generalization into a model. Model inference supports testing, understanding and evolution of the software. However, inferred models may become obsolete at run time, due to the evolution or the self-modifications of the software. We investigate an approach for the automated detection of model discontinuities, based on a trade off between delay of the detection and accuracy, measured in terms of few false negatives.
Bpmn visual rule language for business process aspectization
Beyond the process workflow itself, business processes usually involve several other concerns, often scattered across the whole process and tangled with the main view. Though allowing to represent classic process perspectives, existing business process modelling languages do not provide any constructs to describe these concerns. We consider the use of aspects to cope with this lacking capacity of process languages. An aspect is a module that encapsulates a secondary behaviour (e.g, the exception handling) of a main view (e.g., the “happy path” of a process flow), thus allowing designers to manipulate it independently from the main view. For the definition of aspects in BPMN processes, we propose a visual language extending the BPMN with rules allowing the realization of aspect-oriented mechanisms..
Static analysis for enforcing intra-thread consistent locks in the migration of a legacy system
Often, legacy data management systems provide no native support to transactions. Programmers protect data from concurrent access by adopting commonly agreed patterns, relying on low level concurrency primitives, such as semaphores. In such cases, consistent data access is granted only if all code components are compliant with the adopted mutual exclusion patterns.
Keynote Speakers
Summary form only given. Search based software engineering is not necessarily going to replace existing heuristics and solutions to software engineering problems. In fact, the search based framework is flexible enough to allow for smooth integration with several of the existing techniques. Hence, hybridization of search based algorithms with available methods has the potential of a win-win alliance, where the strengths of the various approaches get amplified by the union. Future research in search based software engineering should investigate in depth the challenging opportunities offered by such hybridization.
Using interactive GA for requirements prioritization
The order in which requirements are implemented in a system affects the value delivered to the final users in the successive releases of the system. Requirements prioritization aims at ranking the requirements so as to trade off user priorities and implementation constraints, such as technical dependencies among requirements and necessarily limited resources allocated to the project. Requirement analysts possess relevant knowledge about the relative importance of requirements. We use an Interactive Genetic Algorithm to produce a requirement ordering which complies with the existing priorities, satisfies the technical constraints and takes into account the relative preferences elicited from the user. On a real case study, we show that this approach improves non interactive optimization, ignoring the elicited preferences, and that it can handle a number of requirements which is otherwise problematic for state of the art techniques.
Natural language parsing of program element names for concept extraction
To support programmers during program maintenance we present an approach which extracts concepts and relations from the source code. Our approach applies natural language parsing to sentences constructed from the terms that appear in program element identifiers. The result of parsing can be represented as a dependency tree. Then, we automatically extract an ontology by mapping linguistic entities (nodes and relations between nodes in the dependency tree) to concepts and relations among concepts. We applied our approach to a case study and assessed the result in terms of the support it can give to concept location, executed in the context of bug fixing.
A design methodology for real services
As we observe in practice, software services act mostly as interface components that provide electronic access to the actual services behind, and the characteristics of these two types of services are very different. The fact that the user will eventually use or take part in actual services motivates us to shift our focus from software services to a more holistic view in order to better design user-centric services. In this paper we discuss the real service concept, which is composed of software and actual services, and the important factors regarding them. We, then, propose a methodology to support the designer in specifying real services and their design artifacts. Finally, we apply the proposed methodology to a motivating example and discuss its advantages as well as some open issues in light of our experience.
Migrating legacy data structures based on variable overlay to Java
Legacy information systems, such as banking systems, are usually organized around their data model. Hence, when these systems are migrated to modern environments, translation of the data model involves the most critical decisions, having strong implications on the rest of the translation. In this paper, we report our experience and describe the approaches adopted in migrating a large banking system (ten million lines of code) to Java, starting from a proprietary data model which gives programmers explicit control of the variable overlay in memory. After presenting the basic translation scheme, we discuss the exceptions that may occur in practice. Then, we consider two heuristic approaches useful to reduce the number of cases where a behavior equivalent to that of unions must be reproduced in Java. Finally, we comment on the experimental results obtained so far. Copyright © 2009 John Wiley & Sons, Ltd.
Under and over approximation of state models recovered for ajax applications
In contrast to conventional multi-page Web applications, an Ajax application is developed as a single-page application in which content and structure are changed at runtime according to user interactions, asynchronous messages received from the server and the current state of the application. These features make Ajax applications quite hard to understand for programmers. In this paper, we summarize an approach for supporting Ajax comprehension by recovering GUI-based state models of Ajax applications. Furthermore, we present a case study in which the model recovery approach has been assessed in terms of under and over approximation.
Empirical comparison of graphical and annotation-based re-documentation approaches
Re-documentation is a complex activity that follows the comprehension of the code. Programmers record the knowledge they have gained in the form of text, views and diagrams that address specific aspects of the system under maintenance. Re-documentation of existing software can be achieved in several ways. The authors focus on two commonly used approaches: either using a drawing editor or annotating the source code. In the first case, diagrams are produced interactively, starting from the reverse engineered information. In the second case, design information is added in the form of code annotations. Diagrams may be produced, if needed, by an annotation-processing tool, which interprets the annotations previously inserted into the code and generates graphical views. The aim of this empirical work is the comparison of these two approaches, in order to understand which is easier to use and which the current limitations of both of them are. Preliminary results with master students indicate the drawing editor approach as the most preferred and usable, with no penalty on the quality of the resulting diagrams and on the effort required.
The difficulties of software quality evaluation found during our activity in different projects and publications led us to investigate a systematic method for building dom...
No abstract available.
Codebender: a tool for remote software protection using orthogonal replacement
CodeBender implements a novel client replacement strategy to counter the malicious host problem and address the problem of guaranteeing client-code security. CodeBender is a tool that implements a novel client-replacement strategy to counter the malicious host problem. It works by limiting the client code's validity and, when the code expires, by having the server provide a new client that replaces the former one. The complexity of analyzing frequently changing, always different (orthogonal) program code deters an adversary's reverse engineering efforts. We've implemented CodeBender and tested its practicability in two case studies.
Why testing autonomous agents is hard and what can be done about it
Intuitively, autonomous agents are hard to test. Their autonomy and their flexible, context-aware behaviour implies unpredictability, and their social ability intuitively leads to unpredictable emergent behaviour. On the other hand, software agents are programs just like those designed and developed in an object-oriented, procedural fashion, or under any other approach, and just because something is intuitive does not mean it is necessarily correct. In their separate respective groups, the authors of this position paper have been examining exactly why agents may be harder to test than the kinds of software we would expect to be developed without an agentoriented approach, quantifying this difficulty, and/or proposing means to address it. This position paper summarises and expands on our groups’ work for the purposes of discussion. While we are aware that there are other relevant approaches to tackling agent testing by other groups (including some presented at the Agent-Oriented Software Engineering workshop series), we do not attempt to give a state of the art survey here, but instead refer to a forthcoming survey of this area by Nguyen et al. [12].
Automock: Automated synthesis of a mock environment for test case generation
During testing, there are several reasons to exclude some of the components used by the unit under test, such as: (1) the component affects the state of the world in an irreversible way; (2) the component is not accessible for testing purposes (e.g., a web service); (3) the component introduces a major performance degradation to the testing phase (e.g., due to long computations); (4) it is hard (i.e., statistically unlikely) to obtain the output required by the test from the component. In such cases, we replace the component with a mock one. In this paper, we integrate the synthesis of mock components with the generation of test cases for the current testing goal (e.g., coverage). To avoid the generation of meaningless data, which may lead to assertion violation not related to bugs, we include a weak mock postcondition. We consider ways to automatically synthesize such postcondition. We empirically evaluate the quality of the mocks generated by our approach, as well as the benefits mocks introduce in terms of improved coverage and improved performance of the test case generator.
A bit of" Persona", a bit of" Goal", a bit of" Process"... a recipe for Analyzing User Intensive Software Systems.
The centrality of users in the design and development of complex systems, such as service-based applications, calls for new methodologies and techniques to extract and represent user needs and to translate them into real processes. In this short paper, we describe the integration of concepts and analysis techniques of different approaches, namely Goal-Oriented Requirements Engineering, User-Centred Design and Process-Oriented Modeling, that are being developed in the context of two projects related to Ambient Assisted Living and Internet of Services.
How developers' experience and ability influence web application comprehension tasks supported by uml stereotypes: A series of four experiments
In recent years, several design notations have been proposed to model domain-specific applications or reference architectures. In particular, Conallen has proposed the UML Web Application Extension (WAE): a UML extension to model Web applications. The aim of our empirical investigation is to test whether the usage of the Conallen notation supports comprehension and maintenance activities with significant benefits, and whether such benefits depend on developers ability and experience. This paper reports and discusses the results of a series of four experiments performed in different locations and with subjects possessing different experience-namely, undergraduate students, graduate students, and research associates-and different ability levels. The experiments aim at comparing performances of subjects in comprehension tasks where they have the source code complemented either by standard UML diagrams or by diagrams stereotyped using the Conallen notation. Results indicate that, although, in general, it is not possible to observe any significant benefit associated with the usage of stereotyped diagrams, the availability of stereotypes reduces the gap between subjects with low skill or experience and highly skilled or experienced subjects. Results suggest that organizations employing developers with low experience can achieve a significant performance improvement by adopting stereotyped UML diagrams for Web applications.
Goal-oriented testing for MASs
As Multi-Agent Systems (MASs) are increasingly applied in complex distributed applications such as financial and healthcare services, assurance needs to be given to the user that the implemented MASs operate properly, i.e., they meet their specifications and the stakeholder's expectations. Testing is an important and widely applied technique in practice to reach this goal. Current Agent-Oriented Software Engineering (AOSE) methodologies address testing only partially. Some of them exploit Object-Oriented (OO) testing techniques, based on a mapping of agent-oriented abstractions into OO constructs; this may require additional development effort and introduce anomalies. Moreover, a structured testing process for AOSE methodologies is still missing. In this paper we introduce a testing methodology, called Goal-Oriented Software Testing (GOST), for MAS. The methodology specifies a testing process that complements goal-oriented analysis and design. Furthermore, GOST provides a systematic way of deriving test cases from goal-oriented specifications and techniques to automate test case generation and their execution.
An empirical validation of a web fault taxonomy and its usage for web testing
Web testing is assuming an increasingly important role in Web engineering, as a result of the quality demands put onto modern Web-based systems and of the complexity of the involved technologies. Most of the existing works in Web testing are focused on the definition of novel testing techniques, while only limited effort was devoted to understanding the specific nature of Web faults. However, the performance of a new Web testing technique is strictly dependent on the classes of Web faults it addresses. In this paper, we describe the process followed in the construction of a Web fault taxonomy. We used an iterative, mixed top-down and bottom-up approach. An initial taxonomy was defined by analyzing the high level characteristics of Web applications. Then the taxonomy was subjected to several iterations of empirical validation. During each iteration the taxonomy was refined by analyzing real faults and mapping them onto the appropriate categories. Metrics collected during this process were used to ensure that in the final taxonomy bugs distribute quite evenly among fault categories; fault categories are not-too-big, not-too-small and not ambiguous. Testers can use our taxonomy to define test cases that target specific classes of Web faults, while researchers can use it to build fault seeding tools, to inject artificial Web faults into benchmark applications. The final taxonomy is publicly available for consultation: since it is organized as a Wiki page, it is also open to external contributions. We conducted a case study in which test cases have been derived from the taxonomy for a sample Web application. The case study indicates that the proposed taxonomy is very effective in directing the testing effort toward those test scenarios that have higher chances of revealing Web specific faults.
Semantically-aided business process modeling
Enriching business process models with semantic annotations taken from an ontology has become a crucial necessity both in service provisioning, integration and composition, and in business processes management. In our work we represent semantically annotated business processes as part of an OWL knowledge base that formalises the business process structure, the business domain, and a set of criteria describing correct semantic annotations. In this paper we show how Semantic Web representation and reasoning techniques can be effectively applied to formalise, and automatically verify, sets of constraints on Business Process Diagrams that involve both knowledge about the domain and the process structure. We also present a tool for the automated transformation of an annotated Business Process Diagram into an OWL ontology. The use of the semantic web techniques and tool presented in the paper results in a novel support for the management of business processes in the phase of process modeling, whose feasibility and usefulness will be illustrated by means of a concrete example.
Lexicon bad smells in software
We introduce the notion of "lexicon bad smell", which parallels that of "code smell" and indicates some potential lexicon construction problems that can be addressed through refactoring (e.g., renaming). We created a catalog of lexicon bad smells and we developed a publicly available suite of detectors to locate them. The paper presents a case study in which we used the detectors on two open-source systems. The study revealed the main challenges faced in detecting the lexicon bad smells.
Recovering structured data types from a legacy data model with overlays
Legacy systems are often written in programming languages that support arbitrary variable overlays. When migrating to modern languages, the data model must adhere to strict structuring rules, such as those associated with an object oriented data model, supporting classes, class attributes and inter-class relationships. In this paper, we deal with the problem of automatically transforming a data model which lacks structure and relies on the explicit layout of variables in memory as defined by programmers. We introduce an abstract syntax and a set of abstract rewrite rules to describe the proposed approach in a language neutral formalism. Then, we instantiate the approach for the proprietary programming language that was used to develop a large legacy system we are migrating to Java.
Improving web application testing using testability measures
One of the challenges of testing web applications derives from their dynamic content and structure. As we test a website, we may discover more about its structure and behaviour. This paper proposes a framework for collection of testability measures during the automated testing process (termed ‘in-testing’ measure collection). The measures gathered in this way can take account of dynamic and content driven aspects of web applications, such as form structure, client-side scripting and server-side code. Their goal is to capture measurements related to on-going testing activity, indicating where additional testing can best lead to higher overall coverage. They denote a form of ‘web testability’ measures. The paper reports on the implementation of a prototype Web Application Testing Tool, WATT, illustrating the in-testing measure collection approach on 34 forms taken from 14 real world web applications.
Collaborative specification of semantically annotated business processes
Semantic annotations are a way to provide a precise meaning to business process elements, which supports reasoning on properties and constraints. The specification and annotation of business processes is a complex activity involving different analysts possibly working on the same business process. In this paper we present a framework which aims at supporting business analysts in the collaborative specification and annotation of business processes. A shared workspace, theoretically grounded in a formal representation, allows to collaboratively manipulate processes, ontologies as well as constraints, while a dedicated tool enables to hide the complexity of the underlying formal representation to the users.
Dynamic aspect mining
Legacy systems often contain several crosscutting concerns that could potentially benefit from an aspect-oriented programming implementation. In this paper, we focus on the problem of aspect identification in existing code. The main idea is that use-cases can be defined in order to separate the base logics from the crosscutting concerns to be aspectised. The relationship between the execution traces associated with the use-cases and the executed computational units (class methods) is analysed through concept analysis. The results obtained on some case studies are discussed in the paper.
Clustering test cases to achieve effective and scalable prioritisation incorporating expert knowledge
Pair-wise comparison has been successfully utilised in order to prioritise test cases by exploiting the rich, valuable and unique knowledge of the tester. However, the prohibitively large cost of the pair-wise comparison method prevents it from being applied to large test suites. In this paper, we introduce a cluster-based test case prioritisation technique. By clustering test cases, based on their dynamic runtime behaviour, we can reduce the required number of pair-wise comparisons significantly. The approach is evaluated on seven test suites ranging in size from 154 to 1,061 test cases. We present an empirical study that shows that the resulting prioritisation is more effective than existing coverage-based prioritisation techniques in terms of rate of fault detection. Perhaps surprisingly, the paper also demonstrates that clustering (even without human input) can outperform unclustered coverage-based technologies, and discusses an automated process that can be used to determine whether the application of the proposed approach would yield improvement.
Trading-off security and performance in barrier slicing for remote software entrusting
Network applications often require that a trust relationship is established between a trusted host (e.g., the server) and an untrusted host (e.g., the client). The remote entrusting problem is the problem of ensuring the trusted host that whenever a request from an untrusted host is served, the requester is in a genuine state, unaffected by malicious modifications or attacks.Barrier slicing helps solve the remote entrusting problem. The computation of the sensitive client state is sliced and moved to the server, where it is not possible to tamper with it. However, this solution might involve unacceptable computation and communication costs for the server, especially when the slice to be moved is large. In this paper, we investigate the trade-off between security loss and performance overhead associated with moving only a portion of the barrier slice to the server and we show that this trade-off can be reduced to a multi-objective optimization problem. We describe how to make decisions in practice with reference to a case study, for which we show how to choose among the alternative options.
Cooperative Aspect Oriented Programming for executable business processes
AO4BPEL applied Aspect Oriented Programming to executable business processes. Although modularized, AO4BPEL aspects do not have an explicit interface and the implicit one, based on XPath, is often fragile, hence reusing aspects in different processes is quite hard. Cooperative Aspect Oriented Programming aims at making aspects reusable by means of cooperative work between base code and aspects, realized by increasing the explicit awareness of aspects at the expense of pure obliviousness. This work investigates the use of Cooperative Aspect Oriented Programming with BPEL processes.
The effectiveness of source code obfuscation: An experimental assessment
Source code obfuscation is a protection mechanism widely used to limit the possibility of malicious reverse engineering or attack activities on a software system. Although several code obfuscation techniques and tools are available, little knowledge is available about the capability of obfuscation to reduce attackers' efficiency, and the contexts in which such an efficiency may vary. This paper reports the outcome of two controlled experiments meant to measure the ability of subjects to understand and modify decompiled, obfuscated Java code, compared to decompiled, clear code. Results quantify to what extent code obfuscation is able to make attacks more difficult to be performed, and reveal that obfuscation can mitigate the effect of factors that can alter the likelihood of a successful attack, such as the attackers' skill and experience, or the intrinsic characteristics of the system under attack.
Search-based testing of Ajax web applications
Ajax is an emerging Web engineering technology that supports advanced interaction features that go beyond Webpage navigation. The Ajax technology is based on asynchronous communication with the Web server and direct manipulation of the GUI, taking advantage of reflection.Correspondingly, new classes of Web faults are associated with Ajax applications.In previous work, we investigated a state-based testing approach, based on semantically interacting events. The main drawback of this approach is that exhaustive generation of semantically interacting event sequences limits quite severely the maximum achievable length, while longer sequences would have higher fault exposing capability. In this paper, we investigate a search-based algorithm for the exploration of the huge space of long interaction sequences, in order to select those that are most promising, based on a measure of test case diversity.
Analyzing the evolution of the source code vocabulary
Source code is a mixed software artifact, containing information for both the compiler and the developers. While programming language grammar dictates how the source code is written, developers have a lot of freedom in writing identifiers and comments. These are intentional in nature and become means of communication between developers.The goal of this paper is to analyze how the source code vocabulary changes during evolution, through an exploratory study of two software systems. Specifically, we collected data to answer a set of questions about the vocabulary evolution, such as: How does the size of the source code vocabulary evolve over time? What do most frequent terms refer to? Are new identifiers introducing new terms? Are there terms shared between different types of identifiers and comments? Are new and deleted terms in a type of identifiers mirrored in other types of identifiers or in comments?
Reverse engineering of business processes exposed as web applications
Business processes are often implemented by means of software systems which expose them to the user as an externally accessible Web application. This paper describes a technique for recovering business processes by dynamic analysis of the Web applications which ex-pose them. This approach does not require full access to internal software artifacts, such as source code or doc-umentation. The business process is instead inferred through analysis of the GUI-forms exercised by the user during the navigation in the Web application which ex-poses the process. The recovered process is then abstracted by clustering its business tasks according to structural or logical criteria.A preliminary experiment has been conducted with the aim of evaluating understandability and readability of the reverse engineered business processes.
Remote software protection by orthogonal client replacement
In a typical client-server scenario, a trusted server provides valuable services to a client, which runs remotely on an untrusted platform. Of the many security vulnerabilities that may arise (such as authentication and authorization), guaranteeing the integrity of the client code is one of the most difficult to address. This security vulnerability is an instance of the malicious host problem, where an adversary in control of the client's host environment tries to tamper with the client code. We propose a novel client replacement strategy to counter the malicious host problem. The client code is periodically replaced by new orthogonal clients, such that their combination with the server is functionally-equivalent to the original client-server application. The reverse engineering efforts of the adversary are deterred by the complexity of analysis of frequently changing, orthogonal program code. We use the underlying concepts of program obfuscation as a basis for formally defining and providing orthogonality. We also give preliminary empirical validation of the proposed approach.
Using acceptance tests as a support for clarifying requirements: A series of experiments
One of the main reasons for the failure of many software projects is the late discovery of a mismatch between the customers’ expectations and the pieces of functionality implemented in the delivered system. At the root of such a mismatch is often a set of poorly defined, incomplete, under-specified, and inconsistent requirements. Test driven development has recently been proposed as a way to clarify requirements during the initial elicitation phase, by means of acceptance tests that specify the desired behavior of the system. The goal of the work reported in this paper is to empirically characterize the contribution of acceptance tests to the clarification of the requirements coming from the customer. We focused on Fit tables, a way to express acceptance tests, which can be automatically translated into executable test cases. We ran two experiments with students from University of Trento and Politecnico of Torino, to assess the impact of Fit tables on the clarity of requirements. We considered whether Fit tables actually improve requirement understanding and whether this requires any additional comprehension effort. Experimental results show that Fit helps in the understanding of requirements without requiring a significant additional effort.
ARE ENGINEERING
Presents the front cover/table of contents for this issue of the periodical.
Migrazione di sistemi software legacy
Diversi produttori di software devono la loro posizione sul mercato a sistemi software sviluppati parecchie decadi fa, con tecnologie che sono diventate obsolete. Se dal punto di vista economico tali sistemi rappresentano una risorsa inestimabile, in quanto codificano una quantità enorme di regole di business e di conoscenza, dal punto di vista tecnico pongono notevoli problemi. In questo articolo riportiamo una sintesi dell’esperienza maturata nel corso della migrazione verso Java di un grosso applicativo bancario, considerando, in particolare, la strutturazione del flusso di controllo e del modello dati.
Supporting ontology-based semantic annotation of business processes with automated suggestions
Business Process annotation with semantic tags taken from an ontology is becoming a crucial activity for business designers. In fact, semantic annotations help business process comprehension, documentation, analysis and evolution. However, building a domain ontology and annotating a process with semantic concepts is a difficult task. In this work, we propose an automated technique to support the business designer both in domain ontology creation/extension and in the semantic annotation of process models expressed in BPMN. We use natural language processing of the labels appearing in the process elements to construct a domain ontology skeleton or to extend an existing ontology, if available. Semantic annotations are automatically suggested to the business designer, based on a measure of similarity between ontology concepts and the labels of the process elements to be annotated.
Reasoning on semantically annotated processes
Enriching business process models with semantic tags taken from an ontology has become a crucial necessity in service provisioning, integration and composition. In this paper we propose to represent semantically labelled business processes as part of a knowledge base that formalises: business process structure, business domains, and a set of criteria describing correct semantic labelling. Our approach allows (1) to impose domain dependent constraints during the phase of process design, and (2) to automatically verify, via logical reasoning, if business processes fulfill a set of given constraints, and to formulate queries that involve both knowledge about the domain and the process structure. Feasibility and usefulness of our approach will be shown by means of two use cases. The first one on domain specific constraints, and the second one on mining and evolution of crosscutting concerns.
A case study-based comparison of web testing techniques applied to AJAX web applications
Asynchronous Javascript And XML (AJAX) is a recent technology used to develop rich and dynamic Web applications. Different from traditional Web applications, AJAX applications consist of a single page whose elements are updated dynamically in response to callbacks activated asynchronously by the user or by a server message. On the one hand, AJAX improves the responsiveness and usability of a Web application, but on the other hand, it makes the testing phase more difficult. In this paper, our state-based testing technique, developed to test AJAX-based applications, is compared to existing Web testing techniques, such as white-box and black-box ones. To this aim, an experiment based on two case studies has been conducted to evaluate effectiveness and test effort involved in the compared Web testing techniques. In particular, the capability of each technique to reveal injected faults of different fault categories is analyzed in detail. The associated effort was also measured. The results show that state-based testing is complementary to the existing Web testing techniques and can reveal faults otherwise unnoticed or hard to reveal with the other techniques.
Towards experimental evaluation of code obfuscation techniques
While many obfuscation schemes proposed, none of them satisfy any strong definition of obfuscation. Furthermore secure general-purpose obfuscation algorithms have been proven to be impossible. Nevertheless, obfuscation schemes which in practice slow down malicious reverse-engineering by obstructing code comprehension for even short periods of time are considered a useful protection against malicious reverse engineering. In previous works, the difficulty of reverse engineering has been mainly estimated by means of code metrics, by the computational complexity of static analysis or by comparing the output of de-obfuscating tools. In this paper we take a different approach and assess the difficulty attackers have in understanding and modifying obfuscated code through controlled experiments involving human subjects.
Data model reverse engineering in migrating a legacy system to Java
Central to any legacy migration project is the translation of the data model. Decisions made here will have strong implications to the rest of the translation. Some legacy languages lack a structured data model, relying instead on explicit programmer control of the overlay of variables. In this paper we present our experience inferring a structured data model in such a language as part of a migration of eight million lines of code to Java. We discuss the common idioms of coding that were observed and give an overview of our solution to this problem.
Dynamic model extraction and statistical analysis of web applications: Follow-up after 6 years
In 2002 we proposed a method to reverse engineer a Web application model. The proposed method deals with dynamic Web applications, consisting of server components, typically interacting with a persistent layer, which build the Web pages displayed on the browser dynamically. Dynamic analysis and page merging heuristics were used for model extraction. The proposed model was successfully adopted in the Web analysis and testing research community. However, the features of future Web applications (involving rich client components and asynchronous communication with the server) challenge its future applicability. In this paper, we analyze the key properties of the 2002 model and identify those modeling decisions that remain valid and can be used to guide the extraction of models for future Web applications.
Using program transformations to add structure to a legacy data model
An appropriate translation of the data model is central to any language migration effort. Finding a mapping between original and target data models may be challenging for legacy languages (e.g., Assembly) which lack a structured data model and rely instead on explicit programmer control of the overlay of variables. Before legacy applications written in languages with an unstructured data model can be migrated to modern languages, a structured data model must be inferred. This paper describes a set of source transformations used to create such a model as part of a migration of eight million lines of code to Java. The original application is written in a proprietary language supporting variable layout by memory relocation.
Introduction: Cultural studies and anti-consumerism: A critical encounter
Anti-consumerism has become a conspicuous part of contemporary activism and popular culture, from &​#x2018;culture jams' and actions against Esso and Starbucks, through the downshifting and voluntary simplicity movements, the rise of ethical consumption and organic and the high profile of films and books like Supersize Me! and No Logo. A rising awareness of labor conditions in overseas plants, the environmental impact of intensified consumer lifestyles and the effects of neo-liberal privatization have all stimulated such popular cultural opposition. However, the subject of anti-consumerism has received relatively little theoretical attention - particularly from cultural studies, which is surprising given the discipline's historical investments in extending radical politics and exploring the complexities of consumer desire. This book considers how the expanding resources of contemporary cultural theory might be drawn upon to understand anti-consumerist identifications and practices; how railing against the social and cultural effects of consumerism has a complex past as well as present; and it pays attention to the interplays between the different movements of anti-consumerism and the particular modes of consumer culture in which they exist. In addition, as well as &​#x2018;using' cultural studies to analyse anti-consumerism, it also asks how such anti-consumerist practices and discourse challenges some of the presumptions and positions currently held in cultural studies
Crosscutting concern documentation by visual query of business processes
Business processes can be very large and may contain several different concerns, scattered across the process and tangled with other concerns. Crosscutting concerns are difficult to find and locate, thus making process design and evolution hard. In this work, we propose a method to support business designers in documenting structural as well as business domain crosscutting concerns, thus facilitating concern understanding and evolution. We introduce a visual query language, which allows business designers to mine, explore, document and evolve crosscutting concerns, by means of visual queries performed on the business process. Such queries can be stored as additional design artefacts which document the existence and location of crosscutting design concerns.
Are fit tables really talking?: a series of experiments to understand whether fit tables are useful during evolution tasks
Test-driven software development tackles the problem of operationally defining the features to be implemented by means of test cases. This approach was recently ported to the early development phase, when requirements are gathered and clarified. Among the existing proposals, Fit (Framework for Integrated Testing) supports the precise specification of requirements by means of so called Fit tables, which express relevant usage scenarios in a tabular format, easily understood also by the customer. Fit tables can be turned into executable test cases through the creation of pieces of glue code, called fixtures. In this paper, we test the claimed benefits of Fit through a series of three controlled experiments in which Fit tables and related fixtures are used to clarify a set of change requirements, in a software evolution scenario. Results indicate improved correctness achieved with no significant impact on time, however benefits of Fit vary in a substantial way depending on the developers' experience. Preliminary results on the usage of Fit in combination with pair programming revealed another relevant source of variation.
eCAT: a tool for automating test cases generation and execution in testing multi-agent systems
We introduce eCAT, a tool that supports deriving test cases semi-automatically from goal-based analysis diagrams, generates meaningful test inputs based on agent interaction ontology, and more importantly it can evolve and execute test cases automatically and continuously on a multi-agent system (MAS). Our experiments have shown that the proposed tool can exercise MAS more extensively and effectively than manual testing under the usual time constraints.
Experimental evaluation of ontology-based test generation for multi-agent systems
Software agents are a promising technology for today’s complex, distributed systems. Methodologies and techniques that address testing and reliability of multi agent systems are increasingly demanded, in particular to support automated test case generation and execution. A novel approach, based on agent interaction ontology, has been recently proposed and integrated into a testing framework, called eCAT, which can generate and evolve test cases automatically, and run them continuously. In this paper, we focus on the experimental evaluation of an ontology-based test generation approach. We use two BDI agent applications as case studies to investigate the performance of the framework as well as its capability to reveal faults.
Ontology-based test generation for multiagent systems
This paper investigates software agents testing, and in particular how to automate test generation. We propose a novel approach, which takes advantage of agent interaction ontologies that define content semantic of agent interactions to: (i) generate test inputs; (ii) guide the exploration of the input space during generation; and, (iii) verify messages exchanged among agents with respect to the defined interaction ontology. We integrated the proposed approach into a testing framework, called eCAT, which can generate and evolve test cases automatically, and run them continuously.
Constraint-based evolutionary testing of autonomous distributed systems
Distributed software systems are characterized by in- creasing autonomy. They often have the capability to sense the environment and react to it, discover the presence of other systems and take advantage of their services, adapt and re-configure themselves in accordance with the inter- nal as well as the global state. Testing this kind of systems is challenging, and systematic and automated approaches are still missing. We propose a novel evolutionary testing framework for autonomous distributed systems. In this framework, test cases are continuously generated and executed. Our cur- rent implementation of the framework provides two tech- niques for the automated, continuous generation of test cases: (1) random; (2) evolutionary-mutation. Preliminary experimental results, obtained on a case study, are encour- aging and indicate that evolutionary testing can comple- ment effectively the manual one.
State-based testing of Ajax web applications
Ajax supports the development of rich-client Web applications, by providing primitives for the execution of asynchronous requests and for the dynamic update of the page structure and content. Often, Ajax Web applications consist of a single page whose elements are updated in response to callbacks activated asynchronously by the user or by a server message. These features give rise to new kinds of faults that are hardly revealed by existing Web testing approaches. In this paper, we propose a novel state-based testing approach, specifically designed to exercise Ajax Web applications. The Document Object Model (DOM) of the page manipulated by the Ajax code is abstracted into a state model. Callback executions triggered by asynchronous messages received from the Web server are associated with state transitions. Test cases are derived from the state model based on the notion of semantically interacting events. We evaluate the approach on a case study in terms of fault revealing capability. We also measure the amount of manual interventions involved in constructing and refining the model required by this approach.
Inference of a structured data model in migrating a legacy system to Java
Central to any legacy migration project is the translation of the data model. Decisions made here will have strong implications to the rest of the translation. Some legacy languages lack a structured data model, relying instead on explicit programmer control of the overlay of variables. In this paper we present our experience inferring a structured data model in such a language as part of a migration of eight million lines of code to Java. We discuss the common idioms of coding that were observed and give an overview of our solution to this problem.
Goto elimination strategies in the migration of legacy code to Java
Legacy systems are often large and difficult to maintain, but rewriting them from scratch is usually not a viable option. Reenginering remains the only way to modernize them. We have been recently involved in a migration project aiming at porting an old, large (8 MLOC) legacy banking system to a modern architecture. The goal of the project is: (I) moving from an old, proprietary language to Java; (2) replacing ISAM indexed files with a relational database; (3) upgrading the character oriented interface to a modern GUI. One of the steps in the migration process deals with the elimination of unstructured code (unconditional jumps such as GOTO statements). In this paper we present four alternative strategies for GOTO elimination that we evaluated in the project. Each has pros and cons, but when used in a real case, it turned out that one produced completely unreadable code, hence it was discarded. The final choice was a combination of the three remaining strategies.
Distributing trust verification to increase application performance
The remote trust problem aims to address the issue of verifying the execution of a program running on an un-trusted host which communicates regularly with a trusted server. One proposed solution to this problem relies on a centralized scheme using assertions and replication to withhold usable services from a tampered client. We show how to extend such a scheme to a distributed trusted hardware such as tamper-resistant smartcards. We compared the performance and security of the proposed distributed system to the original centralized scheme on a case study. Our results indicate that, compared to a centralized scheme, our distributed trust scheme has dramatically lower network traffic, and smaller memory and computational requirements on the trusted server.
The use of executable fit tables to support maintenance and evolution tasks
Acceptance testing is a kind of testing performed prior to software delivery. In the agile approach, acceptance test suites are specified by analysts and customers during the requirement elicitation phase and used to support the development/maintenance activities. This paper reports an experiment with master students that investigates on the usefulness of executable acceptance test cases, developed by using FIT (Framework for Integrated Test), during software maintenance and evolution activities. The preliminary results indicate that FIT tables help students to correctly perform the maintenance/evolution tasks with no significant impact on time.
Remote entrusting by run-time software authentication
The problem of software integrity is traditionally addressed as the static verification of the code before the execution, often by checking the code signature. However, there are no well-defined solutions to the run-time verification of code integrity when the code is executed remotely, which is refer to as run-time remote entrusting. In this paper we present the research challenges involved in run-time remote entrusting and how we intend to solve this problem. Specifically, we address the problem of ensuring that a given piece of code executes on an remote untrusted machine and that its functionalities have not been tampered with both before execution and during run-time.
Business process concern documentation and evolution
Business processes can be very large and can contain many different concerns, scattered across the process and tangled with other concerns. Crosscutting concerns are difficult to find, locate and modify in a consistent way, thus making process maintenance and reuse hard, even for business experts. In this report, we propose a method to support business designers when they need to document existing crosscutting concerns and when they work on their evolution. We have adapted and applied available techniques for crosscutting concern browsing, mining and refactoring to business process models. More precisely, we propose to enrich BPMN process elements with semantic annotations taken from a domain ontology. We support ontology creation or enrichment in the given business domain. We introduce a visual query language, which allows business designers to quantify over processes and reason over the ontology. We use a mechanism for semi-automatic crosscutting concern mining and we support consistent evolution of crosscutting concerns, once these are modularized separately from the principal process flow.
Business Process Concern Documentation And Evolution
Business processes can be very large and can contain many different concerns, scattered across the process and tangled with other concerns. Crosscutting concerns are difficult to find, locate and modify in a consistent way, thus making process maintenance and reuse hard, even for business experts. In this report, we propose a method to support business designers when they need to document existing crosscutting concerns and when they work on their evolution. We have adapted and applied available techniques for crosscutting concern browsing, mining and refactoring to business process models. More precisely, we propose to enrich BPMN process elements with semantic annotations taken from a domain ontology. We support ontology creation or enrichment in the given business domain. We introduce a visual query language, which allows business designers to quantify over processes and reason over the ontology. We use a mechanism for semi-automatic crosscutting concern mining and we support consistent evolution of crosscutting concerns, once these are modularized separately from the principal process flow.
Using the Planning Game for Test Case Prioritization
One of the major risks for software projects is the mismatch between implemented functionalities and customer’s needs. In order to minimize this risk, agile methodologies propose to involve the customer in all phases of development. Customers possess knowledge about the requirements to be implemented, their relative importance and the success criteria for the software project. In Extreme Programming, this knowledge is exploited for release and iteration planning during the so called Planning Game. In this paper, we propose an extension of the Planning Game with a phase explicitly devoted to acceptance test definition and prioritization (Acceptance Test Planning). The customer prioritizes acceptance tests in coordination with the test engineer and defines the value they bring to the user. Prioritized acceptance tests are then available to the next development phases. Preliminary results obtained on a case-study indicate that the technique is effective in exercising the most critical functionalities early.
Code quality from the programmer's perspective
Quality in general and software quality in particular can be defined in several different ways, but any definition is relative to some person, who represents the target of the delivered quality. Quality is value for some person, rather than absolute value. This is especially true for software quality, where we can identify at least two categories of stakeholders for which quality is a central issue: users and developers. For users, software quality means ease of use, no crash at run-time, compliance with the requirements, correctness, etc. For developers, it means good design, encapsulation of functionalities, proper modularization, meaningful identifier names, documented interfaces, etc. These two views of the software quality are usually called external quality and internal quality of the software. Both are important and usually the assumption is made that the two are strongly related, so that it makes sense to invest in the improvement of both to eventually deliver a high quality product to the final user. For example, a good design may facilitate smooth adaptation of the software to a changed requirement or addition of new functionalities. A bad design may not prevent it, but may result in a system that contains more problems, due to the difficulty of evolving it, eventually delivering a lower quality (e.g., more bugs) to the user. The external, user’s, view on the quality is focused on “what” the software does, regardless of how it is implemented, but clearly the “how” is strictly connected with the (quality of) the “what”, in terms of correct and compliant implementation, ease of evolution and misbehaviors (e.g., crashes) exhibited over time. While internal and external quality are both a key issue in a software project, different contexts demand for a different emphasis. Let us consider a commercial, closed source (industrial) context, compared to a free, open source one. In an industrial context, the customer decides the fortune of the software. However, the customer’s needs and domain are only partially known by the developers. Hence, the risk of not matching the requirements accurately and correctly is probably the most important one. This justifies the large investment in system and acceptance testing, addressing the external quality view, that is typical of these organizations. Improvement of the internal code structure is considered of less importance and often there is no remaining time or resources for it. The open source context represents the other extreme of a continuum. Often, open source projects deal with the development of tools that are used by programmers or computer scientists, so domain and requirements are perfectly known and the risk of a requirement mismatch is relatively low. On the other hand, many developers, distributed world-wide, are often involved. This means that the design of the software is the key factor that decides its fortune, together with internal quality attributes, such as the understandability of the code and its capability of self-documentation. We have been involved in the assessment and improvement of the quality of the code developed for the ALICE experiment at CERN since 1999. In our experience, this context has several similarities with the open source context. Developers of the software are also the users, and the domain is perfectly known. Many contributors are spread across many countries. The code is subject to a large amount of changes and evolution over time, which demands for a good internal structure and for an easy way to understand the internal functionalities. In this paper, we first give an overview of the traditional way to address the internal software quality problem, which is the main quality concern for ALICE. Then, we report the results obtained so far, thanks to a tool for the enforcement of coding conventions and for the detection of bad programming practices. Finally we consider one dimension not covered by any existing approach to the internal code quality: the quality of the lexicon used by programmers, which deeply affects code understandability and maintainability. We report some preliminary results obtained from a few empirical studies conducted on the ALICE code. We conclude the paper with the research directions that we consider most promising in trying to address this quality dimension – the programmer’s lexicon.
Measuring the impact of different categories of software evolution
Software evolution involves different categories of interventions, having variable impact on the code. Knowledge about the expected impact of an intervention is fundamental for project planning and resource allocation. Moreover, deviations from the expected impact may hint for areas of the system having a poor design. In this paper, we investigate the relationship between evolution categories and impacted code by means of a set of metrics computed over time for a subject system.
Improving Web site understanding with keyword‐based clustering
Web applications are becoming more and more complex and difficult to maintain. To satisfy the customer's demands, they need to be updated often and quickly. In the maintenance phase, Web site understanding is a central activity. In this phase, programmers spend a lot of time and effort in the comprehension of the internal Web site structure. Such activity is often required because the available documentation is not aligned with the implementation, if not missing at all. Reverse engineering techniques have the potential to support Web site understanding, by providing views that show the organization of a site and its navigational structure. However, representing each Web page as a node in a diagram recovered from the source code of the Web site often leads to huge and unreadable graphs. Moreover, since the level of connectivity is typically high, the edges in such graphs make the overall result even less usable. In this paper, we propose an approach to Web site understanding based on clustering of client‐side HTML pages with similar content. This approach works well with content‐oriented sites rather than application‐oriented ones and uses a crawler to download the Web pages of the target Web site. The presence of common keywords is exploited to decide when it is appropriate to group pages together. An experimental work, including 17 Web sites, validates our approach and shows that the clusters produced automatically are close to those that a human would produce for a given Web site. Copyright © 2007 John Wiley & Sons, Ltd.
