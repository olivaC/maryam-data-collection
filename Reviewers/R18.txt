POSTER: AFL-based Fuzzing for Java with Kelinci
Grey-box fuzzing is a random testing technique that has been shown to be effective at finding security vulnerabilities in software. The technique leverages program instrumentation to gather information about the program with the goal of increasing the code coverage during fuzzing, which makes gray-box fuzzers extremely efficient vulnerability detection tools. One such tool is AFL, a grey-box fuzzer for C programs that has been used successfully to find security vulnerabilities and other critical defects in countless software products. We present Kelinci, a tool that interfaces AFL with instrumented Java programs. The tool does not require modifications to AFL and is easily parallelizable. Applying AFL-type fuzzing to Java programs opens up the possibility of testing Java based applications using this powerful technique. We show the effectiveness of Kelinci by applying it on the image processing library Apache Commons Imaging, in which it identified a bug within one hour.
Deepsafe: A data-driven approach for checking adversarial robustness in neural networks
Deep neural networks have become widely used, obtaining remarkable results in domains such as computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, and bio-informatics, where they have produced results comparable to human experts. However, these networks can be easily fooled by adversarial perturbations: minimal changes to correctly-classified inputs, that cause the network to mis-classify them. This phenomenon represents a concern for both safety and security, but it is currently unclear how to measure a network's robustness against such perturbations. Existing techniques are limited to checking robustness around a few individual input points, providing only very limited guarantees. We propose a novel approach for automatically identifying safe regions of the input space, within which the network is robust against adversarial perturbations. The approach is data-guided, relying on clustering to identify well-defined geometric regions as candidate safe regions. We then utilize verification techniques to confirm that these regions are safe or to provide counter-examples showing that they are not safe. We also introduce the notion of targeted robustness which, for a given target label and region, ensures that a NN does not map any input in the region to the target label. We evaluated our technique on the MNIST dataset and on a neural network implementation of a controller for the next-generation Airborne Collision Avoidance System for unmanned aircraft (ACAS Xu). For these networks, our approach identified multiple regions which were completely safe as well as some which were only safe for specific labels. It also discovered several adversarial perturbations of interest.
Probabilistic programming for Java using symbolic execution and model counting
In this paper we describe a probabilistic programming environment for Java that is based on symbolic execution and model counting. The novelty of the framework is that the probability distributions in the program can themselves be symbolic, which allows parametric probabilistic programming. The framework handles typical probabilistic programming features, such as observe statements, and can be used for the encoding and analysis of Discrete Time Markov Chains (DTMC), Bayesian Networks, etc. We show two examples of using the system: (1) analysis of bubble sort when using an unreliable comparison operation, and, (2) analysis of a simulation model of autonomous aircraft towing vehicles, to show whether plans generated for these vehicles are robust when probability distributions are changed from the ones used to generate the plans.
Synthesis of Adaptive Side-Channel Attacks
We present symbolic analysis techniques for detecting vulnerabilities that are due to adaptive side-channel attacks, and synthesizing inputs that exploit the identified vulnerabilities. We start with a symbolic attack model that encodes succinctly all the side-channel attacks that an adversary can make. Using symbolic execution over this model, we generate a set of mathematical constraints, where each constraint characterizes the set of secret values that lead to the same sequence of side-channel measurements. We then compute the optimal attack, i.e, the attack that yields maximum leakage over the secret, by solving an optimization problem over the computed constraints. We use information-theoretic concepts such as channel capacity and Shannon entropy to quantify the leakage over multiple runs in the attack, where the measurements over the side channels form the observations that an adversary can use to try to infer the secret. We also propose greedy heuristics that generate the attack by exploring a portion of the symbolic attack model in each step. We implemented the techniques in Symbolic PathFinder and applied them to Java programs encoding web services, string manipulations and cryptographic functions, demonstrating how to synthesize optimal side-channel attacks.
A DSL approach to reconcile equivalent divergent program executions
Multi-Version Execution (MVE) deploys multiple versions of the same program, typically synchronizing their execution at the level of system calls. By default, MVE requires all deployed versions to issue the same sequence of system calls, which limits the types of versions which can be deployed. In this paper, we propose a Domain-Specific Language (DSL) to reconcile expected divergences between different program versions deployed through MVE. We evaluate the DSL by adding it to an existing MVE system (Varan) and testing it via three scenarios: (1) deploying the same program under different configurations, (2) deploying different releases of the same program, and (3) deploying dynamic analyses in parallel with the native execution. We also present an algorithm to automatically extract DSL rules from pairs of system call traces. Our results show that each scenario requires a small number of simple rules (at most 14 rules in each case) and that writing DSL rules can be partially automated.
Probabilistic Program Analysis
This paper provides a survey of recent work on adapting techniques for program analysis to compute probabilistic characterizations of program behavior. We survey how the frameworks of data flow analysis and symbolic execution have incorporated information about input probability distributions to quantify the likelihood of properties of program states. We identify themes that relate and distinguish a variety of techniques that have been developed over the past 15 years in this area. In doing so, we point out opportunities for future research that builds on the strengths of different techniques.
Symbolic execution and probabilistic reasoning
Summary form only given. Symbolic execution is a systematic program analysis technique which explores multiple program behaviors all at once by collecting and solving symbolic path conditions over program paths. The technique has been recently extended with probabilistic reasoning. This approach computes the conditions to reach target program events of interest and uses model counting to quantify the fraction of the input domain satisfying these conditions thus computing the probability of event occurrence. This probabilistic information can be used for example to compute the reliability of an aircraft controller under different wind conditions (modeled probabilistically) or to quantify the leakage of sensitive data in a software system, using information theory metrics such as Shannon entropy. In this talk we review recent advances in symbolic execution and probabilistic reasoning and we discuss how they can be used to ensure the safety and security of software systems.
Synthesis of Adaptive Side-Channel Attacks
We present symbolic analysis techniques for detecting vulnerabilities that are due to adaptive side-channel attacks, and synthesizing inputs that exploit the identified vulnerabilities. We start with a symbolic attack model that encodes succinctly all the side-channel attacks that an adversary can make. Using symbolic execution over this model, we generate a set of mathematical constraints, where each constraint characterizes the set of secret values that lead to the same sequence of sidechannel measurements. We then compute the optimal attack, i.e, the attack that yields maximum leakage over the secret, by solving an optimization problem over the computed constraints. We use information-theoretic concepts such as channel capacity and Shannon entropy to quantify the leakage over multiple runs in the attack, where the measurements over the side channels form the observations that an adversary can use to try to infer the secret. We also propose greedy heuristics that generate the attack by exploring a portion of the symbolic attack model in each step.We implemented the techniques in Symbolic PathFinder and applied them to Java programs encoding web services, string manipulations and cryptographic functions, demonstrating how to synthesize optimal side-channel attacks.
A synergistic approach for distributed symbolic execution using test ranges
Symbolic execution is a systematic program analysis technique that has received a lot of attention in the research community. However, scaling symbolic execution continues to pose a major challenge. This paper introduces Synergise, a novel two-fold integration approach. One, it integrates distributed analysis and constraint re-use to enhance symbolic execution using feasible ranges, which allow sharing of constraint solving results among different workers without communicating or sharing potentially large constraint databases (as required traditionally). Two, it integrates complementary techniques for test input generation, e.g., search-based generation and symbolic execution, for creating higher quality tests using unexplored ranges, which allows symbolic execution to re-use tests created by another technique for effective distribution of exploration of previously unexplored paths.
Model-counting approaches for nonlinear numerical constraints
Model counting is of central importance in quantitative reasoning about systems. Examples include computing the probability that a system successfully accomplishes its task without errors, and measuring the number of bits leaked by a system to an adversary in Shannon entropy. Most previous work in those areas demonstrated their analysis on programs with linear constraints, in which cases model counting is polynomial time. Model counting for nonlinear constraints is notoriously hard, and thus programs with nonlinear constraints are not well-studied. This paper surveys state-of-the-art techniques and tools for model counting with respect to SMT constraints, modulo the bitvector theory, since this theory is decidable, and it can express nonlinear constraints that arise from the analysis of computer programs. We integrate these techniques within the Symbolic Pathfinder platform and evaluate them on difficult nonlinear constraints generated from the analysis of cryptographic functions.
Model-Counting Approaches for Nonlinear Numerical Constraints
Model counting is of central importance in quantitative reasoning about systems. Examples include computing the probability that a system successfully accomplishes its task without errors, and measuring the number of bits leaked by a system to an adversary in Shannon entropy. Most previous work in those areas demonstrated their analysis on programs with linear constraints, in which cases model counting is polynomial time. Model counting for nonlinear constraints is notoriously hard, and thus programs with nonlinear constraints are not well-studied. This paper surveys state-of-the-art techniques and tools for model counting with respect to SMT constraints, modulo the bitvector theory, since this theory is decidable, and it can express nonlinear constraints that arise from the analysis of computer programs. We integrate these techniques within the Symbolic Pathfinder platform and evaluate them on difficult nonlinear constraints generated from the analysis of cryptographic functions.
Symbolic Complexity Analysis using Context-preserving Histories
We propose a technique based on symbolic execution for analyzing the algorithmic complexity of programs. The technique uses an efficient guided analysis to compute bounds on the worst-case complexity (for increasing input sizes) and to generate test values that trigger the worst-case behaviors. The resulting bounds are fitted to a function to obtain a prediction of the worst-case program behavior at any input sizes. Comparing these predictions to the programmers' expectations or to theoretical asymptotic bounds can reveal vulnerabilities or confirm that a program behaves as expected. To achieve scalability we use path policies to guide the symbolic execution towards worst-case paths. The policies are learned from the worst-case results obtained with exhaustive exploration at small input sizes and are applied to guide exploration at larger input sizes, where un-guided exhaustive exploration is no longer possible. To achieve precision we use path policies that take into account the history of choices made along the path when deciding which branch to execute next in the program. Furthermore, the history computation is context-preserving, meaning that the decision for each branch depends on the history computed with respect to the enclosing method. We implemented the technique in the Symbolic PathFinder tool. We show experimentally that it can find vulnerabilities in complex Java programs and can outperform established symbolic techniques.
Symbolic Arrays in Symbolic PathFinder
Symbolic Execution is a program analysis technique used to increase software reliability. Modern software often manipulate complex data structures, many of which being similar to arrays. We present a novel approach and implementation in Symbolic PathFinder for handling symbolic arrays in Java. It enables analyzing a broader class of programs that manipulates arrays. We also extend the Symbolic Pathfinder testcase generation to support numeric arrays.
String analysis for side channels with segmented oracles
We present an automated approach for detecting and quantifying side channels in Java programs, which uses symbolic execution, string analysis and model counting to compute information leakage for a single run of a program. We further extend this approach to compute information leakage for multiple runs for a type of side channels called segmented oracles, where the attacker is able to explore each segment of a secret (for example each character of a password) independently. We present an efficient technique for segmented oracles that computes information leakage for multiple runs using only the path constraints generated from a single run symbolic execution. Our implementation uses the symbolic execution tool Symbolic PathFinder (SPF), SMT solver Z3, and two model counting constraint solvers LattE and ABC. Although LattE has been used before for analyzing numeric constraints, in this paper, we present an approach for using LattE for analyzing string constraints. We also extend the string constraint solver ABC for analysis of both numeric and string constraints, and we integrate ABC in SPF, enabling quantitative symbolic string analysis.
Towards MC/DC Coverage of Properties Specification Patterns
Model based testing is used to validate the actual system against its requirements described as formal specification, while formal verification proves that a requirement is not violated in the overall system. Verifying properties, in certain cases, becomes very expensive (or unpractical), mainly when the application of test techniques is enough for the users purposes. The Modified Condition/Decision Coverage (MC/DC), used in the avionics software industry, is recognised as a good technique to find out the possible mistakes on programs logics because it covers how each condition can affect the programs’ decisions outcomes. It has also been adapted to provide the coverage of specifications in the requirements-based approach. This paper proposes a technique to decompose properties (specifications), defined as regular expressions, into subexpressions representing test cases to cover the MD/DC for specifications (Unique First Word Recognition). Then, instead of proving an entire property, we can use a model checker to observe and select program executions that cover all the test cases given as the subexpressions. To support this approach, we give a syntactic characterisation of the properties decomposition, inductively defined over the syntax of regular expressions, and show how to use the technique to decompose Specification Patterns (SPS) and monitor their satisfiability using the Java PathFinder (JPF).
Certified Symbolic Execution
We propose a certification approach for checking the analysis results produced by symbolic execution. Given a program P under test, an analysis producer performs symbolic execution on P and creates a certificate C that represents the results of symbolic execution. The analysis consumer checks the validity of C with respect to P using efficient symbolic re-execution of P. The certificates are simple to create and easy to validate. Each certificate is a list of witnesses that include: test inputs that validate path feasibility without requiring any constraint solving; and infeasibility summaries that provide hints on how to efficiently establish path infeasibility. To account for incompleteness in symbolic execution (due to incompleteness of the backend solver), the certificate also contains an incompleteness summary. Our approach deploys constraint slicing and other heuristics as performance optimizations. Experimental results using a prototype certification tool based on Symbolic PathFinder for Java show that certification can be 3X to 370X (on average, 75X) faster than traditional symbolic execution. We also show the benefits of the approach for the reliability assessment of a software component under different probabilistic environment conditions.
Log2model: inferring behavioral models from log data
We present LOG2MODEL, an approach, supported by a tool, that builds behavioral models from log data. The logged data consists of time series encoding the values of the states of a system observed at discrete time steps. The models generated are Discrete-Time Markov Chains with states and transitions representing the values recorded in the log. The models contain key information that can be visualized and analyzed with respect to safety, delays, throughput etc, using off-the-shelf model checkers such as PRISM. The analysis results can be further used by users or automated tools to monitor and alter the system behavior. We present the architecture of LOG2MODEL and its application in the context of autonomous operations in the airspace domain.
Automated circular assume-guarantee reasoning with N-way decomposition and alphabet refinement
In this work we develop an automated circular reasoning framework that is applicable to systems decomposed into multiple components. Our framework uses a family of circular assume-guarantee rules for which we give conditions for soundness and completeness. The assumptions used in the rules are initially approximate and their alphabets are automatically refined based on the counterexamples obtained from model checking the rule premises. A key feature of the framework is that the compositional rules that are used change dynamically with each iteration of the alphabet refinement, to only use assumptions that are relevant for the current alphabet, resulting in a smaller number of assumptions and smaller state spaces to analyze for each premise. Our preliminary evaluation of the proposed approach shows promising results compared to 2-way and monolithic verification.
Multi-run side-channel analysis using Symbolic Execution and Max-SMT
Side-channel attacks recover confidential information from non-functional characteristics of computations, such as time or memory consumption. We describe a program analysis that uses symbolic execution to quantify the information that is leaked to an attacker who makes multiple side-channel measurements. The analysis also synthesizes the concrete public inputs (the "attack") that lead to maximum leakage, via a novel reduction to Max-SMT solving over the constraints collected with symbolic execution. Furthermore model counting and information-theoretic metrics are used to compute an attacker's remaining uncertainty about a secret after a certain number of side-channel measurements are made. We have implemented the analysis in the Symbolic PathFinder tool and applied it in the context of password checking and cryptographic functions, showing how to obtain tight bounds on information leakage under a small number of attack steps.
Multi-run side-channel analysis using Symbolic Execution and Max-SMT
Side-channel attacks recover confidential information from non-functional characteristics of computations, such as time or memory consumption. We describe a program analysis that uses symbolic execution to quantify the information that is leaked to an attacker who makes multiple side-channel measurements. The analysis also synthesizes the concrete public inputs (the "attack") that lead to maximum leakage, via a novel reduction to Max-SMT solving over the constraints collected with symbolic execution. Furthermore model counting and information-theoretic metrics are used to compute an attacker's remaining uncertainty about a secret after a certain number of side-channel measurements are made. We have implemented the analysis in the Symbolic PathFinder tool and applied it in the context of password checking and cryptographic functions, showing how to obtain tight bounds on information leakage under a small number of attack steps.
Planning, Scheduling and Monitoring for Airport Surface Operations.
This paper explores the problem of managing movements of aircraft along the surface of busy airports. Airport surface management is a complex logistics problem involving the coordination of humans and machines. The work described here arose from the idea that autonomous towing vehicles for taxiing aircraft could offer a solution to the ’capacity problem’ for busy airports, the problem of getting more efficient use of existing surface area to meet increasing demand. Supporting autonomous surface operations requires continuous planning, scheduling and monitoring of operations, as well as systems for optimizing complex human-machine interaction. We identify a set of computational subproblems of the surface management problem that would benefit from recent advances in multi-agent planning and scheduling and probabilistic predictive modeling, and discuss preliminary work at integrating these components into a prototype of a surface management system.
Symbolic execution and timed automata model checking for timing analysis of Java real-time systems
This paper presents SymRT, a tool based on a combination of symbolic execution and real-time model checking for timing analysis of Java systems. Symbolic execution is used for the generation of a safe and tight timing model of the analyzed system capturing the feasible execution paths. The model is combined with suitable execution environment models capturing the timing behavior of the target host platform including the Java virtual machine and complex hardware features such as caching. The complete timing model is a network of timed automata which directly facilitates safe estimates of worst and best case execution time to be determined using the Uppaal model checker. Furthermore, the integration of the proposed techniques into the TetaSARTS tool facilitates reasoning about additional timing properties such as the schedulability of periodically and sporadically released Java real-time tasks (under specific scheduling policies), worst case response time, and more.
Quantification of software changes through probabilistic symbolic execution (N)
Characterizing software changes is fundamental for software maintenance. However existing techniques are imprecise leading to unnecessary maintenance efforts. We introduce a novel approach that computes a precise numeric characterization of program changes, which quantifies the likelihood of reaching target program events (e.g., assert violations or successful termination) and how that evolves with each program update, together with the percentage of inputs impacted by the change. This precise characterization leads to a natural ranking of different program changes based on their probability of execution and their impact on target events. The approach is based on model counting over the constraints collected with a symbolic execution of the program, and exploits the similarity between program versions to reduce cost and improve the quality of analysis results. We implemented our approach in the Symbolic PathFinder tool and illustrate it on several Java case studies, including the evaluation of different program repairs, mutants used in testing, or incremental analysis after a change.
Guest editorial: special multi-issue on selected topics in Automated Software Engineering
This special multi-issue of Automated Software Engineering is an attempt by editors to pull together articles from a range of current interesting topics from the field of Automated Software Engineering. We feel the topics and research reported here form an interesting cross section of current work. While these research efforts have published in recent conferences, we have allowed time for the authors to significantly extend and complete their work, so each of these contributions is novel in its own right and has reached a level of maturity appropriate to an archival publication. This issue is divided into three parts. In the first part, we include diverse papers from the frontier of software engineering research. 1. In “Monitoring Energy Hotspots in Software”, Adel Noureddine. Romain Rouvoy and Lionel Seinturier discuss ways to control the energy usage of software. This field of “Green IT” is concerned with the optimization of software solutions with regards to their energy consumption. In this domain, most current solutions concentrate on coarse-grained approaches to monitor the energy consumption of a device or a process. This new paper reports on a fine-grained runtime energy monitoring framework we developed to help developers to diagnose energy hotspots with a better accuracy. 2. In “User-Aware Privacy Control via Extended Static-Information-Flow Analysis”, Xusheng Xiao, Nikolai Tillmann, Manuel Fahndrich, Jonathan de Halleux, Michal Moskal and Tao Xie discuss the problem of privacy “leakage” in mobile apps. Existing mobile platforms provide little information on how applications use private user data. This makes it difficult for experts to validate applications and for users to grant applications access to their private data. These authors discuss propose a user-aware-privacy-control approach based on static information flows (which are classified as safe/unsafe). When applied to hundreds of scripts developed for a mobile platform, this approach effectively reduces the need to make access-granting choices to only 10.1 % (54) of all scripts. 3. Our next paper is “Balancing Precision and Performance in Structured Merge” by Olaf Leßenich, Sven Apel, and Christian Lengauer. It addresses an old problem with some startling new results. When combining code from different sources, developers use some kind of unstructured-merge (i.e., line-based) tools. These are fast, but imprecise. This paper reports a new structured-merge approach with auto-tuning that tunes the merging process on-line by switching between unstructured and structured merge (depending on the presence of conflicts). In the case studies explored here, this method ran 92 times faster than purely structured merge (10 times on average). 4. Finally, in “A model-driven framework for guided design space exploration”, Abel Hegedus, Akos Horvath, and Daniel Varro discuss methods for design space exploration that aim at searching through various models representing different software design candidates. A full and exhaustive exploration of the design space is infeasible for large models. Hence, in this approach, the traversal is guided by hints, derived by system analysis, to prioritize the next states to traverse (selection criteria) and to avoid searching unpromising states (cut-off criteria). The technique is assessed using a a cloud infrastructure configuration problem. The next two issues in this series will cover Testing & verification Reasoning about source code. The guest editors are indebted to the authors of submitted papers and our small army of reviewers who offered extensive and timely reviews of the papers. Thanks to you all!
Iterative distribution-aware sampling for probabilistic symbolic execution
Probabilistic symbolic execution aims at quantifying the probability of reaching program events of interest assuming that program inputs follow given probabilistic distributions. The technique collects constraints on the inputs that lead to the target events and analyzes them to quantify how likely it is for an input to satisfy the constraints. Current techniques either handle only linear constraints or only support continuous distributions using a “discretization” of the input domain, leading to imprecise and costly results. We propose an iterative distribution-aware sampling approach to support probabilistic symbolic execution for arbitrarily complex mathematical constraints and continuous input distributions. We follow a compositional approach, where the symbolic constraints are decomposed into sub-problems whose solution can be solved independently. At each iteration the convergence rate of the com- putation is increased by automatically refocusing the analysis on estimating the sub-problems that mostly affect the accuracy of the results, as guided by three different ranking strategies. Experiments on publicly available benchmarks show that the proposed technique improves on previous approaches in terms of scalability and accuracy of the results.
Probabilistic Program Analysis
This paper provides a survey of recent work on adapting techniques for program analysis to compute probabilistic characterizations of program behavior. We survey how the frameworks of data flow analysis and symbolic execution have incorporated information about input probability distributions to quantify the likelihood of properties of program states. We identify themes that relate and distinguish a variety of techniques that have been developed over the past 15 years in this area. In doing so, we point out opportunities for future research that builds on the strengths of different techniques.
OpenJDK’s Java. utils. Collection. sort () is broken: The good, the bad and the worst case
We investigate the correctness of TimSort, which is the main sorting algorithm provided by the Java standard library. The goal is functional verification with mechanical proofs. During our verification attempt we discovered a bug which causes the implementation to crash. We characterize the conditions under which the bug occurs, and from this we derive a bug-free version that does not compromise the performance. We formally specify the new version and mechanically verify the absence of this bug with KeY, a state-of-the-art verification tool for Java.
Computer Aided Verification
It is our great pleasure to welcome you to CAV 2015, the 27th International Conference on Computer-Aided Verification, held in San Francisco, California, during July 18–24, 2015. The CAV conference series is dedicated to the advancement of the theory and practice of computer-aided formal analysis methods for hardware and software systems. The conference covers the spectrum from theoretical results to concrete applications, with an emphasis on practical verification tools and the algorithms and techniques that are needed for their implementation. CAV considers it vital to continue spurring advances in hardware and software verification while expanding to new domains such as biological systems and computer security. The CAV 2015 program included five keynotes, technical papers (58 long and 11 short papers accepted out of 252 submissions), 11 co-located events (VSTTE – Verified Software: Theories, Tools, and Experiments; SMT – Satisfiability Modulo Theories, EC2, IPRA – Interpolation: From Proofs to Applications; SYNT – Synthesis; VeriSure – Verification and Assurance; HCVS – Horn Clauses for Verification and Synthesis; VMW – Verification Mentoring Workshop, REORDER, SNR – Symbolic and Numerical Methods for Reachability Analysis; VEMDP – Verification of Engineered Molecular Devices and Programs), the Artifact Evaluation as well as briefings from the SMT and Synthesis competitions. The invited keynote speakers were Philippa Gardner (Imperial College London), Leslie Lamport (Microsoft Research), Bob Kurshan (Cadence), William Hung (Synopsys), and Peter O’Hearn (University College London and Facebook). Many people worked hard to make CAV 2015 a success. We thank the authors and the keynote speakers for providing the excellent technical material, the Program Committee for their thorough reviews and the time spent on evaluating all the submissions and discussing them during the on-line discussion period, and the Steering Committee for their guidance throughout the planning for CAV 2015. We also thank Temesghen Kahsai, Local Chair, for his dedication and help with CAV 2015 planning and Hana Chockler, Sponsorship Chair, for helping to bring much needed financial support to the conference; Dirk Beyer, Workshop Chair, and all the organizers of the co-located events for bringing their events to the CAV week; Elizabeth Polgreen for the program and proceedings; Arie Gurfinkel, Temesghen Kahsai, Michael Tautschnig, and the Artifact Evaluation Committee for their work on evaluating the artifacts submitted. We gratefully acknowledge NSF for providing financial support for student participants. We sincerely thank the CAV sponsors for their generous contributions: – Google (Platinum sponsor) – NASA, Fujitsu, SGT, Facebook, Microsoft (Gold sponsors) – IBM, Cadence (Silver sponsors) – Intel, Samsung (Bronze sponsors)
Automated circular assume-guarantee reasoning
Compositional verification techniques aim to decompose the verification of a large system into the more manageable verification of its components. In recent years, compositional techniques have gained significant successes following a breakthrough in the ability to automate assume-guarantee reasoning. However, automation is still restricted to simple acyclic assume-guarantee rules. In this work, we focus on automating circular assume-guarantee reasoning in which the verification of individual components mutually depends on each other. We use a sound and complete circular assume-guarantee rule and we describe how to automatically build the assumptions needed for using the rule. Our algorithm accumulates joint constraints on the assumptions based on (spurious) counterexamples obtained from checking the premises of the rule, and uses a SAT solver to synthesize minimal assumptions that satisfy these constraints. We implemented our approach and compared it with an established learning-based method that uses an acyclic rule. In all cases, the assumptions generated for the circular rule were significantly smaller, leading to smaller verification problems. Further, on larger examples, we obtained a significant speedup as well.
Compositional symbolic execution with memoized replay
Symbolic execution is a powerful, systematic analysis that has received much visibility in the last decade. Scalability however remains a major challenge for symbolic execution. Compositional analysis is a well-known general purpose methodology for increasing scalability. This paper introduces a new approach for compositional symbolic execution. Our key insight is that we can summarize each analyzed method as a memoization tree that captures the crucial elements of symbolic execution, and leverage these memoization trees to efficiently replay the symbolic execution of the corresponding methods with respect to their calling contexts. Memoization trees offer a natural way to compose in the presence of heap operations, which cannot be dealt with by previous work that uses logical formulas as summaries for compositional symbolic execution. Our approach also enables efficient target oriented symbolic execution for error detection or program coverage. Initial experimental evaluation based on a prototype implementation in Symbolic Path Finder shows that our approach can be up to an order of magnitude faster than traditional non-compositional symbolic execution.
Symbolic execution for checking the accuracy of floating-point programs
Programs with oating-point calculations tend to give rise to hard-to-predict behavior. Such uncertainty cannot be ignored: oating-point errors can have catastrophic consequences, as it happened with the Patriot missile accident in 1991. The likelihood of such incidents can be decreased by using automated technology to reliably analyze numerical code. We present a symbolic execution approach to checking the accuracy of numerical programs, investigating how much a oating-point computation deviates from the "ideal" computation on real values. Our method is implemented in the Symbolic PathFinder tool and leverages and extends the floating-point decision procedure Realizer to check symbolic path constraints and to perform the accuracy checks. We further illustrate the possibility of using our tools to enhance abstract interpretation-based analyses to obtain tighter bounds on the numerical error introduced by oating-point computations. Initial experiments show the promise of our approach.
From Test Purposes to Formal JPF Properties
Software quality is traditionally achieved by applying testing techniques and/or formal verification to the system under development. Formal testing techniques have been defined to assure the software quality based on a set of test purposes the users want to observe on the system under testing (SUT). This paper proposes the use of Java PathFinder (JPF) to observe test purposes, defined as properties, over the whole program behavior. To make this approach available to test community, a user-friendly interface to define test purposes as properties is provided, together with a JPF extension to observe the test purposes. To define a property, the user only needs to select a property pattern, its scope, and the events of interest. Then, a finite-state automaton is generated and can be submitted to JPF to verify the defined property. As a result, JPF provides a report including a property verdict (pass/fail), and the program traces and percentage of the traversed automaton during the verification process. This coverage information can be used to analyze the performed verification and guide a complementary test suite generation.
Concurrent bounded model checking
We introduce a methodology, based on symbolic execution, for Concurrent Bounded Model Checking. In our approach, we translate a program into a formula in a disjunctive form. This design enables concurrent verification, with a main thread running symbolic execution, without any constraint solving, to build subformulas, and a set of worker threads running a decision procedure for satisfiability checks. We have implemented this methodology in a tool called JCBMC, the first bounded model checker for Java. JCBMC is built as an extension of Java Pathfinder, an open-source verification platform developed by NASA. JCBMC uses Symbolic PathFinder (SPF) for the symbolic execution, Z3 as the solver and implements concurrency with multi-threading. For evaluation, we compare JCBMC against SPF and the Bounded Model Checker CBMC. The results of the experiments show that we can achieve significant advantages of performance over these two tools.
Model counting for complex data structures
We extend recent approaches for calculating the probability of program behaviors, to allow model counting for complex data structures with numeric fields. We use symbolic execution with lazy initialization to compute the input structures leading to the occurrence of a target event, while keeping a symbolic representation of the constraints on the numeric data. Off-the-shelf model counting tools are used to count the solutions for numerical constraints and field bounds encoding data structure invariants are used to reduce the search space. The technique is implemented in the Symbolic PathFinder tool and evaluated on several complex data structures. Results show that the technique is much faster than an enumeration-based method that uses the Korat tool and also highlight the benefits of using the field bounds to speed up the analysis.
Assume-guarantee abstraction refinement meets hybrid systems
Compositional verification techniques in the assume-guarantee style have been successfully applied to transition systems to efficiently reduce the search space by leveraging the compositional nature of the systems under consideration. We adapt these techniques to the domain of hybrid systems with affine dynamics. To build assumptions we introduce an abstraction based on location merging. We integrate the assume-guarantee style analysis with automatic abstraction refinement. We have implemented our approach in the symbolic hybrid model checker SpaceEx. The evaluation shows its practical potential. To the best of our knowledge, this is the first work combining assume-guarantee reasoning with automatic abstraction-refinement in the context of hybrid automata.
Statistical symbolic execution with informed sampling
Symbolic execution techniques have been proposed recently for the probabilistic analysis of programs. These techniques seek to quantify the likelihood of reaching program events of interest, e.g., assert violations. They have many promising applications but have scalability issues due to high computational demand. To address this challenge, we propose a statistical symbolic execution technique that performs Monte Carlo sampling of the symbolic program paths and uses the obtained information for Bayesian estimation and hypothesis testing with respect to the probability of reaching the target events. To speed up the convergence of the statistical analysis, we propose Informed Sampling, an iterative symbolic execution that first explores the paths that have high statistical significance, prunes them from the state space and guides the execution towards less likely paths. The technique combines Bayesian estimation with a partial exact analysis for the pruned paths leading to provably improved convergence of the statistical analysis. We have implemented statistical symbolic execution with informed sampling in the Symbolic PathFinder tool. We show experimentally that the informed sampling obtains more precise results and converges faster than a purely statistical analysis and may also be more efficient than an exact symbolic analysis. When the latter does not terminate symbolic execution with informed sampling can give meaningful results under the same time and memory limits.
Rigorous examination of reactive systems
The goal of the RERS challenge is to evaluate the effectiveness of various verification and validation approaches on reactive systems, a class of systems that is highly relevant for industrial critical applications. The RERS challenge brings together researchers from different areas of software verification and validation, including static analysis, model checking, theorem proving, symbolic execution, and testing. The challenge provides a forum for experimental comparison of different techniques on specifically designed verification tasks. These benchmarks are automatically synthesized to exhibit chosen properties, and then enhanced to include dedicated dimensions of difficulty, such as conceptual complexity of the properties (e.g., reachability, safety, liveness), size of the reactive systems (a few hundred lines to millions of lines), and complexity of language features (arrays and pointer arithmetic). The STTT special section on RERS describes the results of the evaluations and the different analysis techniques that were used in the RERS challenges 2012 and 2013.
Special Issue on Formal Aspects of Component Software (Selected Papers from FACS'12)
This issue contains extended versions of selected papers from the 9th International Symposium on Formal Aspects of Component Software (FACS’12). The FACS symposium series addresses formal methods in the context of component-based and service-oriented software development. Formal methods provide a foundation for component-based software by successfully addressing challenging issues such as mathematical models for components, composition and adaptation, or rigorous approaches to verification, deployment, testing, and certification. FACS’12 was the 9th symposium in this series, and was held in Mountain View (USA) on September 12-14, 2012. For FACS’12, we received 40 submissions. After the review process, the international Program Committee decided to select 16 papers for presentation during the symposium and inclusion in the FACS’12 proceedings. From these 16 papers, the authors of six best papers were invited to submit an extended version to this special issue. These extended papers went through a rigorous peer review process. The revised versions of five papers were finally accepted and are included in this special issue. The papers included here provide key insights on different formal aspects of component software, covering topics ranging from real-time and communicating systems, interface theories, probabilistic verification assume-guarantee reasoning. The first article in this special issue, “Formal Patterns for Multirate Distributed Real-Time Systems”, by K. Bae et al., proposes multirate PALS (Physically Asynchronous, Logically Synchronous) as a formalized mathematical model providing a formal pattern that can drastically reduce the complexity of designing, verifying, and implementing multirate Distributed Real-Time Systems that must achieve virtual synchrony in an asynchronous setting. In particular, the authors prove that the entire DRTS design as a concurrent system of asynchronous components communicating in a network is bisimilar to a simpler synchronous multirate ensemble of state machines. This bisimilarity induces a significant reduction on the number of states, making model checking possible in many cases where it is unfeasible in the original DRTS setting. Multirate PALS is supported by Real-Time Maude for specification and model checking purposes, and is illustrated with a multirate hierarchical control system. The second article, “Avoiding Diamonds in Desynchronisation”, by H. Beohar et al., presents sufficient and necessary conditions under which a synchronous design is equivalent to an asynchronous one and formally proves that the so-called diamond property is no longer needed for desynchronisation when half-duplex queues are used as a communication buffer. These theoretical results are illustrated for desynchronising the synchronous systems that are synthesised using supervisory control theory. The third article in this special issue, “A Meta-Theory for Component Interfaces with Contracts and Ports”, by S. Bauer et al., presents a generic framework to construct a theory of component interfaces with port contracts on top of any 1 arbitrary labeled interface theory. The authors study reliable component interfaces and provide methodological guidelines how to design reliable interfaces and how to adapt them to changing environments. The approach was illustrated with two instantiations. First, the authors consider modal component interfaces such that component behaviors and the assume and guarantee behaviors of ports are given in terms of modal I/O (input-output)-transition systems with weak notions of refinement and compatibility. The second instance uses I/O-predicates as interface specifications. The fourth article, “Symbolic Counterexample Generation for Large DiscreteTime Markov Chains”, by N. Jansen et al., presents several symbolic counterexample generation algorithms for Discrete-Time Markov Chains (DTMCs) violating properties written in PCTL. A counterexample is a symbolic representation of a sub-DTMC that is incrementally generated. First, the authors extend bounded model checking and develop a simple heuristic to generate highly probable paths first. Second, they complement the SAT-based approach by a symbolic BDD-based technique. The experimental results show a substantially better scalability than existing explicit techniques. In particular, the BDDbased approach using a method called fragment search allows for counterexample generation for DTMCs with billions of states. The fifth article, “Compositional Assume-Guarantee Reasoning for Input/Output Component Theories”, by C. Chilton et al., introduces a sound and complete assume-guarantee framework for reasoning compositionally about components modelled as a variant of interface automata. A component is specified by finite traces and expresses both safety and progress properties of input and output interactions with the environment. The framework supports dynamic reasoning about components and specifications, and includes rules for parallel composition, logical conjunction and disjunction corresponding to independent development, and quotient for incremental synthesis. The framework is illustrated through a link layer protocol case study. Many people have contributed to this special issue, without whose effort this special issue would not have been possible. Besides the authors of the papers, we would like to thank both the members of the Program Committee of the symposium and the additional reviewers who kindly agreed to help us with the reviewing of the papers in this special issue. All carried out an excellent job during this demanding process: Erika Abraham (RWTH Aachen University, Germany), Farhad Arbab (CWI and Leiden University, The Netherlands), Christian Attiogb´e (University of Nantes, France), Christel Baier (Technical University of Dresden, Germany), Lu´ıs Barbosa (University of Minho, Portugal), Frank de Boer (CWI, The Netherlands), Roberto Bruni (University of Pisa, Italy), Carlos Canal (University of M´alaga, Spain), Jos´e Luiz Fiadeiro (University of Leicester, UK), Carlo Ghezzi (Politecnico di Milano, Italy), Rolf Hennicker (LudwigMaximilians-Universit¨at Munich, Germany), Sascha Kl¨uppelholz (Dresden University of Technology, Germany), Stefan Leue (University of Konstanz, Germany), Michael Lienhardt (University of Paris Diderot, France), Zhiming Liu (Birmingham City University, UK), Markus Lumpe (Swinburne University of Technology, Australia), Eric Madelaine (Inria, Centre Sophia Antipolis, France), 2 Sun Meng (Peking University, China), John Mullins (Polytechnical School of Montreal, Canada), Peter Olveczky (University of Oslo, Norway), Meriem Ouederni (Toulouse INP, France), Frantisek Plasil (Charles University, Czech Republic), Pascal Poizat (Universit´e Paris Ouest Nanterre La D´efense, France), Jos´e Proen¸ca (KU Leuven, Belgium), Shaz Qadeer (Microsoft, USA), John Rushby (SRI International, USA), Bernhard Sch¨atz (fortiss GmbH, Germany), Nishant Sinha (NEC Labs, Princeton, USA), Marjan Sirjani (Reykjavik University, Iceland), Volker Stolz (University of Oslo, Norway), Carolyn Talcott (SRI International, USA), Oksana Tkachuk (NASA Ames, USA), Sebastian Uchitel (University of Buenos Aires, Argentina), James Worrell (University of Oxford, UK), Lina Ye (Inria, France), Gianluigi Zavattaro (University of Bologna, Italy).
Exact and Approximate Probabilistic Symbolic Execution
Probabilistic software analysis seeks to quantify the likelihood of reaching a target event under uncertain environments. Recent approaches compute probabilities of execution paths using symbolic execution, but do not support nondeterminism. Nondeterminism arises naturally when no suitable probabilistic model can capture a program behavior, e.g., for multithreading or distributed systems. In this work, we propose a technique, based on symbolic execution, to synthesize schedulers that resolve nondeterminism to maximize the probability of reaching a target event. To scale to large systems, we also introduce approximate algorithms to search for good schedulers, speeding up established random sampling and reinforcement learning results through the quantification of path probabilities based on symbolic execution. We implemented the techniques in Symbolic PathFinder and evaluated them on nondeterministic Java programs. We show that our algorithms significantly improve upon a state-of- the-art statistical model checking algorithm, originally developed for Markov Decision Processes.
Exact and approximate probabilistic symbolic execution for nondeterministic programs
Probabilistic software analysis seeks to quantify the likelihood of reaching a target event under uncertain environments. Recent approaches compute probabilities of execution paths using symbolic execution, but do not support nondeterminism. Nondeterminism arises naturally when no suitable probabilistic model can capture a program behavior, e.g., for multithreading or distributed systems. In this work, we propose a technique, based on symbolic execution, to synthesize schedulers that resolve nondeterminism to maximize the probability of reaching a target event. To scale to large systems, we also introduce approximate algorithms to search for good schedulers, speeding up established random sampling and reinforcement learning results through the quantification of path probabilities based on symbolic execution. We implemented the techniques in Symbolic PathFinder and evaluated them on nondeterministic Java programs. We show that our algorithms significantly improve upon a state-of-the-art statistical model checking algorithm, originally developed for Markov Decision Processes.
On the probabilistic analysis of software (invited talk abstract)
Probabilistic software analysis aims at quantifying how likely a target event is to occur, given a probabilistic characterization of the behavior of a program or of its execution environment. Examples of target events may include an uncaught exception, the invocation of a certain method, or the access to confidential information.We are working on a symbolic execution approach to probabilistic software analysis that first computes the conditions to reach the target event, and then tries to quantify the fraction of the input domain satisfying these conditions. Unlike past approaches, that were mostly performed at model level, and were thus only applicable to early software design stages or required explicit (and hard to maintain) abstraction from the code, our techniques are performed directly at the code level. Our techniques are built on top of the Symbolic PathFinder symbolic execution tool. We discuss applications to the analysis of Unmanned Aerial Systems developed at NASA Ames.
Quantifying information leaks using reliability analysis
We report on our work-in-progress into the use of reliability analysis to quantify information leaks. In recent work we have proposed a software reliability analysis technique that uses symbolic execution and model counting to quantify the probability of reaching designated program states, e.g. assert violations, under uncertainty conditions in the environment. The technique has many applications beyond reliability analysis, ranging from program understanding and debugging to analysis of cyber-physical systems. In this paper we report on a novel application of the technique, namely Quantitative Information Flow analysis (QIF). The goal of QIF is to measure information leakage of a program by using information-theoretic metrics such as Shannon entropy or Renyi entropy. We exploit the model counting engine of the reliability analyzer over symbolic program paths, to compute an upper bound of the maximum leakage over all possible distributions of the confidential data. We have implemented our approach into a prototype tool, called QILURA, and explore its effectiveness on a number of case studies.
Compositional solution space quantification for probabilistic software analysis
Probabilistic software analysis aims at quantifying how likely a target event is to occur during program execution. Current approaches rely on symbolic execution to identify the conditions to reach the target event and try to quantify the fraction of the input domain satisfying these conditions. Precise quantification is usually limited to linear constraints, while only approximate solutions can be provided in general through statistical approaches. However, statistical approaches may fail to converge to an acceptable accuracy within a reasonable time. We present a compositional statistical approach for the efficient quantification of solution spaces for arbitrarily complex constraints over bounded floating-point domains. The approach leverages interval constraint propagation to improve the accuracy of the estimation by focusing the sampling on the regions of the input domain containing the sought solutions. Preliminary experiments show significant improvement on previous approaches both in results accuracy and analysis time.
Symbolic pathfinder v7
We describe Symbolic PathFinder v7 in terms of its updated design addressing the changes of Java PathFinder v7 and of its new optimization when computing path conditions. Furthermore, we describe the Symbolic Execution Tree Extension; a newly added feature that allows for outputting the symbolic execution tree that characterizes the execution paths covered during symbolic execution. The new extension can be tailored to the needs of subsequent analyses/processing facilities, and we demonstrate this by presenting SPF-Visualizer, which is a tool for customizable visualization of the symbolic execution tree.
Predicate Abstraction in Java Pathfinder
We present our ongoing effort to implement predicate abstraction in Abstract Pathfinder, which is an extension of Java Pathfinder. Our approach builds upon existing abstraction techniques that have been proposed mainly for low-level programs in C. We support predicates over variables having numerical data types. The main challenges that we have addressed include (1) the design of the predicate language, (2) support for arrays, (3) finding predicates affected by a given statement, (4) aliasing between variables, (5) propagating values of predicates over method call boundaries, and (6) computing weakest preconditions for complex predicates. We describe our solution to these challenges and selected details about the implementation. We also discuss our future plans and research ideas.
Reliability analysis in symbolic pathfinder: A brief summary
Designing a software for critical applications requires a precise assessment of reliability. Most of the reliability analysis techniques perform at the architecture level, driving the design since its early stages, but are not directly applicable to source code. We propose a general methodology based on symbolic execution of source code for extracting failure and success paths to be used for probabilistic reliability assessment against relevant usage scenarios. Under the assumption of finite and countable input domains, we provide an efficient implementation based on Symbolic PathFinder that supports the analysis of sequential and parallel Java programs, even with structured data types, at the desired level of confidence. We validated our approach on both NASA prototypes and other test cases showing a promising applicability scope
Abstraction and Learning for Infinite-State Compositional Verification
Despite many advances that enable the application of model checking techniques to the verification of large systems, the state-explosion problem remains the main challenge for scalability. Compositional verification addresses this challenge by decomposing the verification of a large system into the verification of its components. Recent techniques use learning-based approaches to automate compositional verification based on the assume-guarantee style reasoning. However, these techniques are only applicable to finite-state systems. In this work, we propose a new framework that interleaves abstraction and learning to perform automated compositional verification of infinite-state systems. We also discuss the role of learning and abstraction in the related context of interface generation for infinite-state components
Symbolic PathFinder: integrating symbolic execution with model checking for Java bytecode analysis
Symbolic PathFinder (SPF) is a software analysis tool that combines symbolic execution with model checking for automated test case generation and error detection in Java bytecode programs. In SPF, programs are executed on symbolic inputs representing multiple concrete inputs and the values of program variables are represented by expressions over those symbolic inputs. Constraints over these expressions are generated from the analysis of different paths through the program. The constraints are solved with off-the-shelf solvers to determine path feasibility and to generate test inputs. Model checking is used to explore different symbolic program executions, to systematically handle aliasing in the input data structures, and to analyze the multithreading present in the code. SPF incorporates techniques for handling input data structures, strings, and native calls to external libraries, as well as for solving complex mathematical constraints. We describe the tool and its application at NASA, in academia, and in industry.
Multi-solver support in symbolic execution
One of the main challenges of dynamic symbolic execution—an automated program analysis technique which has been successfully employed to test a variety of software—is constraint solving. A key decision in the design of a symbolic execution tool is the choice of a constraint solver. While different solvers have different strengths, for most queries, it is not possible to tell in advance which solver will perform better. In this paper, we argue that symbolic execution tools can, and should, make use of multiple constraint solvers. These solvers can be run competitively in parallel, with the symbolic execution engine using the result from the best-performing solver. We present empirical data obtained by running the symbolic execution engine KLEE on a set of real programs, and use it to highlight several important characteristics of the constraint solving queries generated during symbolic execution. In particular, we show the importance of constraint caching and counterexample values on the (relative) performance of KLEE configured to use different SMT solvers. We have implemented multi-solver support in KLEE, using the metaSMT framework, and explored how different state-of-the-art solvers compare on a large set of constraint-solving queries. We also report on our ongoing experience building a parallel portfolio solver in KLEE.
Symbolic Execution and Software Testing.
Techniques for checking complex software range from model checking and static analysis to testing. We aim to use the power of exhaustive techniques, such as model checking and symbolic execution, to enable thorough testing of complex software. In particular, we have extended the Java PathFinder model checking tool (JPF) [3] with a symbolic execution capability [4, 2] to enable test case generation for Java programs. Our techniques handle complex data structures, arrays, as well as multithreading, and generate optimized test suites that satisfy user-specified testing coverage criteria. Programs are executed on symbolic, rather than concrete, inputs; the variable values are represented as expressions and constraints that reflect the code structure. JPF generates and analyzes different symbolic execution paths. The input constraints for one path are solved (using off-the-shelf constraint solvers) to generate tests that are guaranteed to execute that path. To bound the search space we put a limit on the model checking search depth, or on the number of constraints along one path. Alternatively, we use abstract state matching [1], which enables JPF to analyze an under-approximation of the program behavior. Our techniques have been used in black box and white box fashion [5]. They have been applied to generate test sequences for object-oriented code [6] and test vectors for NASA software. Recently, we have also applied our techniques to (executable) models – using a JPF extension for UML Statecharts.
Reliability analysis in symbolic pathfinder
Software reliability analysis tackles the problem of predicting the failure probability of software. Most of the current approaches base reliability analysis on architectural abstractions useful at early stages of design, but not directly applicable to source code. In this paper we propose a general methodology that exploit symbolic execution of source code for extracting failure and success paths to be used for probabilistic reliability assessment against relevant usage scenarios. Under the assumption of finite and countable input domains, we provide an efficient implementation based on Symbolic PathFinder that supports the analysis of sequential and parallel programs, even with structured data types, at the desired level of confidence. The tool has been validated on both NASA prototypes and other test cases showing a promising applicability scope.
Memoise: a tool for memoized symbolic execution
This tool paper presents a tool for performing memoized symbolic execution (Memoise), an approach we developed in previous work for more efficient application of symbolic execution. The key idea in Memoise is to allow re-use of symbolic execution results across different runs of symbolic execution without having to re-compute previously computed results as done in earlier approaches. Specifically, Memoise builds a trie-based data structure to record path exploration information during a run of symbolic execution, optimizes the trie for the next run, and re-uses the resulting trie during the next run. Our tool optimizes symbolic execution in three standard scenarios where it is commonly applied: iterative deepening, regression analysis, and heuristic search. Our tool Memoise builds on the Symbolic PathFinder framework to provide more efficient symbolic execution of Java programs and is available online for download. The tool demonstration video is available at http://www.youtube.com/watch?v=ppfYOB0Z2vY.
Polyglot: systematic analysis for multiple statechart formalisms
Polyglot is a tool for the systematic analysis of systems integrated from components built using multiple Statechart formalisms. In Polyglot, Statechart models are translated into a common Java representation with pluggable semantics for different Statechart variants. Polyglot is tightly integrated with the Java Pathfinder verification tool-set, providing analysis and test-case generation capabilities. The tool has been applied in the context of safety-critical software systems whose interacting components were modeled using multiple Statechart formalisms.
Formal Aspects of Component Software: 9th International Symposium, FACS 2012, Mountain View, CA, USA, September 11-13, 2012. Revised Selected Papers
This book constitutes the revised selected papers of the 9th International Symposium on Formal Aspects of Component Software, FACS 2012, held in Mountain View, CA, USA in September 2012. The 16 full papers presented were carefully reviewed and selected from 40 submissions. They cover topics such as formal models for software components and their interaction; formal aspects of services, service oriented architectures, business processes, and cloud computing; design and verification methods for software components and services; composition and deployment: models, calculi, languages; formal methods and modeling languages for components and services; model based and GUI based testing of components and services; models for QoS and other extra-functional properties (e.g., trust, compliance, security) of components and services; components for real-time, safety-critical, secure, and/or embedded systems; industrial or experience reports and case studies; update and reconfiguration of component and service architectures; component systems evolution and maintenance; autonomic components and self-managed applications; formal and rigorous approaches to software adaptation and self-adaptive systems.
Formal Aspects of Component Software
This book constitutes the revised selected papers of the 9th International Symposium on Formal Aspects of Component Software, FACS 2012, held in Mountain View, CA, USA in September 2012. The 16 full papers presented were carefully reviewed and selected from 40 submissions. They cover topics such as formal models for software components and their interaction; formal aspects of services, service oriented architectures, business processes, and cloud computing; design and verification methods for software components and services; composition and deployment: models, calculi, languages; formal methods and modeling languages for components and services; model based and GUI based testing of components and services; models for QoS and other extra-functional properties (e.g., trust, compliance, security) of components and services; components for real-time, safety-critical, secure, and/or embedded systems; industrial or experience reports and case studies; update and reconfiguration of component and service architectures; component systems evolution and maintenance; autonomic components and self-managed applications; formal and rigorous approaches to software adaptation and self-adaptive systems.
Compositional symbolic execution through program specialization
Scalability is a major challenge in symbolic execution. The large number of paths that need to be explored and the large size of the constraints that must be carried often compromise the effectiveness of symbolic execution for software testing in practice. Compositional symbolic execution aims to alleviate these scalability issues by executing the methods of a program separately, stowing their results in method summaries and using such summaries to incrementally execute the complete program. We present a novel compositional approach that leverages partial evaluation, a well-established technique that aims at automatically specializing a program with respect to some of its input. We report on its design and implementation in Symbolic PathFinder and on preliminary promising evaluation results.
Tools to generate and check consistency of model classes for Java PathFinder
Java PathFinder (JPF) is a model checker for Java applications. Like any other model checker, JPF has to combat the notorious state space explosion problem. Since JPF is a JVM, it can only model check Java bytecode and needs to handle native calls differently. JPF tackles the state space explosion problem and handles native calls by means of so-called model classes and native peers. In this paper we focus on model classes. For a class that either causes a state space explosion or that contains native calls, one can introduce a model class that either abstracts away particular details or implements the native call in Java. Rather than model checking the original class, JPF model checks the model class instead. Writing such model classes is time consuming and error prone. In this paper we propose two tools to assist with the development of model classes. The one tool generates a skeleton of a model class. The other tool checks whether a model class is consistent with the original class.
Symbolic quantitative information flow
Quantitative Information Flow (QIF) is a powerful approach to quantify leaks of confidential information in a software system. Here we present a novel method that precisely quanties information leaks. In order to mitigate the state-space explosion problem, we propose a symbolic representation of data, and a general SMT-based framework to explore systematically the state space. Symbolic Execution fits well with our framework, so we implement a method of QIF analysis employing Symbolic Execution. We develop our method as a prototype tool that can perform QIF analysis for a software system developed in Java. The tool is built on top of Java Pathfinder, an open source model checking platform, and it is the first tool in the field to support information-theoretic QIF analysis.
Testing android apps through symbolic execution
There is a growing need for automated testing techniques aimed at Android apps. A critical challenge is the systematic generation of test cases. One method of systematically generating test cases for Java programs is symbolic execution. But applying symbolic execution tools, such as Symbolic Pathfinder (SPF), to generate test cases for Android apps is challenged by the fact that Android apps run on the Dalvik Virtual Machine (DVM) instead of JVM. In addition, Android apps are event driven and susceptible to path-divergence due to their reliance on an application development framework. This paper provides an overview of a two-pronged approach to alleviate these issues. First, we have developed a model of Android libraries in Java Pathfinder (JPF) to enable execution of Android apps in a way that addresses the issues of incompatibility with JVM and path-divergence. Second, we have leveraged program analysis techniques to correlate events with their handlers for automatically generating Android-specific drivers that simulate all valid events.
Learning techniques for software verification and validation
Learning techniques are being used increasingly to improve software verification and validation activities. For example, automata learning techniques have been used for extracting behavioral models of software systems, e.g. [8]. These models can serve as formal documentation of the software and they can be verified using automated tools or used for model-based testing. Automata learning techniques have also been used for automating compositional verification, e.g. [3], for building abstractions of software behavior in the context of symbolic or parameterized model checking, e.g. [9] or for the automatic inference and security analysis of botnet protocols, e.g. [1]. This Special Track aims at bringing together researchers and practitioners working on the integration of learning techniques in verification and validation activities for software systems. The Special Track is part of the 2012 International Symposium on Leveraging Applications of Formal Methods, Verification, and Validation (ISoLA).
Improving symbolic execution for statechart formalisms
Symbolic execution is a program analysis technique that attempts to explore all possible paths through a program by using symbolic values rather than actual data values as inputs. When applied to Statecharts, a model-based formalism for reactive systems, symbolic execution can determine all feasible paths through a model up to a specified bound and generate input sequences exercising these paths. The main drawback of this method is its computational expense. This paper describes two efforts to improve the performance of symbolic execution within our previously developed framework for Statechart analysis. One method is a multithreaded symbolic execution engine targeted directly at our framework. A second, orthogonal, method is program specialization with respect to a particular model and Statechart semantics, which uses symbolic execution to rewrite the original code into an equivalent form that has fewer instructions and is easier to analyze.
Memoized symbolic execution
This paper introduces memoized symbolic execution (Memoise), a new approach for more efficient application of forward symbolic execution, which is a well-studied technique for systematic exploration of program behaviors based on bounded execution paths. Our key insight is that application of symbolic execution often requires several successive runs of the technique on largely similar underlying problems, e.g., running it once to check a program to find a bug, fixing the bug, and running it again to check the modified program. Memoise introduces a trie-based data structure that stores the key elements of a run of symbolic execution. Maintenance of the trie during successive runs allows re-use of previously computed results of symbolic execution without the need for re-computing them as is traditionally done. Experiments using our prototype implementation of Memoise show the benefits it holds in various standard scenarios of using symbolic execution, e.g., with iterative deepening of exploration depth, to perform regression analysis, or to enhance coverage using heuristics.
Assume-guarantee abstraction refinement for probabilistic systems
We describe an automated technique for assume-guarantee style checking of strong simulation between a system and a specification, both expressed as non-deterministic Labeled Probabilistic Transition Systems (LPTSes). We first characterize counterexamples to strong simulation as stochastic trees and show that simpler structures are insufficient. Then, we use these trees in an abstraction refinement algorithm that computes the assumptions for assume-guarantee reasoning as conservative LPTS abstractions of some of the system components. The abstractions are automatically refined based on tree counterexamples obtained from failed simulation checks with the remaining components. We have implemented the algorithms for counterexample generation and assume-guarantee abstraction refinement and report encouraging results.
Learning probabilistic systems from tree samples
We consider the problem of learning a non-deterministic probabilistic system consistent with a given finite set of positive and negative tree samples. Consistency is defined with respect to strong simulation conformance. We propose learning algorithms that use traditional and a new stochastic state-space partitioning, the latter resulting in the minimum number of states. We then use them to solve the problem of active learning, that uses a knowledgeable teacher to generate samples as counterexamples to simulation equivalence queries. We show that the problem is undecidable in general, but that it becomes decidable under a suitable condition on the teacher which comes naturally from the way samples are generated from failed simulation checks. The latter problem is shown to be undecidable if we impose an additional condition on the learner to always conjecture a minimum state hypothesis. We therefore propose a semi-algorithm using stochastic partitions. Finally, we apply the proposed (semi-) algorithms to infer intermediate assumptions in an automated assume-guarantee verification framework for probabilistic systems.
Combining model checking and symbolic execution for software testing
Techniques for checking complex software range from model checking and static analysis to testing. Over the years, we have developed a tool, Symbolic PathFinder (SPF), that aims to leverage the power of systematic analysis techniques, such as model checking and symbolic execution, for thorough testing of complex software. Symbolic PathFinder analyzes Java programs by systematically exploring a symbolic representation of the programs' behaviors and it generates test cases that are guaranteed to cover the explored paths. The tool also analyzes different thread inter-leavings and it checks properties of the code during test generation. Furthermore, SPF uses off-the-shelf decision procedures to solve mixed integer-real constraints and uses "lazy initialization" to handle complex input data structures. Recently, SPF has been extended with "mixed concrete-symbolic" constraint solving capabilities, to handle external library calls and to address decision procedures' incompleteness. The tool is part of the Java PathFinder open-source tool-set and has been applied in many projects at NASA, in industry and in academia. We review the tool and its applications and we discuss how it compares with related, "dynamic" symbolic execution approaches.
Statechart analysis with symbolic pathfinder
We report here on our on-going work that addresses the automated analysis and test case generation for software systems modeled using multiple Statechart formalisms. The work is motivated by large programs such as NASA Exploration, that involve multiple systems that interact via safety-critical protocols and are designed with different Statechart variants. To verify these safety-critical systems, we have developed Polyglot, a framework for modeling and analysis of model-based software written using different Statechart formalisms. Polyglot uses a common intermediate representation with customizable Statechart semantics and leverages the analysis and test generation capabilities of the Symbolic PathFinder tool. Polyglot is used as follows. First, the structure of the Statechart model (expressed in Matlab Stateflow or Rational Rhapsody) is translated into a common intermediate representation (IR). The IR is then translated into Java code that represents the structure of the model. The semantics are provided as “pluggable” modules.
Symbolic execution with interval solving and meta-heuristic search
A challenging problem in symbolic execution is to solve complex mathematical constraints such as constraints that include floating-point variables and transcendental functions. The inability to solve such constraints limit the application scope of symbolic execution. In this paper, we present a new method to solve such complex math constraints. Our method combines two existing: meta-heuristic search and interval solving. Conceptually, the combination explores the synergy of the individual methods to improve constraint solving. We implemented the new method in the CORAL constraint-solving infrastructure, and evaluated its effectiveness on a set of publicly-available software from the aerospace domain. Results indicate that the new method can solve significantly more complex mathematical constraints than previous techniques.
Integrating statechart components in polyglot
Statecharts is a model-based formalism for simulating and analyzing reactive systems. In our previous work, we developed Polyglot, a unified framework for analyzing different semantic variants of Statechart models. However, for systems containing communicating, asynchronous components deployed on a distributed platform, additional features not inherent to the basic Statecharts paradigm are needed. These include a connector mechanism for communication, a scheduling framework for sequencing the execution of individual components, and a method for specifying verification properties spanning multiple components. This paper describes the addition of these features to Polyglot, along with an example NASA case study using these new features. Furthermore, the paper describes on-going work on modeling Plexil execution plans with Polyglot, which enables the study of interaction issues for future manned and unmanned missions.
Assume-Guarantee Abstraction Refinement for Probabilistic Systems
We describe an automated technique for assume-guarantee style checking of strong simulation between a system and a specification, both expressed as non-deterministic Labeled Probabilistic Transition Systems (LPTSes). We first characterize counterexamples to strong simulation as "stochastic" trees and show that simpler structures are insufficient. Then, we use these trees in an abstraction refinement algorithm that computes the assumptions for assume-guarantee reasoning as conservative LPTS abstractions of some of the system components. The abstractions are automatically refined based on tree counterexamples obtained from failed simulation checks with the remaining components. We have implemented the algorithms for counterexample generation and assume-guarantee abstraction refinement and report encouraging results.
Symbolic execution enhanced system testing
We describe a testing technique that uses information computed by symbolic execution of a program unit to guide the generation of inputs to the system containing the unit, in such a way that the unit’s, and hence the system’s, coverage is increased. The symbolic execution computes unit constraints at run-time, along program paths obtained by system simulations. We use machine learning techniques –treatment learning and function fitting– to approximate the system input constraints that will lead to the satisfaction of the unit constraints. Execution of system input predictions either uncovers new code regions in the unit under analysis or provides information that can be used to improve the approximation. We have implemented the technique and we have demonstrated its effectiveness on several examples, including one from the aerospace domain.
Assume-Guarantee Abstraction Refinement for Probalistic Systems
We describe an automated technique for assume-guarantee style checking of strong simulation between a system and a specification, both expressed as non-deterministic Labeled Probabilistic Transition Systems (LPTSes). We first characterize counterexamples to strong simulation as "stochastic" trees and show that simpler structures are insufficient. Then, we use these trees in an abstraction refinement algorithm that computes the assumptions for assume-guarantee reasoning as conservative LPTS abstractions of some of the system components. The abstractions are automatically refined based on tree counterexamples obtained from failed simulation checks with the remaining components. We have implemented the algorithms for counterexample generation and assume-guarantee abstraction refinement and report encouraging results.
Investigating termination of affine loops with jpf
We present some preliminary work on how to discover infinite paths through while loop programs in which the variables in the loop condition are only transformed with affine functions. The infinite paths gathered in this manner are repetitive, that is, after a fixed number of iterations the loop condition is no nearer to being violated than it was initially. A proof is given that shows that this period of repetition is 2 for the one variable case, while for the two variable case simulations suggest that the maximum period is at least 6, but a fixed period is not yet known. The algorithm is implemented as a listener in Symbolic Java PathFinder, and this implementation formed part of the Google Summer of Code 2011.
Automated Software Engineering (ASE 2011)
The following topics are dealt with: software testing; software model checking; software analysis; software verification; software validation; software debugging; software traceability; program understanding; software maintenance; knowledge acquisition and software processes.
Polyglot: modeling and analysis for multiple statechart formalisms
In large programs such as NASA Exploration, multiple systems that interact via safety-critical protocols are already designed with different Statechart variants. To verify these safety-critical systems, a unified framework is needed based on a formal semantics that captures the variants of Statecharts. We describe Polyglot, a unified framework for the analysis of models described using multiple State-chart formalisms. In this framework, Statechart models are translated into Java and analyzed using pluggable semantics for different variants operating in a polymorphic execution environment. The framework has been built on the basis of a parametric formal semantics that captures the common core of Statecharts with extensions for different variants, and addresses previous limitations. Polyglot has been integrated with the Java Pathfinder verification tool-set, providing analysis and test-case generation capabilities. We describe the application of this unified framework to the analysis of NASA/JPL's MER Arbiter whose interacting components were modeled using multiple Statechart formalisms.
Symbolic execution with mixed concrete-symbolic solving
Symbolic execution is a powerful static program analysis technique that has been used for the automated generation of test inputs. Directed Automated Random Testing (DART) is a dynamic variant of symbolic execution that initially uses random values to execute a program and collects symbolic path conditions during the execution. These conditions are then used to produce new inputs to execute the program along different paths. It has been argued that DART can handle situations where classical static symbolic execution fails due to incompleteness in decision procedures and its inability to handle external library calls. We propose here a technique that mitigates these previous limitations of classical symbolic execution. The proposed technique splits the generated path conditions into (a) constraints that can be solved by a decision procedure and (b) complex non-linear constraints with uninterpreted functions to represent external library calls. The solutions generated from the decision procedure are used to simplify the complex constraints and the resulting path conditions are checked again for satisfiability. We also present heuristics that can further improve our technique. We show how our technique can enable classical symbolic execution to cover paths that other dynamic symbolic execution approaches cannot cover. Our method has been implemented within the Symbolic PathFinder tool and has been applied to several examples, including two from the NASA domain.
Context synthesis
With the advent of component-based and distributed software development, service-oriented computing, and other such concepts, components are no longer viewed as parts of specific systems, but rather as open systems that can be reused, or connected dynamically, in a variety of environments to form larger systems. Reasoning about components as open systems is different from reasoning about closed systems, since property satisfaction may depend on the context in which a component may be introduced. Component interfaces are an important feature of open sytems, since interfaces summarize the expectations that a component has from the contexts in which it gets introduced. Traditionally, component interfaces have been of a purely syntactic form, including information about the services/methods that can be invoked on the component, and their signatures, meaning the numbers and types of arguments and their return values. However, there is a recognized need for richer interfaces that capture additional aspects of a component. For example, interfaces may characterize legal sequences of invocations to component services. Generating compact and yet useful component interfaces is a challenging task to perform manually. Over the last decade, several approaches have been developed for performing context synthesis, i.e., generating component interfaces automatically. This tutorial mostly reviews such techniques developed by the authors, but also discusses alternative techniques for context synthesis.
Rapid property specification and checking for model-based formalisms
In model-based development, verification techniques can be used to check whether an abstract model satisfies a set of properties. Ideally, implementation code generated from these models can also be verified against similar properties. However, the distance between the property specification languages and the implementation makes verifying such generated code difficult. Optimizations and renamings can blur the correspondence between the two, further increasing the difficulty of specifying verification properties on the generated code. This paper describes methods for specifying verification properties on abstract models that are then checked on implementation level code. These properties are translated by an extended code generator into implementation code and special annotations that are used by a software model checker.
Interface decomposition for service compositions
Service-based applications can be realized by composing existing services into new, added-value composite services. The external services with which a service composition interacts are usually known by means of their syntactical interface. However, an interface providing more information, such as a behavioral specification, could be more useful to a service integrator for assessing that a certain external service can contribute to fulfill the functional requirements of the composite application. Given the requirements specification of a composite service, we present a technique for obtaining the behavioral interfaces - in the form of labeled transition systems - of the external services, by decomposing the global interface specification that characterizes the environment of the service composition. The generated interfaces guarantee that the service composition fulfills its requirements during the execution. Our approach has been implemented in the LTSA tool and has been applied to two case studies.
Symbolic execution for software testing in practice: preliminary assessment
We present results for the "Impact Project Focus Area" on the topic of symbolic execution as used in software testing. Symbolic execution is a program analysis technique introduced in the 70s that has received renewed interest in recent years, due to algorithmic advances and increased availability of computational power and constraint solving technology. We review classical symbolic execution and some modern extensions such as generalized symbolic execution and dynamic test generation. We also give a preliminary assessment of the use in academia, research labs, and industry.
CORAL: solving complex constraints for symbolic pathfinder
Symbolic execution is a powerful automated technique for generating test cases. Its goal is to achieve high coverage of software. One major obstacle in adopting the technique in practice is its inability to handle complex mathematical constraints. To address the problem, we have integrated CORAL’s heuristic solvers into NASA Ames’ Symbolic PathFinder symbolic execution tool. CORAL’s solvers have been designed to deal with mathematical constraints and their heuristics have been improved based on examples from the aerospace domain. This integration significantly broadens the application of Symbolic PathFinder at NASA and in industry.
The problem of interest is to verify data consistency of a concurrent Java program. In particular, we present a new decision procedure for verifying that a class of data...
The problem of interest is to verify data consistency of a concurrent Java program. In particular, we present a new decision procedure for verifying that a class of data races caused by inconsistent accesses on multiple fields of an object cannot occur (so-called atomic-set serializability). Atomic-set serializability generalizes the ordinary notion of a data race (i.e., inconsistent coordination of accesses on a single memory location) to a broader class of races that involve accesses on multiple memory locations. Previous work by some of the authors presented a technique to abstract a concurrent Java program into an EML program, a modeling language based on pushdown systems and a finite set of reentrant locks. Our previous work used only a semi-decision procedure, and hence provides a defi- nite answer only some of the time. In this paper, we rectify this shortcoming by developing a decision procedure for verifying data consistency, i.e., atomic-set serializability, of an EML program. When coupled with the previous work, it provides a decision procedure for verifying data consistency of a concurrent Java program. We implemented the decision procedure, and applied it to detect both single-location and multi-location data races in models of concurrent Java programs. Compared with the prior method based on a semi-decision procedure, not only was the decision procedure 34 times faster overall, but the semidecision procedure timed out on about 50% of the queries, whereas the decision procedure timed out on none of the queries.
Special Issue: Formal Methods in Aerospace
The critical and central nature of computation within Aerospace Systems naturally leads to the development of formalized investigation methods, primarily aimed at improving reliability, predictability and safety. The broad term “Aerospace” can cover many platforms, from piloted fixed/rotary wing vehicles, through unmanned air systems, to satellites and space probes. There are many techniques and technologies used within Aerospace where formal analysis is at least welcome and is often essential: flight control, “detect and avoid” mechanisms, fault diagnosis and recovery, autonomous docking and landing, coordination of multiple vehicles, autonomous decision-making, etc. The complexity of computations within Aerospace applications means that no single formalism is universally suitable for all scenarios, and so many techniques have been utilized, from across many distinct disciplines. Specifically, work in this area underlines the importance of research problems derived from Aerospace scenarios to the Logic, Mathematics and Artificial Intelligence communities. In addition, new techniques within, and new ways of combining techniques from, Logic, Artificial Intelligence, Hybrid Systems, Control Engineering, etc., are vital to progress in the Aerospace area. Logic-based formal methods provide a link between many of the different techniques we might wish to explore. For example using logic-based methods we can express properties of complex temporal structures, of hybrid and continuous systems, of probabilistic behaviours, of resource-boundedness and reliability, of concurrency and of autonomy. Thus, the wide diversity of Aerospace systems provides a strong source of new problems for formal logical, mathematical, or AI techniques. Such systems can be involved in complex activities such as space exploration, telecommunication support, disaster monitoring, environmental sensing, mapping, weather prognoses, search and rescue, naval traffic surveillance, etc. From these applications, new requirements appear: autonomy, collective behaviour, information fusion, cognitive skills, coordination, etc. In addition, new concepts must be formalised: digital pheromones, swarms, systems of systems of robots, sensing, physical actuation, and so on. Aerospace is a complex area and so often requires extension and integration of formal modelling and analysis approaches with techniques from other disciplines, and many such opportunities are now appearing. A good example is the problem of coordination for platoons of UAVs or satellites, which have been successfully tackled using various techniques from control engineering and numerical tools from dynamic programming. In addition, there exist an abundance of examples of AI techniques in Aerospace (target tracking, rover planning, multi-agent technologies and so on). The implementation of these methods could benefit from formal analysis and development. From the cross-fertilization of related multidisciplinary approaches, we expect more robust, safe and mechanisable modelling, development and verification methods for Aerospace systems. In this special issue, we highlight three different directions concerning the application and development of formal methods within Aerospace. In “Formal Testing for Separation Assurance”, Giannakopoulou, Bushnell, Schumann, Erzberger, and Heere, consider the crucial problem of controlling air traffic in our increasingly crowded skies. In particular, they use formal testing techniques to assess whether aircraft will get closer to each other than a minimum safe distance (called “loss of separation”). They develop a range of automated test-case generation techniques that are based on model checking, and assess their viability for use in ensuring safe separation of air vehicles. In “Kripke Modelling and Verification of Temporal Specifications of a Multiple UAV System”, Sirigineedi, Tsourdos, White, and Zbikowski consider a quite different approach. Their context is of a collection of unmanned air vehicles coordinating with each other to carry out ground monitoring. They logically model the scenario, and show how a temporal logic model-checker, SMV, can be used to verify correctness conditions of such cooperating air vehicles. Finally, in “Using Formal Methods with SysML in Aerospace Design and Engineering”, Graves and Bijan consider another approach, distinct from testing and verification. In particular, they address the problem of how one can actually design and develop Aerospace systems in a reliable way and their approach is to combine model-based systems engineering with formal methods to alleviate some of the problems in Aerospace systems development. In particular, they use a SysML based Model-Based System Engineering process to describe and develop models for typical air systems and their operating contexts. The authors couple this with formalmethods based on theorem-proving within type theory in order to show how proofbased verification can be utilized within this model-based approach. While the papers in this special issue cover very different approaches, there is, of course, a vast range of topics beyond those considered here, for example: formal verification of hybrid systems; formal models for cyber-physical systems; autonomous and autonomic systems; performance modelling and verification; multiagent systems and coordination technologies; stochastic modelling and verification methods; etc. Indeed, there are active research events tackling such problems specifically within the Aerospace domain, including Formal Methods in Aerospace (FMA) workshop series http://personalpages.manchester.ac.uk/staff/Manuela.Bujorianu/FMA.htm NASA Formal Methods (NFM) symposium series http://shemesh.larc.nasa.gov/nfm2012
Interface Decomposition for Service Compositions
Service-based applications can be realized by composing existing services into new, added-value composite services. The external services with which a service composition interacts are usually known by means of their syntactical interface. However, an interface providing more information, such as a behavioral specification, could be more useful to a service integrator for assessing that a certain external service can contribute to ful fill the functional requirements of the composite application. Given the requirements specification of a composite service, we present a technique for obtaining the behavioral interfaces - in the form of labeled transition systems - of the external services, by decomposing the global interface speci fication that characterizes the environment of the service composition. The generated interfaces guarantee that the service composition fulfills its requirements during the execution. Our approach has been implemented in the LTSA tool and has been applied to two case studies.
New results in software model checking and analysis
Modern software, which often involves complex concurrent computations and operates in an uncertain environment, must be highly reliable and secure. Commonly used techniques for addressing the reliability and safety of modern software systems include model checking and testing. Testing is widely used but it usually involves manual effort and it is ill-suited for finding concurrency errors. Model checking [3], on the other hand, has shown great promise in finding subtle program errors in a completely automated way. Given a (model of a) system and a property, model checking systematically enumerates, explicitly or symbolically, all the (reachable) system configurations and it checks if they conform with the property. The result of model checking is either “true”, if the property holds, or “false” if the property does not hold; in the latter case the model checking procedure also provides a detailed counter-example trace that leads to the property violation. Properties of interest include absence of deadlocks and data races in concurrent programs, or more general assertions and temporal logic formulae. Such formulae encode the expected behavior of the system in terms of safety and liveness, as well as timed, probabilistic or security properties. To ensure that the model checking terminates, some form of abstraction is usually necessary to reduce the large search space for the original system into a smaller one that is more amenable for verification. Alternately, model checking can be used as an effective bug-finding technique, and the detailed counter-example traces provided can help debugging the discovered errors. The number of possible system configurations that needs to be explored is very large for most realistic practical applications. Consequently there has been C. S. P˘as˘areanu (B) Carnegie Mellon University/NASA Ames Research Center, Moffett Field, CA 94035, USA e-mail: corina.s.pasareanu@nasa.gov continuous effort spent over the years to address this scalability problem. The articles enclosed here describe new model checking techniques, supported by robust and scalable tools, for the automated analysis of modern software systems. The articles have been carefully reviewed and they are based on papers that were considered to be among the best at the SPIN 2009 model checking workshop [11]. The topics addressed range from probabilistic model checking and parallelization techniques for improved scalability to data race detection, symbolic analysis and model checking for security. The first article [5], presents an effective technique for computing the probability of reaching a given set of states in a parametric Markov model. Such models can be used to reason about quantitative properties in systems where certain aspects are not fixed, but rather depend on parameters. Previous work [4] has suggested to convert the Markov chain into a finite automaton, equivalent to a regular expression. The expression can be evaluated to a closed form function representing the reachability probability. The bottleneck of the approach lies in the growth of the regular expression with the number of states. The authors propose to remedy the problem by intertwining the regular expression computation with its evaluation. This results into a practical method that has been implemented in the PARAM tool and has been demonstrated experimentally on network protocols. The second article [2] is also concerned with probabilistic reasoning, in the context of the PRISM model checking tool [9], where the satisfaction of desired properties is quantified with some probability. The authors propose algorithms for parallel probabilistic model checking using general purpose graphic processing units. The proposed improvements target the numerical computations of the traditional sequential algorithms since these computations can be parallelized effi- ciently on graphic processors. The parallel algorithms have 123 2 C. S. P˘as˘areanu been implemented in the PRISM model checker and have been evaluated on several case studies, showing significant speed-up. The third article [8] addresses the problem of verifying data consistency in concurrent Java programs. The work targets data races caused by inconsistent accesses to multiple fields of an object—the so-called atomic-set serializability problem. Previous work used abstraction techniques to translate a concurrent Java program into an EML program, a modeling language based on push-down systems and a finite set of re-entrant locks, and used only a semi-decision procedure to check the program. The present article extends that work by describing a full decision procedure for verifying data consistency, i.e., atomic-set serializability, of an EML program. The procedure has been implemented and it has been applied to detect both single-location and multi-location data races in models of concurrent Java programs. The fourth article [10] presents a generic technique for creating the basic primitives used in symbolic program analysis: forward symbolic evaluation, weakest liberal precondition, and symbolic composition. Using this technique, one can automatically generate an implementation of a (forward or backward) symbolic program execution at the cost of writing only the specification of the concrete program semantics— in the form of an interpreter for the language of interest. The technique can be used for programming languages with pointers and arithmetic operations. The technique has been implemented and it has been used to generate symbolicanalysis primitives for the x86 and PowerPC instruction sets. The symbolic analysis generated with the generic technique presented here can be used in software model checking tools such as SLAM [1] and Blast [6], as well as in other automated bug-finding tools that rely on symbolic reasoning [14,15] Finally, the fifth article [13] presents an application of the SPIN model checker [7] to checking signature specifications. Signatures are matching rules that are used in intrusion detection systems when searching for attack traces in the recorded audit data based on pre-defined patterns. Intrusion detection systems are one of the most important means to protect information technology systems [12]. The effectiveness of an intrusion detection system depends on the adequacy of the signatures, which are usually defined empirically. Modeling a new signature is time-consuming and error-prone; consequently the modeled signature needs to be tested carefully. In this article, the authors present an approach to automatically checking signature specifications using the SPIN model checker. The signatures are modeled in the specification language EDL (a variant of Petri-nets) and then translated into PROMELA, the input language of the SPIN model checking tool. SPIN is used to find specification errors, which are modeled using linear temporal logic. In conclusion, the articles enclosed here describe new results in software model checking and analysis. The presented techniques are most useful at finding subtle and costly errors that can not be found with traditional testing alone. The techniques have been implemented in robust tools and therefore show good promise for adoption in industry
Learning techniques for software verification and validation–special track at ISoLA 2010
Learning techniques are used increasingly to improve software verification and validation activities. For example, automata learning techniques have been used for extracting behavioral models of software systems. Such models can serve as formal documentation of the software and they can be further verified using automated tools or used for model-based test case generation. Automata learning techniques have also been used for automating compositional verification and for building abstractions of software behavior in the context of symbolic or parameterized model checking. Furthermore, various machine-learning techniques have been used in fine-tuning heuristics used in constraint solving, in coming up with new abstraction techniques in the context of bounded model checking or shape analysis, in inferring invariants of parameterized systems, or in classifying data in black box testing.
Special Issue on Selected papers of the 5th International Workshop on Formal Aspects of Component Software (FACS'08) Preface
This issue contains extended versions of selected papers from the 5th International Workshop on Formal Aspects of Component Software (FACS’08). From the thirteen research works that were presented at FACS’08, an initial selection of papers was made by the Program Committee, and their authors were invited to submit an extended version to this special issue. These extended papers went through an anonymous peer review process, and the revised versions of the five papers finally accepted are included in this special issue. We believe that the papers presented here provide key insights on different formal aspects of component software. Component-based software has emerged as a promising paradigm to deal with the increasing need for mastering systems’ complexity, for enabling evolution and reuse, and for driving software engineering into sound production and engineering standards. Many issues in component-based software development remain open and challenging research questions. On the other hand, formal methods are mathematically-based techniques for the specification, development and verification of software and hardware systems. Therefore, they can be of great use in setting up formal foundations of component software and working out challenging issues such as mathematical models for components, their composition and adaptation, or rigorous approaches to verification, deployment, testing and certification. The objective of the FACS workshops is to bring together researchers in the areas of component-based software and formal methods to promote a deep understanding of component-based software and its applications. FACS’08 was held in Málaga, Spain, on September 10–12, 2008. The first two articles in this issue are related to program slicing. Sabouri and Sirjani propose new static slicing techniques for reducing Rebeca models with respect to a property. Similar techniques can be applied on the other actor-based languages. On the other hand, Rodrigues and Barbosa use slicing techniques to extract coordination data from source code; such information can help with program understanding or refactoring. In the third article, Strollo et al. introduce a formal methodology to handle coordination among services from the perspective of a global observer, in the spirit of choreography models. They describe the verification of compliance and consistency between the design of service interactions and the choreography constraints. Finally, Lei et al. address the automated testing of software components in a stressful or unexpected environment, while Héam et al. tackle the problem of formally accepting or rejecting a component in a composition, i.e., component substitutivity, with respect to quality of service. Many people have contributed to making this special issue possible. Besides the authors of the papers, we would like to thank the members of the Program Committee of the workshop: Farhad Arbab (CWI, The Netherlands), Luis Barbosa (University of Minho, Portugal), Frank S. de Boer (CWI, The Netherlands), Tevfik Bultan (University of California at Santa Barbara, USA), Paolo Ciancarini (Università di Bologna, Italy), Dimitra Giannakopoulou (NASA Ames Research Center, USA), Rolf Hennicker (Ludwig-Maximilians-Universität München, Germany), Atsushi Igarashi (University of Kyoto, Japan), Sarfraz Khurshid (University of Texas at Austin, USA), Zhiming Liu (UNU-IIST, Macau, China), Markus Lumpe (Swinburne University of Technology, Australia), Eric Madelaine (INRIA, Centre Sophia Antipolis, France), Jeff Magee (Imperial College, United Kingdom), Rupak Majumdar (University of California at Berkeley, USA), Vladimir Mencl (Charles University, Czech Republic, and University of Canterbury, New Zealand), Marius Minea (Politehnica University of Timisoara, Romania), Frantisek Plasil (Charles University, Czech Republic), Pascal Poizat (ARLES Project-team, INRIA and Université d’Évry, France), Ralf Reussner (Universität Karlsruhe, Germany), Bernhard Schaetz (Technical University of Munich, Germany), Clemens A. Szyperski (Microsoft, USA), Carolyn Talcott (SRI International, USA), Emilio Tuosto (University of Leicester, United Kingdom), and Kurt Wallnau (Carnegie Mellon University, USA). Our thanks go also to the other anonymous referees who kindly agreed to help us with the selection and reviewing of the papers in this special issue.
Symbolic PathFinder: symbolic execution of Java bytecode
Symbolic Pathfinder (SPF) combines symbolic execution with model checking and constraint solving for automated test case generation and error detection in Java programs with unspecified inputs. In this tool, programs are executed on symbolic inputs representing multiple concrete inputs. Values of variables are represented as constraints generated from the analysis of Java bytecode. The constraints are solved using off-the shelf solvers to generate test inputs guaranteed to achieve complex coverage criteria. SPF has been used successfully at NASA, in academia, and in industry.
Preface: Special Issue on Nasa Formal Methods Symposium 2009
This journal issue contains seven articles representing extended versions of the best papers selected from the proceedings of the First NASA Formal Methods Symposium (NFM 2009). The NASA Formal Methods Symposium is a new annual event that focuses on formal techniques, their theory, current capabilities, and limitations, as well as their application to aerospace, robotics, and other safety-critical systems.
Learning component interfaces with may and must abstractions
Component interfaces are the essence of modular program analysis. In this work, a component interface documents correct sequences of invocations to the component’s public methods. We present an automated framework that extracts finite safe, permissive, and minimal interfaces, from potentially infinite software components. Our proposed framework uses the L* automata-learning algorithm to learn finite interfaces for an infinite-state component. It is based on the observation that an interface permissive with respect to the component’s must abstraction and safe with respect to its may abstraction provides a precise characterization of the legal invocations to the methods of the concrete component. The abstractions are refined automatically from counterexamples obtained during the reachability checks performed by our framework. The use of must abstractions enables us to avoid an exponentially expensive determinization step that is required when working with may abstractions only, and the use of L* guarantees minimality of the generated interface. We have implemented the algorithm in the ARMC tool and report on its application to a number of case studies including several Java2SDK and J2SEE library classes as well as to NASA flight-software components.
Parallel symbolic execution for structural test generation
Symbolic execution is a popular technique for automatically generating test cases achieving high structural coverage. Symbolic execution suffers from scalability issues since the number of symbolic paths that need to be explored is very large (or even infinite) for most realistic programs. To address this problem, we propose a technique, Simple Static Partitioning, for parallelizing symbolic execution. The technique uses a set of pre-conditions to partition the symbolic execution tree, allowing us to effectively distribute symbolic execution and decrease the time needed to explore the symbolic execution tree. The proposed technique requires little communication between parallel instances and is designed to work with a variety of architectures, ranging from fast multi-core machines to cloud or grid computing environments. We implement our technique in the Java PathFinder verification tool-set and evaluate it on six case studies with respect to the performance improvement when exploring a finite symbolic execution tree and performing automatic test generation. We demonstrate speedup in both the analysis time over finite symbolic execution trees and in the time required to generate tests relative to sequential execution, with a maximum analysis time speedup of 90x observed using 128 workers and a maximum test generation speedup of 70x observed using 64 workers.
automated compositional verification
Compositional verification presents a divide-and-conquer, thus more scalable approach to verification: it decomposes the verification task for a system into simpler verification problems for individual components of the system. In checking components in isolation, one typically needs to include information about the contexts/environments in which components are expected to operate. Although compositional techniques have been advocated for several decades, there has only recently been a trend towards increasing automation of their application, thus making them more usable in practice.
Selected Papers of the 5th International Workshop on Formal Aspects of Component Software (FACS'08)
This issue contains extended versions of selected papers from the 5th International Workshop on Formal Aspects of Component Software (FACS’08). From the thirteen research works that were presented at FACS’08, an initial selection of papers was made by the Program Committee, and their authors were invited to submit an extended version to this special issue. These extended papers went through an anonymous peer review process, and the revised versions of the five papers finally accepted are included in this special issue. We believe that the papers presented here provide key insights on different formal aspects of component software. Component-based software has emerged as a promising paradigm to deal with the increasing need for mastering systems’ complexity, for enabling evolution and reuse, and for driving software engineering into sound production and engineering standards. Many issues in component-based software development remain open and challenging research questions. On the other hand, formal methods are mathematically-based techniques for the specification, development and verification of software and hardware systems. Therefore, they can be of great use in setting up formal foundations of component software and working out challenging issues such as mathematical models for components, their composition and adaptation, or rigorous approaches to verification, deployment, testing and certification. The objective of the FACS workshops is to bring together researchers in the areas of component-based software and formal methods to promote a deep understanding of component-based software and its applications. FACS’08 was held in Málaga, Spain, on September 10–12, 2008. The first two articles in this issue are related to program slicing. Sabouri and Sirjani propose new static slicing techniques for reducing Rebeca models with respect to a property. Similar techniques can be applied on the other actor-based languages. On the other hand, Rodrigues and Barbosa use slicing techniques to extract coordination data from source code; such information can help with program understanding or refactoring. In the third article, Strollo et al. introduce a formal methodology to handle coordination among services from the perspective of a global observer, in the spirit of choreography models. They describe the verification of compliance and consistency between the design of service interactions and the choreography constraints. Finally, Lei et al. address the automated testing of software components in a stressful or unexpected environment, while Héam et al. tackle the problem of formally accepting or rejecting a component in a composition, i.e., component substitutivity, with respect to quality of service. Many people have contributed to making this special issue possible. Besides the authors of the papers, we would like to thank the members of the Program Committee of the workshop: Farhad Arbab (CWI, The Netherlands), Luis Barbosa (University of Minho, Portugal), Frank S. de Boer (CWI, The Netherlands), Tevfik Bultan (University of California at Santa Barbara, USA), Paolo Ciancarini (Università di Bologna, Italy), Dimitra Giannakopoulou (NASA Ames Research Center, USA), Rolf Hennicker (Ludwig-Maximilians-Universität München, Germany), Atsushi Igarashi (University of Kyoto, Japan), Sarfraz Khurshid (University of Texas at Austin, USA), Zhiming Liu (UNU-IIST, Macau, China), Markus Lumpe (Swinburne University of Technology, Australia), Eric Madelaine (INRIA, Centre Sophia Antipolis, France), Jeff Magee (Imperial College, United Kingdom), Rupak Majumdar (University of California at Berkeley, USA), Vladimir Mencl (Charles University, Czech Republic, and University of Canterbury, New Zealand), Marius Minea (Politehnica University of Timisoara, Romania), Frantisek Plasil (Charles University, Czech Republic), Pascal Poizat (ARLES Project-team, INRIA and Université d’Évry, France), Ralf Reussner (Universität Karlsruhe, Germany), Bernhard Schaetz (Technical University of Munich, Germany), Clemens A. Szyperski (Microsoft, USA), Carolyn Talcott (SRI International, USA), Emilio Tuosto (University of Leicester, United Kingdom), and Kurt Wallnau (Carnegie Mellon University, USA). Our thanks go also to the other anonymous referees who kindly agreed to help us with the selection and reviewing of the papers in this special issue
CORAL: Solving Complex Constraints for Symbolic PathFinder
Symbolic execution is a powerful automated technique for generating test cases. Its goal is to achieve high coverage of software. One major obstacle in adopting the technique in practice is its inability to handle complex mathematical constraints. To address the problem, we have integrated CORAL’s heuristic solvers into NASA Ames’ Symbolic PathFinder symbolic execution tool. CORAL’s solvers have been designed to deal with mathematical constraints and their heuristics have been improved based on examples from the aerospace domain. This integration significantly broadens the application of Symbolic PathFinder at NASA and in industry.
A survey of new trends in symbolic execution for software testing and analysis
Symbolic execution is a well-known program analysis technique which represents program inputs with symbolic values instead of concrete, initialized, data and executes the program by manipulating program expressions involving the symbolic values. Symbolic execution has been proposed over three decades ago but recently it has found renewed interest in the research community, due in part to the progress in decision procedures, availability of powerful computers and new algorithmic developments. We provide here a survey of some of the new research trends in symbolic execution, with particular emphasis on applications to test generation and program analysis. We first describe an approach that handles complex programming constructs such as input recursive data structures, arrays, as well as multithreading. Furthermore, we describe recent hybrid techniques that combine concrete and symbolic execution to overcome some of the inherent limitations of symbolic execution, such as handling native code or availability of decision procedures for the application domain. We follow with a discussion of techniques that can be used to limit the (possibly infinite) number of symbolic configurations that need to be analyzed for the symbolic execution of looping programs. Finally, we give a short survey of interesting new applications, such as predictive testing, invariant inference, program repair, analysis of parallel numerical programs and differential symbolic execution.
Model based analysis and test generation for flight software
We describe a framework for model-based analysis and test case generation in the context of a heterogeneous model-based development paradigm that uses and combines MathWorks and UML 2.0 models and the associated code generation tools. This paradigm poses novel challenges to analysis and test case generation that, to the best of our knowledge, have not been addressed before. The framework is based on a common intermediate representation for different modeling formalisms and leverages and extends model checking and symbolic execution tools for model analysis and test case generation, respectively. We discuss the application of our framework to software models for a NASA flight mission.
A Test Generation Framework for Distributed Fault-Tolerant Algorithms
Heavyweight formal methods such as theorem proving have been successfully applied to the analysis of safety critical fault-tolerant systems. Typically, the models and proofs performed during such analysis do not inform the testing process of actual implementations. We propose a framework for generating test vectors from specifications written in the Prototype Verification System (PVS). The methodology uses a translator to produce a Java prototype from a PVS specification. Symbolic (Java) PathFinder is then employed to generate a collection of test cases. A small example is employed to illustrate how the framework can be used in practice.
Model checking software
This book constitutes the refereed proceedings of the 16th International SPIN workshop on Model Checking Software, SPIN 2009, held in Grenoble, France, in June 2009. The 15 revised full papers presented together with 3 tool papers and 4 invited talks were carefully reviewed and selected from 41 submissions. The papers cover theoretical and algorithmic foundations as well as tools for software model checking by addressing theoretical advances and empirical evaluations related to state-space and path exploration techniques, as implemented in software verification tools.
Model Checking Software: 16th International SPIN Workshop, Grenoble, France, June 26-28, 2009, Proceedings
This book constitutes the refereed proceedings of the 16th International SPIN workshop on Model Checking Software, SPIN 2009, held in Grenoble, France, in June 2009. The 15 revised full papers presented together with 3 tool papers and 4 invited talks were carefully reviewed and selected from 41 submissions. The papers cover theoretical and algorithmic foundations as well as tools for software model checking by addressing theoretical advances and empirical evaluations related to state-space and path exploration techniques, as implemented in software verification tools.
Methods Symposium
This NASA conference publication contains the proceedings of the First NASA Formal Methods Symposium (NFM 2009), held at the NASA Ames Research Center, in Moffett Field, CA, USA, on April 6 – 8, 2009. NFM 2009 is a forum for theoreticians and practitioners from academia and industry, with the goals of identifying challenges and providing solutions to achieving assurance in safety-critical systems. Within NASA, for example, such systems include autonomous robots, separation assurance algorithms for aircraft, and autonomous rendezvous and docking for spacecraft. Moreover, emerging paradigms such as code generation and safety cases are bringing with them new challenges and opportunities. The focus of the symposium is on formal techniques, their theory, current capabilities, and limitations, as well as their application to aerospace, robotics, and other safety-critical systems. The NASA Formal Methods Symposium is a new annual event intended to highlight the state of formal methods’ art and practice. It follows the earlier Langley Formal Methods Workshop series and aims to foster collaboration between NASA researchers and engineers, as well as the wider aerospace, safety-critical and formal methods communities. The specific topics covered by NFM 2009 included but were not limited to: formal verification, including theorem proving, model checking, and static analysis; automated testing and simulation techniques; model-based development; techniques and algorithms for scaling formal methods, such as abstraction and symbolic methods, compositional techniques, as well as parallel and/or distributed techniques; code generation; safety cases; accident/safety analysis; formal approaches to fault tolerance; theoretical advances and empirical evaluations of formal methods techniques for safety-critical systems, including hybrid and embedded systems. We considered two types of papers: regular papers describe fully developed work and complete results, and short papers describe interesting work in progress and/or preliminary results. Both categories must describe original work that has not been published elsewhere. We received 47 submissions (26 long papers and 19 short papers) out of which 22 were accepted (14 long, 8 short). All submissions went through a rigorous reviewing process, where each paper received a minimum of 3 reviews. The program selection was performed through an electronic Program Committee meeting. In addition to the refereed papers, the symposium featured five invited talks given by Ed Clarke (CMU, Turing Award 2007) on Model Checking – My 27-year Quest to Overcome the State Explosion Problem, Bill Othon (NASA JSC) on Applying Formal Methods to NASA Projects: Transition from Research to Practice, Leslie Lamport (MSR) on TLA+: Whence, Wherefore, and Whither, Todd Farley (NASA Ames) on Formal Methods Applications in Air Transportation, and John O’Leary (Intel) on TITLE. The program also included a panel discussion on Formal Methods meet NASA needs that was chaired by Mats Heimdahl (U. Minnesota). We would like to thank the program committee members and the external reviewers for their contribution in paper selection, and the NFM Organizing Committee for its support in setting up this event. Thanks also go to Allen Dutra, Domenico Bianculli, and Chris Fattarsi for their help with the NFM 2009 local organization and to Geoff Sutcliffe for help with the EasyChair style files. Finally, we thank SGT, in particular Larry Markosian, and CMU West, in particular Jim Morris, for their support. The NFM 2009 website can be found at http://ti.arc.nasa.gov/event/nfm09/
Proceedings of the First NASA Formal Methods Symposium
Topics covered include: Model Checking - My 27-Year Quest to Overcome the State Explosion Problem; Applying Formal Methods to NASA Projects: Transition from Research to Practice; TLA+: Whence, Wherefore, and Whither; Formal Methods Applications in Air Transportation; Theorem Proving in Intel Hardware Design; Building a Formal Model of a Human-Interactive System: Insights into the Integration of Formal Methods and Human Factors Engineering; Model Checking for Autonomic Systems Specified with ASSL; A Game-Theoretic Approach to Branching Time Abstract-Check-Refine Process; Software Model Checking Without Source Code; Generalized Abstract Symbolic Summaries; A Comparative Study of Randomized Constraint Solvers for Random-Symbolic Testing; Component-Oriented Behavior Extraction for Autonomic System Design; Automated Verification of Design Patterns with LePUS3; A Module Language for Typing by Contracts; From Goal-Oriented Requirements to Event-B Specifications; Introduction of Virtualization Technology to Multi-Process Model Checking; Comparing Techniques for Certified Static Analysis; Towards a Framework for Generating Tests to Satisfy Complex Code Coverage in Java Pathfinder; jFuzz: A Concolic Whitebox Fuzzer for Java; Machine-Checkable Timed CSP; Stochastic Formal Correctness of Numerical Algorithms; Deductive Verification of Cryptographic Software; Coloured Petri Net Refinement Specification and Correctness Proof with Coq; Modeling Guidelines for Code Generation in the Railway Signaling Context; Tactical Synthesis Of Efficient Global Search Algorithms; Towards Co-Engineering Communicating Autonomous Cyber-Physical Systems; and Formal Methods for Automated Diagnosis of Autosub 6000.
Interface generation and compositional verification in JavaPathfinder
We present a novel algorithm for interface generation of software components. Given a component, our algorithm uses learning techniques to compute a permissive interface representing legal usage of the component. Unlike our previous work, this algorithm does not require knowledge about the component’s environment. Furthermore, in contrast to other related approaches, our algorithm computes permissive interfaces even in the presence of non-determinism in the component. Our algorithm is implemented in the JavaPathfinder model checking framework for UML statechart components. We have also added support for automated assume-guarantee style compositional verification in JavaPathfinder, using component interfaces. We report on the application of the approach to interface generation for flight-software components.
Verification and validation of air traffic systems: Tactical separation assurance
The expected future increase in air traffic requires the development of innovative algorithms and software systems to automate safety critical functions such as separation assurance - the task of maintaining a safe distance between aircraft at all times. Extensive verification and validation (V&V) of such functions will be crucial for the acceptance of new air traffic management systems. This paper reports on work performed at the NASA Ames Research Center. We discuss how advanced V&V technologies can be used to create robust software prototypes for air traffic control software, and how conformance of production code with such prototypes can be assured. We present preliminary results of V&V efforts for a prototype of the Tactical Separation Assisted Flight Environment system (TSAFE).
Software V&V support by parametric analysis of large software simulation systems
Modern aerospace software systems simulations usually contain many (dependent and independent) parameters. Due to the large parameter space, and the complex, highly coupled nonlinear nature of the different system components, analysis is complicated and time consuming. Thus, such systems are generally validated only in regions local to anticipated operating points rather than through characterization of the entire feasible operational envelope of the system. We have addressed the factors deterring such a comprehensive analysis with a tool to support parametric analysis and envelope assessment: a combination of advanced Monte Carlo generation with n-factor combinatorial parameter variations and model-based testcase generation is used to limit the number of cases without sacrificing important interactions in the parameter space. For the automatic analysis of the generated data we use unsupervised Bayesian clustering techniques (AutoBayes) and supervised learning of critical parameter ranges using the treatment learner TAR3. This unique combination of advanced machine learning technology enables a fast and powerful multivariate analysis that supports finding of root causes.
Symbolic execution with abstraction
We address the problem of error detection for programs that take recursive data structures and arrays as input. Previously we proposed a combination of symbolic execution and model checking for the analysis of such programs: we put a bound on the size of the program inputs and/or the search depth of the model checker to limit the search state space. Here we look beyond bounded model checking and consider state matching techniques to limit the state space. We describe a method for examining whether a symbolic state that arises during symbolic execution is subsumed by another symbolic state. Since the number of symbolic states may be infinite, subsumption is not enough to ensure termination. Therefore, we also consider abstraction techniques for computing and storing abstract states during symbolic execution. Subsumption checking determines whether an abstract state is being revisited, in which case the model checker backtracks—this enables analysis of an under-approximation of the program behaviors. We illustrate the technique with abstractions for lists and arrays. We also discuss abstractions for more general data structures. The abstractions encode both the shape of the program heap and the constraints on numeric data. We have implemented the techniques in the Java PathFinder tool and we show their effectiveness on Java programs. This paper is an extended version of Anand et al. (Proceedings of SPIN, pp. 163–181, 2006).
Partial-order reduction is one of the main techniques used to tackle the combinatorial state explosion problem occurring in explicit-state model checking of concurren...
Partial-Order Reduction is one of the main techniques used to tackle the combinatorial state explosion problem occurring in explicit-state model checking of concurrent systems. The reduction is performed by exploiting the independence of concurrently executed events which allows portions of the state space to be pruned. An important condition for the soundness of partial-order based reduction algorithms is a condition that prevents indefinite ignoring of actions when pruning the state space. This condition is commonly known as the cycle proviso. In this paper we present a new version of this proviso which is applicable to a general search algorithm skeleton that we refer to as the General State Expanding Algorithm (GSEA). GSEA maintains a set of open states from which states are iteratively selected for expansion and moved to a closed set of states. Depending on the data structure used to represen the open set, GSEA can be instantiated as a depth-first, a breadthfirst, or a directed search algorithm such as Best-First Search or A*. The proviso is characterized by reference to the open and closed set of states of the search algorithm. As a result it can be computed in an efficient manner during the search based on local information. We implemented partial-order reduction for GSEA based on our proposed proviso in the tool HSF-SPIN, which is an extension of the explicit-state model checker SPIN for directed model checking. We evaluate the state space reduction achieved by partial-order reduction using the proposed proviso by comparing it on a set of benchmark problems to the use of other provisos. We also compare the use of breadth-first search (BFS) and A*, two algorithms ensuring that counterexamples of minimal length will be found, together with the proviso that we propose.
Assume-guarantee testing for software components
Integration issues of component-based systems tend to be targeted at the later phases of the software development, mostly after components have been assembled to form an executable system. However, errors discovered at these phases are typically hard to localise and expensive to fix. To address this problem, the authors introduce assume-guarantee testing, a technique that establishes key properties of a component-based system before component assembly, when the cost of fixing errors is smaller. Assume-guarantee testing is based on the (automated) decomposition of system-level requirements into local component requirements at design time. The local requirements are in the form of assumptions and guarantees that each component makes on, or provides to the system, respectively. Checking requirements is performed during testing of individual components (i.e. unit testing) and it may uncover system-level violations prior to system testing. Furthermore, assume-guarantee testing may detect such violations with a higher probability than traditional testing. The authors also discuss an alternative technique, namely predictive testing, that uses the local component assumptions and guarantees to test assembled systems: given a non-violating system run, this technique can predict violations by alternative system runs without constructing those runs.The authors demonstrate the proposed approach and its benefits by means of two NASA case studies: a safety-critical protocol for autonomous rendez-vous and docking and the executive subsystem of the planetary rover controller K9.
Differential symbolic execution
Detecting and characterizing the effects of software changes is a fundamental component of software maintenance. Version differencing information can be used to perform version merging, infer change characteristics, produce program documentation, and guide program re-validation. Existing techniques for characterizing code changes, however, are imprecise leading to unnecessary maintenance efforts. In this paper, we introduce a novel extension and application of symbolic execution techniques that computes a precise behavioral characterization of a program change. This technique, which we call differential symbolic execution (DSE), exploits the fact that program versions are largely similar to reduce cost and improve the quality of analysis results. We define the foundational concepts of DSE, describe cost-effective tool support for DSE, and illustrate its potential benefit through an exploratory study that considers version histories of two Java code bases.
Tool Support for Parametric Analysis of Large Software Simulation Systems
The analysis of large and complex parameterized software systems, e.g., systems simulation in aerospace, is very complicated and time-consuming due to the large parameter space, and the complex, highly coupled nonlinear nature of the different system components. Thus, such systems are generally validated only in regions local to anticipated operating points rather than through characterization of the entire feasible operational envelope of the system. We have addressed the factors deterring such an analysis with a tool to support envelope assessment: we utilize a combination of advanced Monte Carlo generation with n-factor combinatorial parameter variations to limit the number of cases, but still explore important interactions in the parameter space in a systematic fashion. Additional test-cases, automatically generated from models (e.g., UML, Simulink, Stateflow) improve the coverage. The distributed test runs of the software system produce vast amounts of data, making manual analysis impossible. Our tool automatically analyzes the generated data through a combination of unsupervised Bayesian clustering techniques (AutoBayes) and supervised learning of critical parameter ranges using the treatment learner TAR3. The tool has been developed around the Trick simulation environment, which is widely used within NASA. We will present this tool with a GN&C (Guidance, Navigation and Control) simulation of a small satellite system.
Combining unit-level symbolic execution and system-level concrete execution for testing NASA software
We describe an approach to testing complex safety critical software that combines unit-level symbolic execution and system-level concrete execution for generating test cases that satisfy user-specified testing criteria. We have developed Symbolic Java PathFinder, a symbolic execution framework that implements a non-standard bytecode interpreter on top of the Java PathFinder model checking tool. The framework propagates the symbolic information via attributes associated with the program data. Furthermore, we use two techniques that leverage system-level concrete program executions to gather information about a unit's input to improve the precision of the unit-level test case generation. We applied our approach to testing a prototype NASA flight software component. Our analysis helped discover a serious bug that resulted in design changes to the software. Although we give our presentation in the context of a NASA project, we believe that our work is relevant for other critical systems that require thorough testing.
Automated Assume-Guarantee Reasoning by Abstraction Refinement
	Current automated approaches for compositional model checking in the assume-guarantee style are based on learning of assumptions as deterministic automata. We propose an alternative approach based on abstraction refinement. Our new method computes the assumptions for the assume-guarantee rules as conservative and not necessarily deterministic abstractions of some of the components, and refines those abstractions using counter-examples obtained from model checking them together with the other components. Our approach also exploits the alphabets of the interfaces between components and performs iterative refinement of those alphabets as well as of the abstractions. We show experimentally that our preliminary implementation of the proposed alternative achieves similar or better performance than a previous learning-based implementation.
Automated assume-guarantee reasoning by abstraction refinement
Current automated approaches for compositional model checking in the assume-guarantee style are based on learning of assumptions as deterministic automata. We propose an alternative approach based on abstraction refinement. Our new method computes the assumptions for the assume-guarantee rules as conservative and not necessarily deterministic abstractions of some of the components, and refines those abstractions using counterexamples obtained from model checking them together with the other components. Our approach also exploits the alphabets of the interfaces between components and performs iterative refinement of those alphabets as well as of the abstractions. We show experimentally that our preliminary implementation of the proposed alternative achieves similar or better performance than a previous learning-based implementation.
Learning to divide and conquer: applying the L* algorithm to automate assume-guarantee reasoning
Assume-guarantee reasoning enables a “divide-and-conquer” approach to the verification of large systems that checks system components separately while using assumptions about each component’s environment. Developing appropriate assumptions used to be a difficult and manual process. Over the past five years, we have developed a framework for performing assume-guarantee verification of systems in an incremental and fully automated fashion. The framework uses an off-the-shelf learning algorithm to compute the assumptions. The assumptions are initially approximate and become more precise by means of counterexamples obtained by model checking components separately. The framework supports different assume-guarantee rules, both symmetric and asymmetric. Moreover, we have recently introduced alphabet refinement, which extends the assumption learning process to also infer assumption alphabets. This refinement technique starts with assumption alphabets that are a subset of the minimal interface between a component and its environment, and adds actions to it as necessary until a given property is shown to hold or to be violated in the system. We have applied the learning framework to a number of case studies that show that compositional verification by learning assumptions can be significantly more scalable than non-compositional verification.
Special issue on learning techniques for compositional reasoning
Compositional reasoning aims to improve scalability of verification tools by reducing the original verification task into subproblems. The simplification is typically based on assume-guarantee reasoning principles, and requires user guidance to identify appropriate assumptions for components. In this paper, we propose a fully automated approach to compositional reasoning that consists of automated decomposition using a hypergraph partitioning algorithm for balanced clustering of variables, and discovering assumptions using the L * algorithm for active learning of regular languages. We present a symbolic implementation of the learning algorithm, and incorporate it in the model checker NuSmv. In some cases, our experiments demonstrate significant savings in the computational requirements of symbolic model checking.
Learning to divide and conquer: applying the L* algorithm to automate assume-guarantee reasoning
Assume-guarantee reasoning enables a “divide-and-conquer” approach to the verification of large systems that checks system components separately while using assumptions about each component’s environment. Developing appropriate assumptions used to be a difficult and manual process. Over the past five years, we have developed a framework for performing assume-guarantee verification of systems in an incremental and fully automated fashion. The framework uses an off-the-shelf learning algorithm to compute the assumptions. The assumptions are initially approximate and become more precise by means of counterexamples obtained by model checking components separately. The framework supports different assume-guarantee rules, both symmetric and asymmetric. Moreover, we have recently introduced alphabet refinement, which extends the assumption learning process to also infer assumption alphabets. This refinement technique starts with assumption alphabets that are a subset of the minimal interface between a component and its environment, and adds actions to it as necessary until a given property is shown to hold or to be violated in the system. We have applied the learning framework to a number of case studies that show that compositional verification by learning assumptions can be significantly more scalable than non-compositional verification.
Assume-guarantee verification for interface automata
Interface automata provide a formalism capturing the high level interactions between software components. Checking compatibility, and other safety properties, in an automata-based system suffers from the scalability issues inherent in exhaustive techniques such as model checking. This work develops a theoretical framework and automated algorithms for modular verification of interface automata. We propose sound and complete assume-guarantee rules for interface automata, and learning-based algorithms to automate assumption generation. Our algorithms have been implemented in a practical model-checking tool and have been applied to a realistic NASA case study.
Automatic Testcase Generation for Flight Software
The TacSat3 project is applying Integrated Systems Health Management (ISHM) technologies to an Air Force spacecraft for operational evaluation in space. The experiment will demonstrate the effectiveness and cost of ISHM and vehicle systems management (VSM) technologies through onboard operation for extended periods. We present two approaches to automatic testcase generation for ISHM: 1) A blackbox approach that views the system as a blackbox, and uses a grammar-based specification of the system's inputs to automatically generate *all* inputs that satisfy the specifications (up to prespecified limits); these inputs are then used to exercise the system. 2) A whitebox approach that performs analysis and testcase generation directly on a representation of the internal behaviour of the system under test. The enabling technologies for both these approaches are model checking and symbolic execution, as implemented in the Ames' Java PathFinder (JPF) tool suite. Model checking is an automated technique for software verification. Unlike simulation and testing which check only some of the system executions and therefore may miss errors, model checking exhaustively explores all possible executions. Symbolic execution evaluates programs with symbolic rather than concrete values and represents variable values as symbolic expressions. We are applying the blackbox approach to generating input scripts for the Spacecraft Command Language (SCL) from Interface and Control Systems. SCL is an embedded interpreter for controlling spacecraft systems. TacSat3 will be using SCL as the controller for its ISHM systems. We translated the SCL grammar into a program that outputs scripts conforming to the grammars. Running JPF on this program generates all legal input scripts up to a prespecified size. Script generation can also be targeted to specific parts of the grammar of interest to the developers. These scripts are then fed to the SCL Executive. ICS's in-house coverage tools will be run to measure code coverage. Because the scripts exercise all parts of the grammar, we expect them to provide high code coverage. This blackbox approach is suitable for systems for which we do not have access to the source code. We are applying whitebox test generation to the Spacecraft Health INference Engine (SHINE) that is part of the ISHM system. In TacSat3, SHINE will execute an on-board knowledge base for fault detection and diagnosis. SHINE converts its knowledge base into optimized C code which runs onboard TacSat3. SHINE can translate its rules into an intermediate representation (Java) suitable for analysis with JPF. JPF will analyze SHINE's Java output using symbolic execution, producing testcases that can provide either complete or directed coverage of the code. Automatically generated test suites can provide full code coverage and be quickly regenerated when code changes. Because our tools analyze executable code, they fully cover the delivered code, not just models of the code. This approach also provides a way to generate tests that exercise specific sections of code under specific preconditions. This capability gives us more focused testing of specific sections of code.
Program model checking: A practitioner's guide
Program model checking is a verification technology that uses state-space exploration to evaluate large numbers of potential program executions. Program model checking provides improved coverage over testing by systematically evaluating all possible test inputs and all possible interleavings of threads in a multithreaded system. Model-checking algorithms use several classes of optimizations to reduce the time and memory requirements for analysis, as well as heuristics for meaningful analysis of partial areas of the state space Our goal in this guidebook is to assemble, distill, and demonstrate emerging best practices for applying program model checking. We offer it as a starting point and introduction for those who want to apply model checking to software verification and validation. The guidebook will not discuss any specific tool in great detail, but we provide references for specific tools.
Verification of plans and procedures
Procedures and plans are used across NASA missions. For example, astronaut activities on the International Space Station are regulated by procedures which are uploaded from the ground. It is critical that these procedures are verified and validated before being executed by astronauts. This paper describes how we are applying advanced formal verification techniques, such as model checking, to plans and procedures expressed in semantically well-defined languages such as PRL and PLEXIL.
Program Model Checking--A Practitioner’s Guide
Program model checking is a verification technology that uses state-space exploration to evaluate large numbers of potential program executions. Program model checking provides improved coverage over testing by systematically evaluating all possible test inputs and all possible interleavings of threads in a multithreaded system. Model-checking algorithms use several classes of optimizations to reduce the time and memory requirements for analysis, as well as heuristics for meaningful analysis of partial areas of the state space Our goal in this guidebook is to assemble, distill, and demonstrate emerging best practices for applying program model checking. We offer it as a starting point and introduction for those who want to apply model checking to software verification and validation. The guidebook will not discuss any specific tool in great detail, but we provide references for specific tools.
A small-step semantics of PLEXIL
A key design principle of a high-level plan execution language is the ability to predict the output of a planning task under limited information on the external environment and on the platform where the plan is executed. This type of determinism is particularly critical in unmanned space exploration systems, such as robotic rovers, which are often deployed in a semi-autonomous mode. This paper proposes a formal framework for specifying the semantics of general plan execution languages based on NASA’ Plan Execution Interchange Language (PLEXIL), a rich concurrent language that is being developed in support of NASA’s Space Exploration Program. The semantic framework allows for the formal study of properties such as determinism for different assumptions on the knowledge of the external environment. The framework is organized as a stack of parametric layers defining the execution mechanism of a parallel synchronous eventdriven language. This modular presentation enables the instantiation of the framework to different semantic variants of PLEXIL-like languages. The mathematical development presented in this paper has been formalized and mechanically checked in the Program Verification System (PVS).