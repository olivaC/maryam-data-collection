Dagstuhl Reports, Vol. 7, Issue 3 ISSN 2192-5283
Abstract not available.
The work life of developers: Activities, switches and perceived productivity
Many software development organizations strive to enhance the productivity of their developers. All too often, efforts aimed at improving developer productivity are undertaken without knowledge about how developers spend their time at work and how it influences their own perception of productivity. To fill in this gap, we deployed a monitoring application at 20 computers of professional software developers from four companies for an average of 11 full work day in situ. Corroborating earlier findings, we found that developers spend their time on a wide variety of activities and switch regularly between them, resulting in highly fragmented work. Our findings extend beyond existing research in that we correlate developers' work habits with perceived productivity and also show productivity is a personal matter. Although productivity is personal, developers can be roughly grouped into morning, low-at-lunch and afternoon people. A stepwise linear regression per participant revealed that more user input is most often associated with a positive, and emails, planned meetings and work unrelated websites with a negative perception of productivity. We discuss opportunities of our findings, the potential to predict high and low productivity and suggest design approaches to create better tool support for planning developers' work day and improving their personal productivity.
Characterizing Software Developers by Perceptions of Productivity
Understanding developer productivity is important to deliver software on time and at reasonable cost. Yet, there are numerous definitions of productivity and, as previous research found, productivity means different things to different developers. In this paper, we analyze the variation in productivity perceptions based on an online survey with 413 professional software devel-opers at Microsoft. Through a cluster analysis, we identify and describe six groups of developers with similar perceptions of productivity: social, lone, focused, balanced, leading, and goal-oriented developers. We argue why personalized recommendations for improving software developers' work is important and discuss design implications of these clusters for tools to support developers' productivity.
What Makes a Great Manager of Software Engineers?
Having great managers is as critical to success as having a good team or organization. In general, a great manager is seen as fuelling the team they manage, enabling it to use its full potential. Though software engineering research studies factors that may affect the performance and productivity of software engineers and teams (like tools and skill), it has overlooked the software engineering manager. The software industry's growth and change in the last decades is creating a need for a domain-specific view of management. On the one hand, experts are questioning how the abundant work in management applies to software engineering. On the other hand, practitioners are looking to researchers for evidence-based guidance on how to manage software teams. We conducted a mixed methods empirical study of software engineering management at Microsoft to investigate what manager attributes developers and engineering managers perceive important and why. We present a conceptual framework of manager attributes, and find that technical skills are not the sign of greatness for an engineering manager. Through statistical analysis we identify how engineers and managers relate in their views, and how software engineering differs from other knowledge work groups in its perceptions about what makes great managers. We present strategies for putting the attributes to use, discuss implications for research and practice, and offer avenues for further work.
Characterizing Software Engineering Work with Personas Based on Knowledge Worker Actions
Mistaking versatility for universal skills, some companies tend to categorize all software engineers the same not knowing a difference exists. For example, a company may select one of many software engineers to complete a task, later finding that the engineer’s skills and style do not match those needed to successfully complete that task. This can result in delayed task completion and demonstrates that a one-size fits all concept should not apply to how software engineers work. In order to gain a comprehensive understanding of different software engineers and their working styles we interviewed 21 participants and surveyed 868 software engineers at a large software company and asked them about their work in terms of knowledge worker actions. We identify how tasks, collaboration styles, and perspectives of autonomy can significantly effect different approaches to software engineering work. To characterize differences, we describe empirically informed personas on how they work. Our defined software engineering personas include those with focused debugging abilities, engineers with an active interest in learning, experienced advisors who serve as experts in their role, and more. Our study and results serve as a resource for building products, services, and tools around these software engineering personas.
Data Scientists in Software Teams: State of the Art and Challenges
The demand for analyzing large scale telemetry, machine, and quality data is rapidly increasing in software industry. Data scientists are becoming popular within software teams, e.g., Facebook, LinkedIn and Microsoft are creating a new career path for data scientists. In this paper, we present a large-scale survey with 793 professional data scientists at Microsoft to understand their educational background, problem topics that they work on, tool usages, and activities. We cluster these data scientists based on the time spent for various activities and identify 9 distinct clusters of data scientists, and their corresponding characteristics. We also discuss the challenges that they face and the best practices they share with other data scientists. Our study finds several trends about data scientists in the software engineering context at Microsoft, and should inform managers on how to leverage data science capability effectively within their teams.
How do Practitioners Perceive the Relevance of Requirements Engineering Research? An Ongoing Study
The relevance of Requirements Engineering (RE) research to practitioners is a prerequisite for problem-driven research in the area and key for a long-term dissemination of research results to everyday practice. To understand better how industry practitioners perceive the practical relevance of RE research, we have initiated the RE-Pract project, an international collaboration conducting an empirical study. This project opts for a replication of previous work done in two different domains and relies on survey research. To this end, we have designed a survey to be sent to several hundred industry practitioners at various companies around the world and ask them to rate their perceived practical relevance of the research described in a sample of 418 RE papers published between 2010 and 2015 at the RE, ICSE, FSE, ESEC/FSE, ESEM and REFSQ conferences. In this paper, we summarize our research protocol and present the current status of our study and the planned future steps.
Software productivity decoded: how data science helps to achieve more (keynote)
Many companies are looking into understanding and improving productivity of individual software developers as well as software teams. In this talk, I will motivate the need for data analytics in software teams and describe how data scientists work in a large software companies helping software teams to infer actionable insights. I will then show how data from software development can be used to learn more about the productivity of organizations, teams, and individuals and help them to become more effective in building software.
Master maker: Understanding gaming skill through practice and habit from gameplay behavior
The study of expertise is difficult to do in a laboratory environment due to the challenge of finding people at different skill levels and the lack of time for participants to acquire mastery. In this paper, we report on two studies that analyze naturalistic gameplay data using cohort analysis to better understand how skill relates to practice and habit. Two cohorts are analyzed, each from two different games (Halo Reach and StarCraft 2). Our work follows skill progression through 7 months of Halo matches for a holistic perspective, but also explores low‐level in‐game habits when controlling game units in StarCraft 2. Players who played moderately frequently without long breaks were able to gain skill the most efficiently. What set the highest performers apart was their ability to gain skill more rapidly and without dips compared to other players. At the beginning of matches, top players habitually warmed up by selecting and re‐selecting groups of units repeatedly in a meaningless cycle. They exhibited unique routines during their play that aided them when under pressure.
Ramp-up Journey of New Hires: Do strategic practices of software companies influence productivity?
Software companies regularly recruit skilled and talented employees to meet evolving business requirements. Although companies expect early contributions, new hires often take several weeks to reach the same productivity level as existing employees. We refer to this transition of new hires from novices to experts as ramp-up journey. There can be various factors such as lack of technical skills or lack of familiarity with the process that influence the ramp-up journey of new hires. The goal of our work is to identify those factors and study their influence on the ramp-up journey. We expect the results from this study to help identify the need of various types of assistance to new hires to ramp-up faster. As a first step towards our goal, this paper explores the impact of two strategic practices, namely distributed development and internship on the ramp-up journey of new hires. Our results show that new hires in proximity to the core development team and new hires with prior internship experience perform better than others in the beginning. In the overall ramp-up journey, the effect of the two factors attenuates, yet nevertheless better compared to their counterparts. Product teams can use this information to pay special attention to non-interns and use better tools for distributed, cooperative work to help new hires ramp-up faster.
Coq's Prolog and application to defining semi-automatic tactics
We report on a work-in-progress to re-implement Coq's apply tactic in order to embed some form of simple automation. We design it in a declarative way, relying on typeclasses eauto, a tactic which gives access to the proof-search mechanism behind type classes. We qualify this mechanism of " Coq's Prolog " and describe it in a generic way and explain how it can be used to support the construction of automatic and semi-automatic tactics.
Predicting software build errors
Systems and methods for predicting a software build error are described herein. In one example, a method includes detecting a plurality of changes in software. The method also includes identifying a plurality of change lists, wherein a change list is identified for each of the plurality of changes in the software. Additionally, the method includes identifying a characteristic for each change list in the plurality of change lists. Furthermore, the method includes calculating a plurality of probabilities based at least in part on the characteristic of each of the plurality of change lists, wherein each of the probabilities indicates the likelihood of one of the plurality of change lists creating the software build error. The method also includes reporting the plurality of probabilities of the software build error.
Retrospecting on Work and Productivity A Study on Self-Monitoring Software Developers’ Work
One way to improve the productivity of knowledge workers is to increase their self-awareness about productivity at work through self-monitoring. Yet, little is known about expectations of, the experience with, and the impact of self-monitoring in the workplace. To address this gap, we studied software developers, as one community of knowledge workers. We used an iterative, user-feedbackdriven development approach (N=20) and a survey (N=413) to infer design elements for workplace self-monitoring, which we then implemented as a technology probe called WorkAnalytics. We field-tested these design elements during a three-week study with software development professionals (N=43). Based on the results of the field study, we present design recommendations for self-monitoring in the workplace, such as using experience sampling to increase the awareness about work and to create richer insights, the need for a large variety of different metrics to retrospect about work, and that actionable insights, enriched with benchmarking data from co-workers, are likely needed to foster productive behavior change and improve collaboration at work. Our work can serve as a starting point for researchers and practitioners to build self-monitoring tools for the workplace.
Design Recommendations for Self-Monitoring in the Workplace: Studies in Software Development
One way to improve the productivity of knowledge workers is to increase their self-awareness about productivity at work through self-monitoring. Yet, little is known about expectations of, the experience with, and the impact of self-monitoring in the workplace. To address this gap, we studied software developers, as one community of knowledge workers. We used an iterative, user-feedback-driven development approach (N=20) and a survey (N=413) to infer design elements for workplace self-monitoring, which we then implemented as a technology probe called WorkAnalytics. We field-tested these design elements during a three-week study with software development professionals (N=43). Based on the results of the field study, we present design recommendations for self-monitoring in the workplace, such as using experience sampling to increase the awareness about work and to create richer insights, the need for a large variety of different metrics to retrospect about work, and that actionable insights, enriched with benchmarking data from co-workers, are likely needed to foster productive behavior change and improve collaboration at work. Our work can serve as a starting point for researchers and practitioners to build self-monitoring tools for the workplace.
Databases on Future Hardware (Dagstuhl Seminar 17101)
A number of physical limitations mandate radical changes in the way how we build computing hard- and software, and there is broad consensus that a stronger interaction between hard- and software communities is needed to meet the ever-growing demand for application performance. Under this motivation, representatives from various hard- and software communities have met at the Dagstuhl seminar "Databases on Future Hardware" to discuss the implications in the context of database systems. The outcome of the seminar was not only a much better understanding of each other's needs, constraints, and ways of thinking. Very importantly, the group identified topic areas that seem key for the ongoing shift, together with suggestions on how the field could move forward. During the seminar, it turned out that the future of databases is not only a question of technology. Rather, economic considerations have to be taken into account when building next-generation database engines.
Rethinking Productivity in Software Engineering (Dagstuhl Seminar 17102)
This report documents the program and the outcomes of Dagstuhl Seminar 17102 "Rethinking Productivity in Software Engineering". In the following, we briefly summarize the goals and format of the of the seminar, before we provide insights and an outlook, including a few grand challenges, based on the results and statements collected during the seminar.
An empirical investigation of single‐objective and multiobjective evolutionary algorithms for developer's assignment to bugs
In this paper, the modeling of developers’ assignment to bugs (DAB) is studied. The problem is modeled both as a single objective (minimize bug fix time) and as a bi‐objective (minimize bug fix time and cost) combinatorial optimization problem. Two models of developer assignment are considered where in the first model a single developer is assigned per bug (single developer model), while in the second model a single developer is assigned for each competency area of a bug (individual competency model). The latter model is proposed in this paper. For the single developer model, GA@DAB, an existing genetic algorithm‐based approach, is extended to support precedence among bugs. For the individual competency model of DAB, one genetic algorithm‐based approach (Competence@DAB) and one nondominated sorting genetic algorithm II‐based approach (CompetenceMulti2@DAB ) are proposed to generate solutions minimizing time and minimizing both time and cost, respectively. The performance of the proposed approaches was evaluated for 2040 bugs of 19 open‐source milestone projects from the Eclipse platform. Our results and analysis show that the proposed individual competency model is far better than the single developer model, with average bug fix time reduction of 39.7% across all projects. Copyright © 2016 John Wiley & Sons, Ltd.
How Practitioners Perceive the Relevance of ESEM Research
Background: The relevance of ESEM research to industry practitioners is key to the long-term health of the conference. Aims: The goal of this work is to understand how ESEM research is perceived within the practitioner community and provide feedback to the ESEM community ensure our research remains relevant. Method: To understand how practitioners perceive ESEM research, we replicated previous work by sending a survey to several hundred industry practitioners at a number of companies around the world. We asked the survey participants to rate the relevance of the research described in 156 ESEM papers published between 2011 and 2015. Results: We received 9,941 ratings by 437 practitioners who labeled ideas as Essential, Worth-while, Unimportant, or Unwise. The results showed that overall, industrial practitioners find the work published in ESEM to be valuable: 67% of all ratings were essential or worthwhile. We found no correlation between citation count and perceived relevance of the papers. Through a qualitative analysis, we also identified a number of research themes on which practitioners would like to see an increased research focus. Conclusions: The work published in ESEM is generally relevant to industrial practitioners. There are a number of topics for which those practitioners would like to see additional research undertaken.
A perspective on blending programming environments and games: Beyond points, badges, and leaderboards
Programming environments and game environments share many of the same characteristics, such as requiring their users to understand strategies and solve difficult challenges. Yet, only game designers have been able to capitalize on methods that are consistently able to keep their users engaged. Consequently, software engineers have been increasingly interested in understanding how these game experiences can be transferred to programming experiences, a process termed gamification. In this perspective paper, we offer a formal argument that gamification as applied today is predominately narrow, placing emphasis on the reward aspects of game mechanics at the expense of other important game elements, such as framing. We argue that more authentic game experiences are possible when programming environments are re-conceptualized and assessed as holistic, serious games. This broad gamification enables us to more effectively apply and leverage the breadth of game elements to the construction and understanding of programming environments.
Keynote Speeches
These Keynotes speeches the following: The Sound of Data: Make Great Software!, Networks of `Things', Automated Software Transplantation, Cross-Disciplinary Modeling-The Good, the Bad, and the Ugly, New Threat Models for Cryptography, Even the Very Wise Cannot See All Ends: Many Facets of the Test Oracle Problem.
Analyzing power consumption in mobile computing devices
Techniques pertaining to analyzing power consumed by a processing unit in a mobile computing device caused by execution of certain modules are described herein. A power trace is generated that indicates an amount of power consumed by the processing unit over time, and the power trace is aligned with an execution log. Spikes are extracted from the power trace, and computing operations are performed over the spikes to acquire data pertaining to power consumed by the processing unit that are attributable to modules in the execution log.
Perspectives on Data Science for Software Engineering
Perspectives on Data Science for Software Engineering presents the best practices of seasoned data miners in software engineering. The idea for this book was created during the 2014 conference at Dagstuhl, an invitation-only gathering of leading computer scientists who meet to identify and discuss cutting-edge informatics topics. At the 2014 conference, the concept of how to transfer the knowledge of experts from seasoned software engineers and data scientists to newcomers in the field highlighted many discussions. While there are many books covering data mining and software engineering basics, they present only the fundamentals and lack the perspective that comes from real-world experience. This book offers unique insights into the wisdom of the community’s leaders gathered to share hard-won lessons from the trenches. Ideas are presented in digestible chapters designed to be applicable across many domains. Topics included cover data collection, data sharing, data mining, and how to utilize these techniques in successful software projects. Newcomers to software engineering data science will learn the tips and tricks of the trade, while more experienced data scientists will benefit from war stories that show what traps to avoid.
Predicting defects in code
A system is described herein that predicts defects in a portion of code of an application that is configured to execute on a computing device. Versions of code are analyzed to locate change bursts, which are alterations to at least one portion of code over time-related events. If a change burst is identified, defects are predicted with respect to the code based at least in part upon the identified change burst.
Belief & evidence in empirical software engineering
Empirical software engineering has produced a steady stream of evidence-based results concerning the factors that affect important outcomes such as cost, quality, and interval. However, programmers often also have strongly-held a priori opinions about these issues. These opinions are important, since developers are highlytrained professionals whose beliefs would doubtless affect their practice. As in evidence-based medicine, disseminating empirical findings to developers is a key step in ensuring that the findings impact practice. In this paper, we describe a case study, on the prior beliefs of developers at Microsoft, and the relationship of these beliefs to actual empirical data on the projects in which these developers work. Our findings are that a) programmers do indeed have very strong beliefs on certain topics b) their beliefs are primarily formed based on personal experience, rather than on findings in empirical research and c) beliefs can vary with each project, but do not necessarily correspond with actual evidence in that project. Our findings suggest that more effort should be taken to disseminate empirical findings to developers and that more in-depth study the interplay of belief and evidence in software practice is needed.
Beliefs, practices, and personalities of software engineers: a survey in a large software company
In this paper we present the results from a survey about the beliefs, practices, and personalities of software engineers in a large software company. The survey received 797 responses. We report statistics about beliefs of software engineers, their work practices, as well as differences in those with respect to personality traits. For example, we observed no personality differences between developers and testers; managers were conscientious and more extraverted. We observed several differences for engineers who are listening to music and for engineers who have built a tool. We also observed that engineers who agree with the statement "Agile development is awesome" were more extroverted and less neurotic.
What went right and what went wrong: an analysis of 155 postmortems from game development
In game development, software teams often conduct postmortems to reflect on what went well and what went wrong in a project. The postmortems are shared publicly on gaming sites or at developer conferences. In this paper, we present an analysis of 155 postmortems published on the gaming site Gamasutra.com. We identify characteristics of game development, link the characteristics to positive and negative experiences in the postmortems and distill a set of best practices and pitfalls for game development.
The emerging role of data scientists on software development teams
Creating and running software produces large amounts of raw data about the development process and the customer usage, which can be turned into actionable insight with the help of skilled data scientists. Unfortunately, data scientists with the analytical and software engineering skills to analyze these large data sets have been hard to come by; only recently have software companies started to develop competencies in software-oriented data analytics. To understand this emerging role, we interviewed data scientists across several product groups at Microsoft. In this paper, we describe their education and training background, their missions in software engineering contexts, and the type of problems on which they work. We identify five distinct working styles of data scientists: (1) Insight Providers, who work with engineers to collect the data needed to inform decisions that managers make; (2) Modeling Specialists, who use their machine learning expertise to build predictive models; (3) Platform Builders, who create data platforms, balancing both engineering and data analysis concerns; (4) Polymaths, who do all data science activities themselves; and (5) Team Leaders, who run teams of data scientists and spread best practices. We further describe a set of strategies that they employ to increase the impact and actionability of their work.
One size does not fit all
Not every prediction model can be transferred from one context to another. I've learned this the hard way in my work on defect prediction. The good news is that while prediction models might not always be applicable to other contexts, the methods typically are.
Past, present, and future of analyzing software data
This chapter introduces the book and offers some context for the rest of the chapters. Specifically, we explore different definitions of ”software analytics” as well the historical evolution of the data science for software engineering.
Card-sorting: From text to themes
Suppose you just ran a survey with the question “What problems are stopping us from meeting our deadlines?” Since you didn’t know the problems in advance, you asked participants to simply write the problems into a textbox. This type of question is called an open-ended question; in contrast to closed-ended questions, which limit participants to a list of predefined answer choices. Your survey was very successful and you received hundreds of responses. Now you want to make sense of the data. How can you do this in a systematic way? In this essay, I will show you how card sorting can infer themes from text responses. You will learn everything you need to know for your first card sort.
Ramp-Up Journey of New Hires: Tug of War of Aids and Impediments
Hiring top talent is essential for any software company's success. After joining the company, new hires often spend weeks or months before making any major contribution and attaining the same productivity level as existing employees. We use the term ramp-up journey to refer to this transition of new hires from novice to experts. There can be several factors, such as lack of experience or lack of familiarity with processes unique to the new company, which influence the ramp-up journey. To understand such aids and impediments in the ramp-up journey, we conducted a study by analyzing data extracted from version control systems of eight large and popular product groups in Microsoft with several thousand software developers. In particular, we studied two aspects of the ramp-up journey. First, we studied time taken to make the first check-in into the version control system, an important milestone in the ramp-up journey indicating the first contribution. Second, we analyzed the time taken to reach the same productivity level as existing employees in terms of check-ins. We further augmented our quantitative study with qualitative results derived by surveying 411 professional developers. Our study produced promising results, including factors such as having a mentor, prior knowledge of required skill sets, and proactively asking questions, that could help reduce the ramp-up journey of new hires.
What drives people: Creating engagement profiles of players from game log data
A central interest of game designers and game user researchers is to understand why players enjoy their games. While a number of researchers have explored player enjoyment in general, few have talked about methods for enabling designers to understand the players of their specific game. In this paper we explore the creation of engagement profiles of game players based on log data. These profiles take into account the different ways that players engage with the game and highlight patterns associated with active play. We demonstrate our approach by performing a descriptive analysis of the game Forza Motorsport 5 using data from a sample of 1.2 million users of the game and discuss the implications of our findings.
Discovering and exploiting relationships in software repositories
Software development items can be represented in a graph data structure. Relationships between the represented items can be detected and reflected in the graph data structure. Queries can be run against the data structure to determine which software development items are related to each other. Implicit query can be implemented in a software development context. A graph browser can present panes showing related items.In some embodiments, a set of regular expressions can be used to identify paths in a graph. Probability scores for the identified paths can be computed. Path data for the identified paths, including the probability scores, can be stored in a searchable location accessible by one or more applications. A query of the path data can be processed to return query results associated with at least one of the identified paths.
The art and science of analyzing software data
The Art and Science of Analyzing Software Data provides valuable information on analysis techniques often used to derive insight from software data. This book shares best practices in the field generated by leading data scientists, collected from their experience training software engineering students and practitioners to master data science. The book covers topics such as the analysis of security data, code reviews, app stores, log files, and user telemetry, among others. It covers a wide variety of techniques such as co-change analysis, text analysis, topic analysis, and concept analysis, as well as advanced topics such as release planning and generation of source code comments. It includes stories from the trenches from expert data scientists illustrating how to apply data analysis in industry and open source, present results to stakeholders, and drive decisions.
How practitioners perceive the relevance of software engineering research
The number of software engineering research papers over the last few years has grown significantly. An important question here is: how relevant is software engineering research to practitioners in the field? To address this question, we conducted a survey at Microsoft where we invited 3,000 industry practitioners to rate the relevance of research ideas contained in 571 ICSE, ESEC/FSE and FSE papers that were published over a five year period. We received 17,913 ratings by 512 practitioners who labelled ideas as essential, worthwhile, unimportant, or unwise. The results from the survey suggest that practitioners are positive towards studies done by the software engineering research community: 71% of all ratings were essential or worthwhile. We found no correlation between the citation counts and the relevance scores of the papers. Through a qualitative analysis of free text responses, we identify several reasons why practitioners considered certain research ideas to be unwise. The survey approach described in this paper is lightweight: on average, a participant spent only 22.5 minutes to respond to the survey. At the same time, the results can provide useful insight to conference organizers, authors, and participating practitioners.
Products, developers, and milestones: how should I build my N-Gram language model
Recent work has shown that although programming languages enable source code to be rich and complex, most code tends to be repetitive and predictable. The use of natural language processing (NLP) techniques applied to source code such as n-gram language models show great promise in areas such as code completion, aiding impaired developers, and code search. In this paper, we address three questions related to different methods of constructing language models in an industrial context. Specifically, we ask: (1) Do application specific, but smaller language models perform better than language models across applications? (2) Are developer specific language models effective and do they differ depending on what parts of the codebase a developer is working in? (3) Finally, do language models change over time, i.e., does a language model from early development model change later on in development? The answers to these questions enable techniques that make use of programming language models in development to choose the model training corpus more effectively. We evaluate these questions by building 28 language models across developers, time periods, and applications within Microsoft Office and present the results in this paper. We find that developer and application specific language models perform better than models from the entire codebase, but that temporality has little to no effect on language model performance.
Quantifying developers' adoption of security tools
Security tools could help developers find critical vulnerabilities, yet such tools remain underused. We surveyed developers from 14 companies and 5 mailing lists about their reasons for using and not using security tools. The resulting thirty-nine predictors of security tool use provide both expected and unexpected insights. As we expected, developers who perceive security to be important are more likely to use security tools than those who do not. But that was not the strongest predictor of security tool use, it was instead developers' ability to observe their peers using security tools.
Software Analytics for Digital Games
Software and its development generates an inordinate amount of data. Development activities such as check-ins, work items, bug reports, code reviews, and test executions are recorded in software repositories. User interactions that reflect how customers experience software are recorded in telemetry data, run-time traces, and log files and helps to track application and feature usage and expose performance and reliability. Software analytics takes this data and turns it into actionable insight to better inform decisions related to software. In this talk, I will summarize our efforts in the area of software analytics with a special focus on digital games. I will present several examples of games studies, which we have worked on at Microsoft Research such as how players are engaged in Project Gotham Racing, how skill develops over time in Halo Reach and Forza Motor sports, and the initial experience of game play. I will also point out important differences between games development and traditional software development. The work presented in this talk has been done by Nachi Nagappan, myself, and many others who have visited our group over the past years.
1st international workshop on big data software engineering (BIGDSE 2015)
Big Data is about extracting valuable information from data in order to use it in intelligent ways such as to revolutionize decision-making in businesses, science and society. BIGDSE 2015 discusses the link between Big Data and software engineering and critically looks into issues such as cost-benefit of big data.
Build it yourself!: homegrown tools in a large software company
Developers sometimes take the initiative to build tools to solve problems they face. What motivates developers to build these tools? What is the value for a company? Are the tools built useful for anyone besides their creator? We conducted a qualitative study of tool building, adoption, and impact within Microsoft. This paper presents our findings on the extrinsic and intrinsic factors linked to toolbuilding, the value of building tools, and the factors associated with tool spread. We find that the majority of developers build tools. While most tools never spread beyond their creator's team, most have more than one user, and many have more than one collaborator. Organizational cultures that are receptive towards toolbuilding produce more tools, and more collaboration on tools. When nurtured and spread, homegrown tools have the potential to create significant impact on organizations.
The uniqueness of changes: characteristics and applications
Changes in software development come in many forms. Some changes are frequent, idiomatic, or repetitive (e.g. adding checks for nulls or logging important values) while others are unique. We hypothesize that unique changes are different from the more common similar (or non-unique) changes in important ways; they may require more expertise or represent code that is more complex or prone to mistakes. As such, these unique changes are worthy of study. In this paper, we present a definition of unique changes and provide a method for identifying them in software project history. Based on the results of applying our technique on the Linux kernel and two large projects at Microsoft, we present an empirical study of unique changes. We explore how prevalent unique changes are and investigate where they occur along the architecture of the project. We further investigate developers' contribution towards uniqueness of changes. We also describe potential applications of leveraging the uniqueness of change and implement two of those applications, evaluating the risk of changes based on uniqueness and providing change recommendations for non-unique changes.
Understanding the test automation culture of app developers
Smartphone applications (apps) have gained popularity recently. Millions of smartphone applications (apps) are available on different app stores which gives users plethora of options to choose from, however, it also raises concern if these apps are adequately tested before they are released for public use. In this study, we want to understand the test automation culture prevalent among app developers. Specifically, we want to examine the current state of testing of apps, the tools that are commonly used by app developers, and the problems faced by them. To get an insight on the test automation culture, we conduct two different studies. In the first study, we analyse over 600 Android apps collected from F- Droid, one of the largest repositories containing information about open-source Android apps. We check for the presence of test cases and calculate code coverage to measure the adequacy of testing in these apps. We also survey developers who have hosted their applications on GitHub to understand the testing practices followed by them. We ask developers about the tools that they use and ''pain points'' that they face while testing Android apps. For the second study, based on the responses from Android developers, we improve our survey questions and resend it to Windows app developers within Microsoft. We conclude that many Android apps are poorly tested - only about 14% of the apps contain test cases and only about 9% of the apps that have executable test cases have coverage above 40\%. Also, we find that Android app developers use automated testing tools such as JUnit, Monkeyrunner, Robotium, and Robolectric, however, they often prefer to test their apps manually, whereas Windows app developers prefer to use in-house tools such as Visual Studio and Microsoft Test Manager. Both Android and Windows app developers face many challenges such as time constraints, compatibility issues, lack of exposure, cumbersome tools, etc. We give suggestions to improve the test automation culture in the growing app community.
Do topics make sense to managers and developers?
Large organizations like Microsoft tend to rely on formal requirements documentation in order to specify and design the software products that they develop. These documents are meant to be tightly coupled with the actual implementation of the features they describe. In this paper we evaluate the value of high-level topic-based requirements traceability and issue report traceability in the version control system, using Latent Dirichlet Allocation (LDA). We evaluate LDA topics on practitioners and check if the topics and trends extracted match the perception that industrial Program Managers and Developers have about the effort put into addressing certain topics. We then replicate this study again on Open Source Developers using issue reports from issue trackers instead of requirements, confirming our previous industrial conclusions. We found that efforts extracted as commits from version control systems relevant to a topic often matched the perception of the managers and developers of what actually occurred at that time. Furthermore we found evidence that many of the identified topics made sense to practitioners and matched their perception of what occurred. But for some topics, we found that practitioners had difficulty interpreting and labelling them. In summary, we investigate the high-level traceability of requirements topics and issue/bug report topics to version control commits via topic analysis and validate with the actual stakeholders the relevance of these topics extracted from requirements and issues.
Analyzing power consumption in mobile computing devices
Techniques pertaining to analyzing power consumed by a processing unit in a mobile computing device caused by execution of certain modules are described herein. A power trace is generated that indicates an amount of power consumed by the processing unit over time, and the power trace is aligned with an execution log. Spikes are extracted from the power trace, and computing operations are performed over the spikes to acquire data pertaining to power consumed by the processing unit that are attributable to modules in the execution log.
The design space of bug fixes and how developers navigate it
When software engineers fix bugs, they may have several options as to how to fix those bugs. Which fix they choose has many implications, both for practitioners and researchers: What is the risk of introducing other bugs during the fix? Is the bug fix in the same code that caused the bug? Is the change fixing the cause or just covering a symptom? In this paper, we investigate alternative fixes to bugs and present an empirical study of how engineers make design choices about how to fix bugs. We start with a motivating case study of the Pex4Fun environment. Then, based on qualitative interviews with 40 engineers working on a variety of products, data from six bug triage meetings, and a survey filled out by 326 Microsoft engineers and 37 developers from other companies, we found a number of factors, many of them non-technical, that influence how bugs are fixed, such as how close to release the software is. We also discuss implications for research and practice, including how to make bug prediction and localization more accurate.
Data hard with a vengeance (invited talk)
Action flicks and the analysis of software data in industry have more in common than you think. Both action heroes and development teams are on tight deadlines to save the day. Getting wrong information can lead to disastrous outcomes. In this talk, I will share experiences from my six years of research in the Empirical Software Engineering Group working with engineers towards sound data-driven decision about software.
Software developers' perceptions of productivity
The better the software development community becomes at creating software, the more software the world seems to demand. Although there is a large body of research about measuring and investigating productivity from an organizational point of view, there is a paucity of research about how software developers, those at the front-line of software construction, think about, assess and try to improve their productivity. To investigate software developers' perceptions of software development productivity, we conducted two studies: a survey with 379 professional software developers to help elicit themes and an observational study with 11 professional software developers to investigate emergent themes in more detail. In both studies, we found that developers perceive their days as productive when they complete many or big tasks without significant interruptions or context switches. Yet, the observational data we collected shows our participants performed significant task and activity switching while still feeling productive. We analyze such apparent contradictions in our findings and use the analysis to propose ways to better support software developers in a retrospection and improvement of their productivity through the development of new tools and the sharing of best practices.
The first hour experience: how the initial play can engage (or lose) new players
The first time a player sits down with a game is critical for their engagement. Games are a voluntary activity and easy to abandon. If the game cannot hold player attention, it will not matter how much fun the game is later on if the player quits early. Worse, if the initial experience was odious enough, the player will dissuade others from playing. Industry advice is to make the game fun from the start to hook the player. In our analysis of over 200 game reviews and interviews with industry professionals, we advance an alternative, complementary solution. New design terminology is introduced such as "holdouts" (what keeps players playing despite poor game design) and the contrast between momentary fun vs. intriguing experiences. Instead of prioritizing fun, we assert that intrigue and information should be seen as equally valuable for helping players determine if they want to continue playing. The first sustained play session (coined "first hour"), when inspected closely, offers lessons for game development and our understanding of how players evaluate games as consumable products.
Keynote: Large Scale Analysis of Software Repositories in Industry: Experiences from the CodeMine Project by Thomas Zimmermann
These Keynotes speeches the following: Large Scale Analysis of Software Repositories in Industry: Experiences from the CodeMine Project.
Mining energy traces to aid in software development: An empirical case study
Context: With the advent of increased computing on mobile devices such as phones and tablets, it has become crucial to pay attention to the energy consumption of mobile applications. Goal: The software engineering field is now faced with a whole new spectrum of energy-related challenges, ranging from power budgeting to testing and debugging the energy consumption, for which exists only limited tool support. The goal of this work is to provide techniques to engineers to analyze power consumption and detect anomalies. Method: In this paper, we present our work on analyzing energy patterns for the Windows Phone platform. We first describe the data that is collected for testing (power traces and execution logs). We then present several approaches for describing power consumption and detecting anomalous energy patterns and potential energy defects. Finally we show prediction models based on usage of individual modules that can estimate the overall energy consumption with high accuracy. Results: The techniques in this paper were successful in modeling and estimating power consumption and in detecting anomalies. Conclusions: The techniques presented in the paper allow assessing the individual impact of modules on the overall energy consumption and support overall energy planning.Mining energy traces to aid in software development: An empirical case study
An empirical study of refactoringchallenges and benefits at microsoft
It is widely believed that refactoring improves software quality and developer productivity. However, few empirical studies quantitatively assess refactoring benefits or investigate developers' perception towards these benefits. This paper presents a field study of refactoring benefits and challenges at Microsoft through three complementary study methods: a survey, semi-structured interviews with professional software engineers, and quantitative analysis of version history data. Our survey finds that the refactoring definition in practice is not confined to a rigorous definition of semantics-preserving code transformations and that developers perceive that refactoring involves substantial cost and risks. We also report on interviews with a designated refactoring team that has led a multi-year, centralized effort on refactoring Windows. The quantitative analysis of Windows 7 version history finds the top 5 percent of preferentially refactored modules experience higher reduction in the number of inter-module dependencies and several complexity measures but increase size more than the bottom 95 percent. This indicates that measuring the impact of refactoring requires multi-dimensional assessment.
CA-Fi: Ubiquitous mobile wireless networking without 802.11 overhead and restrictions
The proliferation, flexibility, and mobility of wireless communication devices provides a readily available basis for ubiquitous mobile communication. However, the network-centric design of 802.11 does not support the spontaneity, continuity, and ubiquitous scope of mobile communication for two reasons: i) The time and maintenance overhead of 802.11 networks prevents spontaneous device interaction, and ii) 802.11 restricts the communication scope to devices in a common network. We thus enable ubiquitous wireless communication, independent from 802.11 network structures in CA-Fi, a continuous, low-overhead communication channel concurrent to 802.11 associations. CA-Fi augments 802.11 with an association-less broadcast mechanism of up to 30 kB/s, preserving up to 70 % of simultaneous 802.11 throughput and enabling unrestricted and instant networking in the wireless medium. Directly addressing applications in 802.11 frames using Bloom filters enables message aggregation for space efficiency and saves communication overhead by unifying peer and application discovery. CA-Fi inherently supports duty cycling and saves up to 44 % of the energy consumption of the 802.11 ad-hoc mode.
Confessions of an industrial researcher: a typical bollywood story (invited talk)
There’s a guy. There’s a girl. In the beginning they don’t like each other. Then they fall in love but when they fall in love their families do not accept. In the end, how they unite is the story of many Bollywood movies. Research-practice partnerships are similar. How research and practice can overcome hurdles and join together is the topic of this talk. At Microsoft Research I am in the fortunate position of being able to bridge two worlds. As part of my work, I collaborate with engineers from large-scale software projects at Microsoft as well as students and professors from universities all over the world. I will describe the collaboration model of the Empirical Software Engineering (ESE) group at Microsoft Research and share my experiences based on several successful collaborations over the past years. I will discuss best practices, challenges, common mistakes, and lessons learned based on several empirical studies related to refactoring, bug reporting, and analytics in general.
Analyzing software data: after the gold rush (a goldfish-bowl panel)
Over the past few years, the volume and types of data related to software engineering has grown at an unprecedented rate and shows no sign of slowing. This turn of events has led to a veritable gold rush, as researchers attempt to mine raw data and extract nuggets of insight. A very real danger is that the landscape may become a Wild West where inexperienced software "cowboys" sell hastily generated models to unsophisticated business users, without any concern for best or safe practices. Given the current enthusiasm for data analysis in software engineering, it is time to review how we using those techniques and can we use them better. While there may be no single best "right" way to analyze software data, there are many wrong ways. As data techniques mature, we need to move to a new era where data scientists understand and share the strengths and drawbacks of the many methods that might be deployed in industry. In this highly interactive panel skilled practitioners and academics can (a) broadcast their insights and (b) hear the issues of newcomers in this field.
Extrinsic influence factors in software reliability: A study of 200,000 windows machines
Reliability of software depends not only on intrinsic factors such as its code properties, but also on extrinsic factors—that is, the properties of the environment it operates in. In an empirical study of more than 200,000 Windows users, we found that the reliability of individual applications is related to whether and which other applications are in-stalled: While games and file-sharing applications tend to decrease the reliability of other applications, security applications tend to increase it. Furthermore, application reliability is related to the usage profiles of these applications; generally, the more an application is used, the more likely it is to have negative impact on reliability of others. As a conse-quence, software testers must be careful to investigate and control these factors.
Understanding and improving software build teams
Build, creating software from source code, is a fundamental activity in software development. Build teams manage this process and ensure builds are produced reliably and efficiently. This paper presents an exploration into the nature of build teams--how they form, work, and relate to other teams--through three multi-method studies conducted at Microsoft. We also consider build team effectiveness and find that many challenges are social, not technical: role ambiguity, knowledge sharing, communication, trust, and conflict. Our findings validate theories from group dynamics and organization science, and using a cross-discipline approach, we apply learnings from these fields to inform the design of engineering tools and practices to improve build team effectiveness
Cowboys, ankle sprains, and keepers of quality: How is video game development different from software development?
Video games make up an important part of the software industry, yet the software engineering community rarely studies video games. This imbalance is a problem if video game development differs from general software development, as some game experts suggest. In this paper we describe a study with 14 interviewees and 364 survey respondents. The study elicited substantial differences between video game development and other software development. For example, in game development, “cowboy coders” are necessary to cope with the continuous interplay between creative desires and technical constraints. Consequently, game developers are hesitant to use automated testing because of these tests’ rapid obsolescence in the face of shifting creative desires of game designers. These differences between game and non-game development have implications for research, industry, and practice. For instance, as a starting point for impacting game development, researchers could create testing tools that enable game developers to create tests that assert flexible behavior with little up-front investment.
Analyze this! 145 questions for data scientists in software engineering
In this paper, we present the results from two surveys related to data science applied to software engineering. The first survey solicited questions that software engineers would like data scientists to investigate about software, about software processes and practices, and about software engineers. Our analyses resulted in a list of 145 questions grouped into 12 categories. The second survey asked a different pool of software engineers to rate these 145 questions and identify the most important ones to work on first. Respondents favored questions that focus on how customers typically use their applications. We also saw opposition to questions that assess the performance of individual employees or compare them with one another. Our categorization and catalog of 145 questions can help researchers, practitioners, and educators to more easily focus their efforts on topics that are important to the software industry.
Recommendation systems in software engineering
With the growth of public and private data stores and the emergence of off-the-shelf data-mining technology, recommendation systems have emerged that specifically address the unique challenges of navigating and interpreting software engineering data. This book collects, structures and formalizes knowledge on recommendation systems in software engineering. It adopts a pragmatic approach with an explicit focus on system design, implementation, and evaluation. The book is divided into three parts: “Part I – Techniques” introduces basics for building recommenders in software engineering, including techniques for collecting and processing software engineering data, but also for presenting recommendations to users as part of their workflow. “Part II – Evaluation” summarizes methods and experimental designs for evaluating recommendations in software engineering. “Part III – Applications” describes needs, issues and solution concepts involved in entire recommendation systems for specific software engineering tasks, focusing on the engineering insights required to make effective recommendations. The book is complemented by the webpage rsse.org/book, which includes free supplemental materials for readers of this book and anyone interested in recommendation systems in software engineering, including lecture slides, data sets, source code, and an overview of people, groups, papers and tools with regard to recommendation systems in software engineering. The book is particularly well-suited for graduate students and researchers building new recommendation systems for software engineering applications or in other high-tech fields. It may also serve as the basis for graduate courses on recommendation systems, applied data mining or software engineering. Software engineering practitioners developing recommendation systems or similar applications with predictive functionality will also benefit from the broad spectrum of topics covered.
Persuasive technology in the real world: a study of long-term use of activity sensing devices for fitness
Persuasive technology to motivate healthy behavior is a growing area of research within HCI and ubiquitous computing. The emergence of commercial wearable devices for tracking health- and fitness-related activities arguably represents the first widespread adoption of dedicated ubiquitous persuasive technology. The recent ubiquity of commercial systems allows us to learn about their value and use in truly "in the wild" contexts and understand how practices evolve over long-term, naturalistic use. We present a study with 30 participants who had adopted wearable activity-tracking devices of their own volition and had continued to use them for between 3 and 54 months. The findings, which both support and contrast with those of previous research, paint a picture of the evolving benefits and practices surrounding these emerging technologies over long periods of use. They also serve as the basis for design implications for personal informatics technologies for long-term health and fitness support.
Off with their assists: An empirical study of driving skill in Forza Motorsport 4.
This paper describes patterns of skill progression in Forza Motorsport 4, an Xbox 360 racing game. Using in-game telemetry data from more than 200,000 players and 24 million races, we characterize how players use and customize driving assists such as the trajectory line, automatic gear shifting, or assisted braking over time. We find that some of the assists are never disabled by significant player segments. Some “yoyo” players repeatedly enable and disable assists. We also present a model to predict when a player is ready to successfully disable an assist with a precision ranging from 60 to 90 percent.
Analyzing metropolitan-area networking within public transportation systems for smart city applications
In the scope of smart cities, mobile participatory sensing and metropolitan area networking on top of public transportation systems for communication offers widespread dissemination of information in both time and spatial domains. Specifically, the transportation network naturally reflects urban human mobility patterns between places of interest and interconnects hotspots where information is created and consumed by city- wide applications. Previous work has targeted communication exclusively between users of the public transportation system. In this paper, we provide an analysis of metropolitan networking within the transportation system itself. Instead of relying on user-generated contact traces purely between mobile entities, i.e., busses, we build on a comprehensive data set that contains the schedules and location data of busses as well as the location of infrastructure elements, such as bus stops. Our analysis shows the general feasibility of such a network as well as the, previously not considered, impact of infrastructure elements for information dissemination. The latter motivates delay-tolerant and location- driven communication, as well as participatory sensing using the transport system as a communication infrastructure.
Software Development Analytics (Dagstuhl Seminar 14261)
This report documents the program and the outcomes of Dagstuhl Seminar 14261 "Software Development Analytics". We briefly summarize the goals and format of the seminar, the results of the break out groups, and a draft of a manifesto for software analytics. The report also includes the abstracts of the talks presented at the seminar.
Mining bug data
Although software systems control many aspects of our daily life world, no system is perfect. Many of our day-to-day experiences with computer programs are related to software bugs. Although software bugs are very unpopular, empirical software engineers and software repository analysts rely on bugs or at least on those bugs that get reported to issue management systems. So what makes data software repository analysts appreciate bug reports? Bug reports are development artifacts that relate to code quality and thus allow us to reason about code quality, and quality is key to reliability, end-users, success, and finally profit. This chapter serves as a hand-on tutorial on how to mine bug reports, relate them to source code, and use the knowledge of bug fix locations to model, estimate, or even predict source code quality. This chapter also discusses risks that should be addressed before one can achieve reliable recommendation systems.
Introduction to the special issue on mining software repositories
The Mining Software Repositories field analyzes the rich data available in software repositories to uncover interesting and actionable information about software systems and projects. Thanks to the ready availability of software configuration management, mailing list, and bug tracking repositories from open source projects, it has gained popularity since 2004 with the first instance of the MSR workshop (now conference) and continues to be one of the fastest growing fields in the area of software engineering. Researchers in this field empirically explore a range of software engineering questions using software repository data as the primary source of information. Some commonly explored areas include software evolution, models of software development processes, characterization of developers and their activities, prediction of future software qualities, use of machine learning techniques on software project data, software bug prediction, analysis of software change patterns, and analysis of code clones. There has also been a stream of work on tools for mining software repositories, and techniques for visualizing software repository data. In recent years the importance of the field has further increased as today's society and businesses become more data-driven. Industry has a strong interest in transforming software repository data into actionable insights to inform better development decisions. Analytics is already commonly used in many businesses—notably in marketing, to better reach and understand customers (May 2009). The application of analytics to software development is becoming more popular. For those wishing to develop a broad understanding of the software repository mining research field, there are several resources. Tao Xie and Ahmed E. Hassan provide an overview of software repository mining research areas and methods in the tutorial notes to their Mining Software Engineering Data tutorial, given at several recent software engineering conferences (notes are available at https://sites.google.com/site/asergrp/dmse/). Kagdi et al. provide a survey of software repository mining techniques in (Kagdi et al. 2007) that focuses on software evolution research, but which provides broad coverage of a wide range of research methods and repository types used in the field. Hassan provides a survey of the entire field along with future research challenges in (Hassan 2008). Together, these three sources give a strong general introduction to software repository mining research. The PROMISE repository (http://promisedata.org/) is a collection of 140+ software engineering data sets related to defect prediction, effort estimation, model-based software engineering and many other topics. The Working Conference on Mining Software Repositories (MSR) is co-located every year with the ACM/IEEE International Conference on Software Engineering (ICSE). The MSR conference brings together researchers who share an interest in advancing the science and practice of software engineering via the analysis of data stored in software repositories. Papers in the Mining Software Repositories field tend to take a quantitative empirical approach to exploring research questions. As a consequence, it is natural to select the best papers from the MSR conference for inclusion in Empirical Software Engineering. The papers in this special issue provide a good cross section of the topics and approaches recently explored in the mining software repositories community (MSR 2011). In the first paper, “Adoption and use of Java generics”, Parnin, Bird, and Murphy-Hill aim at obtaining insight in the usefulness of Java generics. To that end, they analyze actual usage of Java generics throughout the history of 40 popular open source Java programs. Their observations help us understand how new language features are adopted, and to what extent developers believe that the benefits of generics outweigh the pain of adoption. Key insights include that projects are cautious, and usually do not adopt a large-scale conversion of raw to parameterized types. Furthermore, generics are usually adopted by a single champion in the project, rather than by all committers. In the paper “How do open source communities blog?” Pagano and Maalej analyzed the behavior of 1,100 bloggers in four large open source communities to understand how software communities use blogs compared to conventional development infrastructures. Among other findings they observed an intensive usage of blogs with one new entry every eight hours. Their findings call for a hypothesis-driven research to further understand the role of social media in dissolving the collaboration boundaries between developers and other stakeholders and integrate social media into development processes and tools. In the paper “Automated topic naming”, Hindle, Ernst, Godfrey, and Mylopoulos propose an automated solution to suggest topics that describe and relate software artifacts. Previous work attempted to summarize, categorize, and relate software artifacts by using machine-learning techniques such as Latent Dirichlet Allocation (LDA). However, results produced by the previous work are difficult to interpret without meaningful summary labels. The solution proposed by Hindle extracts topics using LDA from commit-log comments and then labels these topics from a generalizable cross-project taxonomy. Their study results on the MySQL, PostgreSQL and MaxDB projects show that their solution can produce appropriate, context-sensitive, insightful labels relevant to these projects. In the paper “How (and why) developers use the dynamic features of programming languages: the case of Smalltalk”, Callaú, Robbes, Tanter, and Röthlisberger perform an empirical study of a large Smalltalk codebase to assess how much dynamic and reflective features are actually used in practice, whether some are used more than others, and in which kinds of projects. In addition, they uncover reasons that drive people to use dynamic features, and how these dynamic feature usages can be removed or converted to safer usages. These results are useful to make informed decisions about which features to consider when designing language extensions or tool support. In the paper “Software Bertillonage”, Davies, German, Godfrey, and Hindle recover the provenance of software entities with a fast, simple, and approximate technique. The provenance of components included in a deployed software system (e.g., external libraries or cloned source code) is often not clearly documented, causing technical and ethical concerns when managing software assets. To address such issue, Davies et al. have developed a fast, simple, and approximate technique called anchored signature matching for identifying the source origin of binary libraries within a given Java application. To demonstrate the validity and effectiveness of their technique, they conducted an empirical study on 945 jars from the Debian GNU/Linux distribution, as well as an industrial case study on 81 jars from an e-commerce application. We hope you enjoy the papers in this special issue.
Predicting risk of pre-release code changes with checkinmentor
Code defects introduced during the development of the software system can result in failures after its release. Such post-release failures are costly to fix and have negative impact on the reputation of the released software. In this paper we propose a methodology for early detection of faulty code changes. We describe code changes with metrics and then use a statistical model that discriminates between faulty and non-faulty changes. The predictions are done not at a file or binary level but at the change level thereby assessing the impact of each change. We also study the impact of code branches on collecting code metrics and on the accuracy of the model. The model has shown high accuracy and was developed into a tool called CheckinMentor. CheckinMentor was deployed to predict risk for the Windows Phone software. However, our methodology is versatile and can be used to predict risk in a variety of large complex software systems.
Have agile techniques been the silver bullet for software development at microsoft?
Background. The pressure to release high-quality, valuable software products at an increasingly faster rate is forcing software development organizations to adapt their development practices. Agile techniques began emerging in the mid-1990s in response to this pressure and to increased volatility of customer requirements and technical change. Theoretically, agile techniques seem to be the silver bullet for responding to these pressures on the software industry. Aims. This paper tracks the changing attitudes to agile adoption and techniques, within Microsoft, in one of the largest longitudinal surveys of its kind (2006-2012). Method. We collected the opinions of 1,969 agile and non-agile practitioners in five surveys over a six-year period. Results. The survey results reveal that despite intense market pressure, the growth of agile adoption at Microsoft is slower than would be expected. Additionally, no individual agile practice exhibited strong growth trends. We also found that while development practices of teams may be similar, some perceive and declare themselves to be following an agile methodology while others do not. Both agile and non-agile practitioners agree on the relative benefits and problem areas of agile techniques. Conclusions. We found no clear trends in practice adoption. Non-agile practitioners are less enamored of the benefits and more strongly in agreement with the problem areas. The ability for agile practices to be used by large-scale teams generally concerned all respondents, which may limit its future adoption.
Software analytics= sharing information
Software and its development generates an inordinate amount of data. Development activities such as check-ins, work items, bug reports, code reviews, and test executions are recorded in software repositories. User interactions that reflect how customers experience software are recorded in telemetry data, run-time traces, and log files and helps to track application and feature usage and expose performance and reliability. Software analytics takes this data and turns it into actionable insight to better inform decisions related to software. To a large extent, software analytics is about what we can learn and share about software. This include our own projects but also projects by others. Looking back at decades of research in empirical software engineering and mining software repositories, software analytics lets us share different things: insights, models, methods, and data. In this talk, I will present successful efforts from Microsoft and academia on software analytics and try to convince you that analytics is all about sharing information.
The many faces of software analytics
Articles regarding the many faces of software analytics highlight the power of analytics for different types of organizations: large organizations and open source projects, as well as small- to medium-sized projects.
Diversity in software engineering research
One of the goals of software engineering research is to achieve generality: Are the phenomena found in a few projects reflective of others? Will a technique perform as well on projects other than the projects it is evaluated on? While it is common sense to select a sample that is representative of a population, the importance of diversity is often overlooked, yet as important. In this paper, we combine ideas from representativeness and diversity and introduce a measure called sample coverage, defined as the percentage of projects in a population that are similar to the given sample. We introduce algorithms to compute the sample coverage for a given set of projects and to select the projects that increase the coverage the most. We demonstrate our technique on research presented over the span of two years at ICSE and FSE with respect to a population of 20,000 active open source projects monitored by Ohloh.net. Knowing the coverage of a sample enhances our ability to reason about the findings of a study. Furthermore, we propose reporting guidelines for research: in addition to coverage scores, papers should discuss the target population of the research (universe) and dimensions that potentially can influence the outcomes of a research (space).
Cooperative alternation for termination and non-termination
I present the work I have realised during my internship at Microsoft Research Cambridge, under the supervision of Byron Cook. The goal was to build a tool to combine already existing termination and non-termination provers with an alternating procedure. We have got some interesting results which confirm the interest of such an alternation. However, the main contributions of my own work are concentrated in the non-termination prover as I have developed new ideas and implemented them.
Software analytics: so what?
The guest editors of this special issue of IEEE Software invited submissions that reflected the benefits (and drawbacks) of software analytics, an area of explosive growth. They had so many excellent submissions that they had to split this special issue into two volumes--you'll see even more content in the September/October issue. They divided the articles on conceptual grounds, so both volumes will feature equally excellent work. The Web extra at http://youtu.be/nO6X0azR0nw is a video interview in which IEEE Software editor in chief Forrest Shull speaks with Tim Menzies about the growing importance of software analytics.
A power system redution method for the torsional torque analysis of turbine-generator shafts
This paper proposes a method to reduce a large power system to a simple system in order to analyze the torsional torque of a turbine-generator shaft. First, a large power system is divided into 2 areas such as the area of concern and the external area. Secondly, the external area is simplified to the equivalent Thevenin's voltage sources. The reduced simple power system is modeled with EMTP-RV software to simulate the power system disturbances in accordance with the predetermined sequence of events. To verify the proposed reduction method, the outputs of EMTP-RV such as voltage profile, active power and reactive power at the generator output terminal in the reduced power system are compared with the results of PSS/E simulation in the original large power system.
Local versus global lessons for defect prediction and effort estimation
Existing research is unclear on how to generate lessons learned for defect prediction and effort estimation. Should we seek lessons that are global to multiple projects or just local to particular projects? This paper aims to comparatively evaluate local versus global lessons learned for effort estimation and defect prediction. We applied automated clustering tools to effort and defect datasets from the PROMISE repository. Rule learners generated lessons learned from all the data, from local projects, or just from each cluster. The results indicate that the lessons learned after combining small parts of different data sources (i.e., the clusters) were superior to either generalizations formed over all the data or local lessons formed from particular projects. We conclude that when researchers attempt to draw lessons from some historical data source, they should 1) ignore any existing local divisions into multiple sources, 2) cluster across all available data, then 3) restrict the learning of lessons to the clusters from other sources that are nearest to the test data.
Improving developer participation rates in surveys
Doing high quality research about the human side of software engineering necessitates the participation of real software developers in studies, but getting high levels of participation is a challenge for software engineering researchers. In this paper, we discuss several factors that software engineering researchers can use when recruiting participants, drawn from a combination of general research on survey design, research on persuasion, and our experience in conducting surveys. We study these factors by performing post-hoc analysis on several previously conducted surveys. Our results provide insight into the factors associated with increased response rates, which are neither wholly composed of factors associated strictly with persuasion research, nor those of conventional wisdom in software engineering.
10th Working Conference on Mining Software Repositories (MSR)
No abstract available.
1st International workshop on data analysis patterns in software engineering (DAPSE 2013)
Data scientists in software engineering seek insight in data collected from software projects to improve software development. The demand for data scientists with domain knowledge in software development is growing rapidly and there is already a shortage of such data scientists. Data science is a skilled art with a steep learning curve. To shorten that learning curve, this workshop will collect best practices in form of data analysis patterns, that is, analyses of data that leads to meaningful conclusions and can be reused for comparable data. In the workshop we compiled a catalog of such patterns that will help experienced data scientists to better communicate about data analysis. The workshop was targeted at experienced data scientists and researchers and anyone interested in how to analyze data correctly and efficiently in a community accepted way.
Distributed development considered harmful?
We offer a case study illustrating three rules for reporting research to industrial practitioners. Firstly, report “relevant” results; e.g. this paper explores the effects of distributed development on software products. Second: “recheck” old results if new results call them into question. Many papers say distributed development can be harmful to software quality. Previous work by Bird et al. allayed that concern but a recent paper by Posnett et al. suggests that the Bird result was biased by the kinds of files it explored. Hence, this paper rechecks that result and finds significant differences in Microsoft products (Office 2010) between software built by distributed or collocated teams. At first glance, this recheck calls into question the widespread practice of distributed development. Our third rule is to “reflect” on results to avoid confusing practitioners with an arcane mathematical analysis. For example, on reflection, we found that the effect size of the differences seen in the collocated and distributed software was so small that it need not concern industrial practitioners. Our conclusion is that at least for Microsoft products, distributed development is not considered harmful.
The design of bug fixes
When software engineers fix bugs, they may have several options as to how to fix those bugs. Which fix they choose has many implications, both for practitioners and researchers: What is the risk of introducing other bugs during the fix? Is the bug fix in the same code that caused the bug? Is the change fixing the cause or just covering a symptom? In this paper, we investigate alternative fixes to bugs and present an empirical study of how engineers make design choices about how to fix bugs. Based on qualitative interviews with 40 engineers working on a variety of products, data from 6 bug triage meetings, and a survey filled out by 326 engineers, we found a number of factors, many of them non-technical, that influence how bugs are fixed, such as how close to release the software is. We also discuss several implications for research and practice, including ways to make bug prediction and localization more accurate.
C4 plant biology C4 plant biology, 285-312, 1999
Due to many issues related to long-term carbon dynamics, an improved understanding of the biology of C4 photosynthesis is required by more than the traditional audience of crop scientists, plant physiologists, and plant ecologists. This work synthesizes the latest developments in C4 biochemistry, physiology, systematics, and ecology. The book concludes with chapters discussing the role of C4 plants in the future development of the biosphere, particularly their interactive effects on soil, hydrological, and atmospheric processes.
Climate Change 2007: the physical science basis. Contribution of working group I to the fourth assessment report of the Intergovernmental Panel on Climate Change...
The Working Group I contribution to the IPCC Fourth Assessment Report describes progress in understanding of the human and natural drivers of climate change,1 observed climate change, climate processes and attribution, and estimates of projected future climate change. It builds upon past IPCC assessments and incorporates new fi ndings from the past six years of research. Scientifi c progress since the Third Assessment Report (TAR) is based upon large amounts of new and more comprehensive data, more sophisticated analyses of data, improvements in understanding of processes and their simulation in models and more extensive exploration of uncertainty ranges. The basis for substantive paragraphs in this Summary for Policymakers can be found in the chapter sections specifi ed in curly brackets.
Phylogenetic relationship and molecular taxonomy of African grasses of the genus Panicum inferred from four chloroplast DNA-barcodes and nuclear gene sequenc...
The genus Panicum s.l. comprises about 450 grass species in which the C4 and the C3 metabolic pathways of photosynthesis are realized. In the West African savannah, Panicum spp. and closely related taxa dominate the landscape, with species differentially adapted to drought conditions. We obtained four chloroplast DNA barcode sequences, rbcL, matK, ndhF and trnH-psbA intergenic region, for nine Panicum spp. with a focus on West African species, and we performed maximum likelihood analysis to infer their phylogenetic relationship. Furthermore the phylogenetic placement of five newly sequenced taxa was achieved using a published phylogeny of more than 300 Panicoids based on ndhF sequences. The comparison of the resulting phylogenetic tree constructed from a combination of all four barcode sequences with the one based on rbcL and matK showed that the latter combination of the two, is sufficient for the analysis. A tree constructed from amino acid sequences derived from isolated cDNAs of the nucleus-encoded phosphoenolpyruvate carboxylase displayed a similar topology. All ppc-sequences could be annotated to either ppc-B2 or ppc-aR. Moreover the inclusion of the West African Panicum species in an extensive dataset of Panicoids supports the proposition that within the subtribe Panicinae only the NAD-malic enzyme type of C4 photosynthesis is present.
Mastering the art of war: how patterns of gameplay influence skill in Halo
How do video game skills develop, and what sets the top players apart? We study this question of skill through a rating generated from repeated multiplayer matches called TrueSkill. Using these ratings from 7 months of games from over 3 million players, we look at how play intensity, breaks in play, skill change over time, and other games affect skill. These analyzed factors are then combined to model future skill and games played; the results show that skill change in early matches is a useful metric for modeling future skill, while play intensity explains eventual games played. The best players in the 7-month period, who we call "Master Blasters", have varied skill patterns that often run counter to the trends we see for typical players. The data analysis is supplemented with a 70 person survey to explore how players' self-perceptions compare to the gameplay data; most survey responses align well with the data and provide insight into player beliefs and motivation. Finally, we wrap up with a discussion about hiding skill information from players, and implications for game designers.
Appendix to the Design of Bug Fixes
This technical report contains supplemental information to the paper “The Design of Bug Fixes”, published in the Proceedings of the 35th International Conference on Software Engineering (ICSE 2013), San Francisco, CA, USA, May 2013. The technical report contains the interview guide, list of codes used for the analysis of the interviews with example quotations, as well as the full text of a follow-up survey.
Predicting method crashes with bytecode operations
Software monitoring systems have high performance overhead because they typically monitor all processes of the running program. For example, to capture and replay crashes, most current systems monitor all methods; thus yielding a significant performance overhead. Lowering the number of methods being monitored to a smaller subset can dramatically reduce this overhead. We present an approach that can help arrive at such a subset by reliably identifying methods that are the most likely candidates to crash in a future execution of the software. Our approach involves learning patterns from features of methods that previously crashed to classify new methods as crash-prone or non-crash-prone. An evaluation of our approach on two large open source projects, ASPECTJ and ECLIPSE, shows that we can correctly classify crash-prone methods with an accuracy of 80--86%. Notably, we found that the classification models can also be used for cross-project prediction with virtually no loss in classification accuracy. In a further experiment, we demonstrate how a monitoring tool, RECRASH could take advantage of only monitoring crash-prone methods and thereby, reduce its performance overhead and maintain its ability to perform its intended tasks.
Dwelling in Software: Aspects of the felt-life of engineers in large software projects
The organizational and social aspects of software engineering (SE) are now increasingly well investigated. This paper proposes that there are a number of approaches taken in research that can be distinguished not by their method or topic but by the different views they construct of the human agent acting in SE. These views have implications for the pragmatic outcome of the research, such as whether systems design suggestions are made, proposals for the development of practical reasoning tools or the effect of Social Network Systems on engineer’s sociability. This paper suggests that these studies tend to underemphasize the felt-life of engineers, a felt-life that is profoundly emotional though played in reference to ideas of moral propriety and ethics. This paper will present a study of this felt-life, suggesting it consists of a form of digital dwelling. The perspective this view affords are contrasted with process and ‘scientific’ approaches to the human agent in SE, and with the more humanistic studies of SE reasoning common in CSCW.
Revival Actions in a Shooter Game
In this paper we present an analysis of revival actions in a third person shooter video game. We observe that players who revived their teammates played significantly more sessions. They appear also to be more skilled and successful in the game in terms of kills, wins, and deaths. We then discuss how to extend our analysis methodology to other types of social play as well as other games.
Assessing the value of branches with what-if analysis
Branches within source code management systems (SCMs) allow a software project to divide work among its teams for concurrent development by isolating changes. However, this benefit comes with several costs: increased time required for changes to move through the system and pain and error potential when integrating changes across branches. In this paper, we present the results of a survey to characterize how developers use branches in a large industrial project and common problems that they face. One of the major problems mentioned was the long delay that it takes changes to move from one team to another, which is often caused by having too many branches (branchmania). To monitor branch health, we introduce a novel what-if analysis to assess alternative branch structures with respect to two properties, isolation and liveness. We demonstrate with several scenarios how our what-if analysis can support branch decisions. By removing high-cost-low-benefit branches in Windows based on our what-if analysis, changes would each have saved 8.9 days of delay and only introduced 0.04 additional conflicts on average.
A field study of refactoring challenges and benefits
It is widely believed that refactoring improves software quality and developer productivity. However, few empirical studies quantitatively assess refactoring benefits or investigate developers' perception towards these benefits. This paper presents a field study of refactoring benefits and challenges at Microsoft through three complementary study methods: a survey, semi-structured interviews with professional software engineers, and quantitative analysis of version history data. Our survey finds that the refactoring definition in practice is not confined to a rigorous definition of semantics-preserving code transformations and that developers perceive that refactoring involves substantial cost and risks. We also report on interviews with a designated refactoring team that has led a multi-year, centralized effort on refactoring Windows. The quantitative analysis of Windows 7 version history finds that the binary modules refactored by this team experienced significant reduction in the number of inter-module dependencies and post-release defects, indicating a visible benefit of refactoring.
Relating requirements to implementation via topic analysis: Do topics extracted from requirements make sense to managers and developers?
Large organizations like Microsoft tend to rely on formal requirements documentation in order to specify and design the software products that they develop. These documents are meant to be tightly coupled with the actual implementation of the features they describe. In this paper we evaluate the value of high-level topic-based requirements traceability in the version control system, using Latent Dirichlet Allocation (LDA). We evaluate LDA topics on practitioners and check if the topics and trends extracted matches the perception that Program Managers and Developers have about the effort put into addressing certain topics. We found that effort extracted from version control that was relevant to a topic often matched the perception of the managers and developers of what occurred at the time. Furthermore we found evidence that many of the identified topics made sense to practitioners and matched their perception of what occurred. But for some topics, we found that practitioners had difficulty interpreting and labelling them. In summary, we investigate the high-level traceability of requirements topics to version control commits via topic analysis and validate with the actual stakeholders the relevance of these topics extracted from requirements.
The effect of branching strategies on software quality
Branching plays a major role in the development process of large software. Branches provide isolation so that multiple pieces of the software system can be modified in parallel without affecting each other during times of instability. However, branching has its own issues. The need to move code across branches introduces addition-al overhead and branch use can lead to integration failures due to conflicts or unseen dependencies. Although branches are used ex-tensively in commercial and open source development projects, the effects that different branch strategies have on software quality are not yet well understood. In this paper, we present the first empirical study that evaluates and quantifies the relationship between soft-ware quality and various aspects of the branch structure used in a software project. We examine Windows Vista and Windows 7 and compare components that have different branch characteristics to quantify differences in quality. We also examine the effectiveness of two branching strategies -- branching according to the software architecture versus branching according to organizational structure. We find that, indeed, branching does have an effect on software quality and that misalignment of branching structure and organiza-tional structure is associated with higher post-release failure rates.
Representativeness in software engineering research
One of the goals of software engineering research is to achieve generality: Are the phenomena found in a few projects reflective of what goes on in others? Will a technique benefit more than just the projects it is evaluated on? The discipline of our community has gained rigor over the past twenty years and is now attempting to achieve generality through evaluation and study of an increasing number of software projects (sometime hundreds!). However, quantity is not the only important component. Selecting projects that are representative of a larger body of software of interest is just as critical. Little attention has been paid to selecting projects in such a way that generality and representativeness is maximized or even quantitatively characterized and reported. In this paper, we present a general technique for quantifying how representative a sample of software projects is of a population across many dimensions. We also present a greedy algorithm for choosing a maximally representative sample. We demonstrate our technique on research presented over the past two years at ICSE and FSE with respect to a population of 20,000 active open source projects. Finally, we propose methods of reporting objective measures of representativeness in research.
BCL2-associated athanogene 3 (BAG3) knock-down revealed a heart failure phenotype and disturbed sarcomerogenesis in zebrafish
Bcl-2–associated athanogene 3 (BAG3) is an evolutionarily conserved protein expressed at high levels in the heart and the vasculature and in many cancers. While altered BAG3 expression has been associated with cardiac dysfunction, its role in ischemia/reperfusion (I/R) is unknown. To test the hypothesis that BAG3 protects the heart from reperfusion injury, in vivo cardiac function was measured in hearts infected with either recombinant adeno-associated virus serotype 9–expressing (rAAV9-expressing) BAG3 or GFP and subjected to I/R. To elucidate molecular mechanisms by which BAG3 protects against I/R injury, neonatal mouse ventricular cardiomyocytes (NMVCs) in which BAG3 levels were modified by adenovirus expressing (Ad-expressing) BAG3 or siBAG3 were exposed to hypoxia/reoxygenation (H/R). H/R significantly reduced NMVC BAG3 levels, which were associated with enhanced expression of apoptosis markers, decreased expression of autophagy markers, and reduced autophagy flux. The deleterious effects of H/R on apoptosis and autophagy were recapitulated by knockdown of BAG3 with Ad-siBAG3 and were rescued by Ad-BAG3. In vivo, treatment of mice with rAAV9-BAG3 prior to I/R significantly decreased infarct size and improved left ventricular function when compared with mice receiving rAAV9-GFP and improved markers of autophagy and apoptosis. These findings suggest that BAG3 may provide a therapeutic target in patients undergoing reperfusion after myocardial infarction.
Introduction to the Special Issue on Mining Software Repositories in 2010
An abstract is not available.
Welcome to RSSE 2012
Recommendation systems for software engineering (RSSEs) are tools that help developers and managers to better cope with the huge amount of information faced in today's software projects. They provide developers with information to guide them in a number of activities (e.g., software navigation, debugging, refactoring), or to alert them of potential issues (e.g., conflicting changes, failure-inducing changes, duplicated functionality). Similarly, they provide managers with information that is relevant to make a certain decision (e.g., bug distribution when allocating resources).
Goldfish bowl panel: Software development analytics
Gaming companies now routinely apply data mining to their user data in order to plan the next release of their software. We predict that such software development analytics will become commonplace, in the near future. For example, as large software systems migrate to the cloud, they are divided and sold as dozens of smaller apps; when shopping inside the cloud, users are free to mix and match their apps from multiple vendors (e.g. Google Docsâ word processor with Zohoâs slide manager); to extend, or even retain, market share cloud vendors must mine their user data in order to understand what features best attract their clients. This panel will address the open issues with analytics. Issues addressed will include the following. What is the potential for software development analytics? What are the strengths and weaknesses of the current generation of analytics tools? How best can we mature those tools?
Characterizing and predicting which bugs get reopened
Fixing bugs is an important part of the software development process. An underlying aspect is the effectiveness of fixes: if a fair number of fixed bugs are reopened, it could indicate instability in the software system. To the best of our knowledge there has been on little prior work on understanding the dynamics of bug reopens. Towards that end, in this paper, we characterize when bug reports are reopened by using the Microsoft Windows operating system project as an empirical case study. Our analysis is based on a mixed-methods approach. First, we categorize the primary reasons for reopens based on a survey of 358 Microsoft employees. We then reinforce these results with a large-scale quantitative study of Windows bug reports, focusing on factors related to bug report edits and relationships between people involved in handling the bug. Finally, we build statistical models to describe the impact of various metrics on reopening bugs ranging from the reputation of the opener to how the bug was found.
Information needs for software development analytics
Software development is a data rich activity with many sophisticated metrics. Yet engineers often lack the tools and techniques necessary to leverage these potentially powerful information resources toward decision making. In this paper, we present the data and analysis needs of professional software engineers, which we identified among 110 developers and managers in a survey. We asked about their decision making process, their needs for artifacts and indicators, and scenarios in which they would use analytics. The survey responses lead us to propose several guidelines for analytics tools in software development including: Engineers do not necessarily have much expertise in data analysis; thus tools should be easy to use, fast, and produce concise output. Engineers have diverse analysis needs and consider most indicators to be important; thus tools should at the same time support many different types of artifacts and many indicators. In addition, engineers want to drill down into data based on time, organizational structure, and system architecture.
An empirical investigation of a genetic algorithm for developer's assignment to bugs
Software development teams consist of developers with varying expertise and levels of productivity. With reported productivity variation of up to 1:20, the quality of assignment of developers to tasks can have a huge impact on project performance. Developers are characterized according to a defined core set of technical competence areas. The objective is to find a feasible assignment, which minimizes the total time needed to fix all given bugs. In this paper, the modeling of the developer’s assignment to bugs is given. Subsequently, a genetic algorithm called GA@DAB (Genetic Algorithm for Developer’s Assignment to Bugs) is proposed and empirically evaluated. The performance of GA@DAB was evaluated for 2040 bugs of 19 open-source milestone projects from the Eclipse platform. As part of that, a comparative analysis was done with a previously developed approach using K-Greedy search. Our results and analysis shows that GA@DAB performs statistically significantly better than K-greedy search in 17 out of 19 projects. Overall, the results support the argument of the applicability of customized genetic search techniques in the context of developer-to-bug assignments.
Data-driven games user research
Gaming companies now routinely apply data mining to their user data in order to provide a better user experience and to plan the next release of their software. In this position paper we will present some of the games user research activities at Microsoft that leverage such automatically collected data. We will show how this data can lead to insights about game usage and player progression and discuss our current plans for future work.
Appendix to a field study of refactoring rationale, benefits, and challenges at microsoft
In order to understand refactoring practices at Microsoft, we sent a survey to 1290 engineers, whose change comments include a keyword, “refactoring” in the last 2 years for five Microsoft products (Windows Phone, Exchange, Windows, Office Communication and Services, and Office). We purposely targeted the engineers who are already familiar with the term, “refactoring,” because our goal is to understand their own refactoring definition and their perception about the value of refactoring. This document contains the complete set of survey questions. The results of the survey will be published in a separate paper.
Collaborative software development in ten years: Diversity, tools, and remix culture
Over the next ten years, collaboration in software engineering will change in a number of ways and research will need to shift its focus to enable and enhance such collaboration. Specifically, we claim that software in the small will become more popular and even large software will be built by fewer people due to better tools. For large projects, research will need to address the collaboration needs of project members other than just developers, including quality assurance engineers, build engineers, architects, and operations managers. Finally, code reuse and sharing will change as a result of a growing software remix culture, leading to more loosely coupled and indirect collaboration.
The inductive software engineering manifesto: principles for industrial data mining
The practices of industrial and academic data mining are very different. These differences have significant implications for (a) how we manage industrial data mining projects; (b) the direction of academic studies in data mining; and (c) training programs for engineers who seek to use data miners in an industrial setting.
Local vs. global models for effort estimation and defect prediction
Data miners can infer rules showing how to improve either (a) the effort estimates of a project or (b) the defect predictions of a software module. Such studies often exhibit conclusion instability regarding what is the most effective action for different projects or modules.
Detecting energy patterns in software development
With the advent of increased computing on mobile devices such as phones and tablets, it has become crucial to pay attention to the energy consumption of mobile applications. The software engineering field is now faced with a whole new spectrum of energy-related challenges, ranging from power budgeting to testing and debugging the energy consumption. To the best of our knowledge there has been little work on the analysis of energy patterns. In this paper, we present our work for the Windows Phone platform. We first describe the data that is collected for testing (power traces and execution logs). We then present several approaches for describing power consumption and detecting anomalous energy patterns and potential energy defects. Finally, we describe prediction models to estimate the overall energy consumption based on usage of individual modules. This allows assessing the individual impact of modules on the overall energy consumption and supports overall energy planning.
Non-rigid image registration of brain magnetic resonance images using graph-cuts
We present a graph-cuts based method for non-rigid medical image registration on brain magnetic resonance images. In this paper, the non-rigid medical image registration problem is reformulated as a discrete labeling problem. Based on a voxel-to-voxel intensity similarity measure, each voxel in the source image is assigned a displacement label, which represents a displacement vector indicating which position in the floating image it is spatially corresponding to. In the proposed method, a smoothness constraint based on the first derivative is used to penalize sharp changes in the adjacent displacement labels across voxels. The image registration problem is therefore modeled by two energy terms based on intensity similarity and smoothness of the displacement field. These energy terms are submodular and can be optimized by using the graph-cuts method via ‐ , which is a powerful combinatorial optimization tool and capable of yielding either a global minimum or a local minimum in a strong sense. Using the realistic brain phantoms obtained from the Simulated Brain Database, we compare the registration results of the proposed method with two state-of-the-art medical image registration approaches: free-form deformation based method and demons method. In addition, the registration results are also compared with that of the linear programming based image registration method. It is found that the proposed method is more robust against different challenging non-rigid registration cases with consistently higher registration accuracy than those three methods, and gives realistic recovered deformation fields.
An integration resolution algorithm for mining multiple branches in version control systems
The high cost of software maintenance necessitates methods to improve the efficiency of the maintenance process. Such methods typically need a vast amount of knowledge about a system, which is often mined from software repositories. Collecting this data becomes a challenge if the system was developed using multiple code branches. In this paper we present an integration resolution algorithm that facilitates data collection across multiple code branches. The algorithm tracks code integrations across different branches and associates code changes in the main development branch with corresponding changes in other branches. We provide evidence for the practical relevance of this algorithm during the development of the Windows Vista Service Pack 2.
Failure is a four-letter word: a parody in empirical research
Background: The past years have seen a surge of techniques predicting failure-prone locations based on more or less complex metrics. Few of these metrics are actionable, though. Aims: This paper explores a simple, easy-to-implement method to predict and avoid failures in software systems. The IROP method links elementary source code features to known software failures in a lightweight, easy-to-implement fashion. Method: We sampled the Eclipse data set mapping defects to files in three Eclipse releases. We used logistic regression to associate programmer actions with defects, tested the predictive power of the resulting classifier in terms of precision and recall, and isolated the most defect-prone actions. We also collected initial feedback on possible remedies. Results: In our sample set, IROP correctly predicted up to 74% of the failure-prone modules, which is on par with the most elaborate predictors available. We isolated a set of four easy-to-remember recommendations, telling programmers precisely what to do to avoid errors. Initial feedback from developers suggests that these recommendations are straightforward to follow in practice. Conclusions: With the abundance of software development data, even the simplest methods can produce "actionable" results.
An explanatory analysis on eclipse beta-release bugs through in-process metrics
Failures after the release of software products are expensive and time-consuming to fix. Each of these failures has different reasons pointing into different portions of code. We conduct a retrospective analysis on bugs reported after beta release of Eclipse versions. Our objective is to investigate what went wrong during the development process. We identify six in-process metrics that have explanatory effects on beta-release bugs. We conduct statistical analyses to check relationships between files and metrics. Our results show that files with beta-release bugs have different characteristics in terms of in-process metrics. Those bugs are specifically concentrated on Eclipse files with little activity: few edits by few committers. We suggest that in-process metrics should be investigated individually to identify beta-release bugs. Companies may benefit from such a retrospective analysis to understand characteristics of failures. Corrective actions can be taken earlier in the process to avoid similar failures in future releases.
Crash graphs: An aggregated view of multiple crashes to improve crash triage
Crash reporting systems play an important role in the overall reliability and dependability of the system helping in identifying and debugging crashes in software systems deployed in the field. In Microsoft for example, the Windows Error Reporting (WER) system receives crash data from users, classifies them, and presents crash information for developers to fix crashes. However, most crash reporting systems deal with crashes individually; they compare crashes individually to classify them, which may cause misclassification. Developers need to download multiple crash data files for debugging, which requires non-trivial effort. In this paper, we propose an approach based on crash graphs, which are an aggregated view of multiple crashes. Our experience with crash graphs indicates that it reduces misclassification and helps identify fixable crashes in advance.
Dynamine: Finding usage patterns and their violations by mining software repositories
In this chapter we propose an automatic way to extract likely error patterns by mining software revision histories. Moreover, in order to ensure that all the errors we find are relatively easy to confirm and fix, we pay particular attention in our experiments to errors that can be fixed with a one-line change. It is worth pointing out that many well-known error patterns such as memory leaks, double-free’s, mismatched locks, open and close operations on operating system resources, buffer overruns, and format string errors can often be addressed with a one-line fix. Looking at incremental changes between revisions as opposed to complete snapshots of the source allows us to better focus our mining strategy and obtain more precise results. Our approach uses revision history information to infer likely error patterns. We then experimentally evaluate the patterns we extracted by checking for them dynamically.
A theory of branches as goals and virtual teams
A common method of managing the complexity of both technical and organizational relationships in a large software project is to use branches within the source code management system to partition the work into teams and tasks. We claim that the files modified on a branch are changed together in a cohesive way to accomplish some task such as adding a feature, fixing a related set of bugs, or implementing a subsystem, which we collectively refer to as the goal of the branch. Further, the developers that work on a branch represent a virtual team. In this paper, we develop a theory of the relationship between goals and virtual teams on different branches. Due to expertise, ownership, and awareness concerns, we expect that if two branches have similar goals, they will also have similar virtual teams or be at risk for communication and coordination breakdowns with the accompanying negative effects. In contrast, we do not expect the converse to always be true. In the first step towards an actionable result, we have evaluated this theory empirically on two releases of the Windows operating system and found support in both.
Comparing fine-grained source code changes and code churn for bug prediction
A significant amount of research effort has been dedicated to learning prediction models that allow project managers to efficiently allocate resources to those parts of a software system that most likely are bug-prone and therefore critical. Prominent measures for building bug prediction models are product measures, e.g., complexity or process measures, such as code churn. Code churn in terms of lines modified (LM) and past changes turned out to be significant indicators of bugs. However, these measures are rather imprecise and do not reflect all the detailed changes of particular source code entities during maintenance activities. In this paper, we explore the advantage of using fine-grained source code changes (SCC) for bug prediction. SCC captures the exact code changes and their semantics down to statement level. We present a series of experiments using different machine learning algorithms with a dataset from the Eclipse platform to empirically evaluate the performance of SCC and LM. The results show that SCC outperforms LM for learning bug prediction models.
Not my bug! and other reasons for software bug report reassignments
Bug reporting/fixing is an important social part of the soft-ware development process. The bug-fixing process inher-ently has strong inter-personal dynamics at play, especially in how to find the optimal person to handle a bug report. Bug report reassignments, which are a common part of the bug-fixing process, have rarely been studied. In this paper, we present a large-scale quantitative and qualitative analysis of the bug reassignment process in the Microsoft Windows Vista operating system project. We quantify social interactions in terms of both useful and harmful reassignments. For instance, we found that reassignments are useful to determine the best person to fix a bug, contrary to the popular opinion that reassignments are always harmful. We categorized five primary reasons for reassignments: finding the root cause, determining ownership, poor bug report quality, hard to determine proper fix, and workload balancing. We then use these findings to make recommendations for the design of more socially-aware bug tracking systems that can overcome some of the inefficiencies we observed in our study.
Empirical software engineering at microsoft research
We describe the activities of the Empirical Software Engi-neering (ESE) group at Microsoft Research. We highlight our research themes and activities using examples from our research on socio technical congruence, bug reporting and triaging, and data-driven software engineering to illustrate our relationship to the CSCW community. We highlight our unique ability to leverage industrial data and developers and the ability to make near term impact on Microsoft via the results of our studies. We also present the collaborations our group has with academic researchers.
An empirical study of the factors relating field failures and dependencies
Changing source code in large software systems is complex and requires a good understanding of dependencies between software components. Modification to components with little regard to dependencies may increase have an adverse impact on the quality of the latter, i.e., increase their risk to fail. We conduct an empirical study to understand the relationship between the quality of components and the characteristics of their dependencies such as their frequency of change, their complexity, number of past failures and like. Our study has been conducted on two large software systems: Microsoft VISTA and ECLIPSE. Our results show that components that have outgoing dependencies to components with higher objectoriented complexity tend to have fewer field failures for VISTA, but the opposite relation holds for ECLIPSE. Likewise, other notable observations have been made through our study that (a) confirm that certain characteristics of components increase the risk of their dependencies to fail and (b) some of the characteristics are projectspecific while some were also found to be common. We expect that such results can leveraged to provide new directions for research in defect prediction, test prioritization and related research fields that utilize code dependencies in their empirical analysis. Additionally, these results provide insights to engineers on the potential reliability impacts of new component dependencies based upon the characteristics of the component.
WhoselsThat: finding software engineers with codebook
In this demo, we describe WhoseIsThat, a social search portal which we built using the Codebook framework. We improve the search experience in two ways: first, we search across multiple software repositories at once with a single query; second, we return not just a list of artifacts in the results, but also engineers.
Analytics for software development
Despite large volumes of data and many types of metrics, software projects continue to be difficult to predict and risky to conduct. In this paper we propose software analytics which holds out the promise of helping the managers of software projects turn their plentiful information resources, produced readily by current tools, into insights they can act on. We discuss how analytics works, why it's a good fit for software engineering, and the research problems that must be overcome in order to realize its promise.
Social media for software engineering
Social media has changed the way that people collaborate and share information. In this paper, we highlight its impact for enabling new ways for software teams to form and work together. Individuals will self-organize within and across organizational boundaries. Grassroots software development communities will emerge centered around new technologies, common processes and attractive target markets. Companies consisting of lone individuals will able to leverage social media to conceive of, design, develop, and deploy successful and profitable product lines. A challenge for researchers who are interested in studying, influencing, and supporting this shift in software teaming is to make sure that their research methods protect the privacy and reputation of their stakeholders.
Security trend analysis with cve topic models
We study the vulnerability reports in the Common Vulnerability and Exposures (CVE) database by using topic models on their description texts to find prevalent vulnerability types and new trends semi-automatically. In our study of the 39,393 unique CVEs until the end of 2009, we identify the following trends, given here in the form of a weather forecast: PHP: declining, with occasional SQL injection. Buffer Overflows: flattening out after decline. Format Strings: in steep decline. SQL Injection and XSS: remaining strong, and rising. Cross-Site Request Forgery: a sleeping giant perhaps, stirring. Application Servers: rising steeply.
Change bursts as defect predictors
In software development, every change induces a risk. What happens if code changes again and again in some period of time? In an empirical study on Windows Vista, we found that the features of such change bursts have the highest predictive power for defect-prone components. With precision and recall values well above 90%, change bursts significantly improve upon earlier predictors such as complexity metrics, code churn, or organizational structure. As they only rely on version history and a controlled change process, change bursts are straight-forward to detect and deploy.
The art of collecting bug reports
Kids love bugs, and some kids even collect bugs and keep them in precious “kill jars.” Over a period of time, bug collectors can amass a large number of different species of bugs. Some kids study the bugs they collected and label them based on such characteristics as shape, size, color, number of legs, whether it can fly, and so on. The bugs may be valued differently depending upon how rare they are or how difficult they are to catch. The collection may have some duplicate bugs. But duplicates are rarely identical, as characteristics such as appearance and size can differ widely. But we software developers do not like bugs. We hope to have none in our software, and when they are found, we squash them! Unfortunately, squashing bugs, or more politely, responding to software change requests, is rarely easy. Developers have to study the information about the bug in detail, conduct a thorough investigation on how to resolve the bug, examine its side effects, and eventually decide on and take a course of action. This is a difficult task because, like earthly bugs, software bugs differ widely. Often software bugs that are collected in bug databases of projects are studied in isolation because they are different from the other bugs in their effects on the software system, their cause and location, and their severity. Over time, a project will accumulate duplicate bugs, just as a live-bug collector may have multiple bugs of the same species. And finally, nearly every project knows about more bug reports than it can fix, just as there are too many live bugs to be collected by a single person. So the study of software bugs is a valuable pastime. The starting point for the study of software bugs is the information filed in bug reports by those who report them. In this chapter, we discuss the characteristics of good bug reports and implications for the practice of collecting bug reports.
What makes a good bug report?
n software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are, at the same time, most difficult to provide for users. Such insight is helpful for designing new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. The participants of our survey also provided 175 comments on hurdles in reporting and resolving bugs. Based on these comments, we discuss several recommendations for better bug tracking systems, which should focus on engaging bug reporters, better tool support, and improved handling of bug duplicates.
Analytics for Software Development
Despite large volumes of data and many types of metrics, software projects continue to be difficult to predict and risky to conduct. In this paper we propose software analytics which holds out the promise of helping the managers of software projects turn their plentiful information resources, produced readily by current tools, into insights they can act on. We discuss how analytics works, why it’s a good fit for software engineering, and the research problems that must be overcome in order to realize its promise.
Recommendation systems for software engineering
Software development can be challenging because of the large information spaces that developers must navigate. Without assistance, developers can become bogged down and spend a disproportionate amount of their time seeking information at the expense of other value-producing tasks. Recommendation systems for software engineering (RSSEs) are software tools that can assist developers with a wide range of activities, from reusing code to writing effective bug reports. The authors provide an overview of recommendation systems for software engineering: what they are, what they can do for developers, and what they might do in the future.
Keeping up with your friends: function Foo, library Bar. DLL, and work item 24
Development teams who work with others need to be aware of what everyone is doing in order to manage the risk of taking on dependencies. Using newsfeeds of software development activities mined from software repositories, teams can find relevant information to help them make well-informed decisions that affect the success of their endeavors. In this paper, we describe the architecture of a newsfeed system that we are currently building on top of the Codebook software repository mining platform. We discuss the design, construction and aggregation of newsfeeds, and include other important aspects such as summarization, filtering, context, and privacy.
Welcome from the chairs
Welcome to MSR 2010, the Seventh IEEE Working Conference on Mining Software Repositories, held May 2–3 in Cape Town, South Africa, and co-located with the 2010 ACM/IEEE International Conference on Software Engineering (ICSE 2010).
Characterizing and predicting which bugs get fixed: an empirical study of Microsoft Windows
We performed an empirical study to characterize factors that affect which bugs get fixed in Windows Vista and Windows 7, focusing on factors related to bug report edits and relationships between people involved in handling the bug. We found that bugs reported by people with better reputations were more likely to get fixed, as were bugs handled by people on the same team and working in geographical proximity. We reinforce these quantitative results with survey feedback from 358 Microsoft employees who were involved in Windows bugs. Survey respondents also mentioned additional qualitative influences on bug fixing, such as the importance of seniority and interpersonal skills of the bug reporter. Informed by these findings, we built a statistical model to predict the probability that a new bug will be fixed (the first known one, to the best of our knowledge). We trained it on Windows Vista bugs and got a precision of 68% and recall of 64% when predicting Windows 7 bug fixes. Engineers could use such a model to prioritize bugs during triage, to estimate developer workloads, and to decide which bugs should be closed or migrated to future product versions.
Rsse 2010: Second international workshop on recommendation systems for software engineering
The goal of this one-day workshop is to bring together researchers and practitioners with interest and experience in the theory, elaboration, and evaluation of concepts, techniques, and tools for providing recommendations to developers, managers, and other stakeholders involved in software engineering tasks.
Codebook: discovering and exploiting relationships in software repositories
Large-scale software engineering requires communication and collaboration to successfully build and ship products. We conducted a survey with Microsoft engineers on inter-team coordination and found that the most impactful problems concerned finding and keeping track of other engineers. Since engineers are connected by their shared work, a tool that discovers connections in their work-related repositories can help. Here we describe the Codebook framework for mining software repositories. It is flexible enough to address all of the problems identified by our survey with a single data structure (graph of people and artifacts) and a single algorithm (regular language reachability). Codebook handles a larger variety of problems than prior work, analyzes more kinds of work artifacts, and can be customized by and for end-users. To evaluate our framework's flexibility, we built two applications, Hoozizat and Deep Intellisense. We evaluated these applications with engineers to show effectiveness in addressing multiple inter-team coordination problems.
Searching for a needle in a haystack: Predicting security vulnerabilities for windows vista
Many factors are believed to increase the vulnerability of software system; for example, the more widely deployed or popular is a software system the more likely it is to be attacked. Early identification of defects has been a widely investigated topic in software engineering research. Early identification of software vulnerabilities can help mitigate these attacks to a large degree by focusing better security verification efforts in these components. Predicting vulnerabilities is complicated by the fact that vulnerabilities are, most often, few in number and introduce significant bias by creating a sparse dataset in the population. As a result, vulnerability prediction can be thought of us preverbally “searching for a needle in a haystack.” In this paper, we present a large-scale empirical study on Windows Vista, where we empirically evaluate the efficacy of classical metrics like complexity, churn, coverage, dependency measures, and organizational structure of the company to predict vulnerabilities and assess how well these software measures correlate with vulnerabilities. We observed in our experiments that classical software measures predict vulnerabilities with a high precision but low recall values. The actual dependencies, however, predict vulnerabilities with a lower precision but substantially higher recall.
Information needs in bug reports: improving cooperation between developers and users
For many software projects, bug tracking systems play a central role in supporting collaboration between the developers and the users of the software. To better understand this collaboration and how tool support can be improved, we have quantitatively and qualitatively analysed the questions asked in a sample of 600 bug reports from the MOZILLA and ECLIPSE projects. We categorised the questions and analysed response rates and times by category and project. Our results show that the role of users goes beyond simply reporting bugs: their active and ongoing participation is important for making progress on the bugs they report. Based on the results, we suggest four ways in which bug tracking systems can be improved.
Investigating information needs to improve cooperation between developers and bug reporters
For many software projects, bug tracking systems play a central role in supporting collaboration between the developers and the users of the software. To better understand this collaboration and how tool support can be improved, we have quantitatively and qualitatively analysed the questions asked in a sample of 600 bug reports from the MOZILLA and ECLIPSE projects. We categorised the questions and analysed response rates and times by category and project. Our results show that the role of users goes beyond simply reporting bugs: their active and ongoing participation is important for making progress on the bugs they report. Based on the results, we suggest four ways in which bug tracking systems can be improved. Copyright 2010 ACM.
Beautiful Bug Reports
No abstract available.
Optimized assignment of developers for fixing bugs an initial evaluation for eclipse projects
Decisions on “Who should fix this bug” have substantial impact on the duration of the process and its results. In this paper, optimized strategies for the assignment of the “right” developers for doing the “right” task are studied and the results are compared to manual (called ad hoc) assignment. The quality of assignment is measured by the match between requested (from bugs) and available (from developers) competence profile. Different variants of Greedy search with varying parameter of look-ahead time are studied. The quality of the results has been evaluated for nine milestones of the open source Eclipse JDT project. The optimized strategies with largest look ahead time are demonstrated to be substantially better than the ad hoc solutions in terms of the quality of the assignment and the number of bugs which can be fixed within the given time interval.
Predicting defects with program dependencies
Software development is a complex and error-prone task. An important factor during the development of complex systems is the understanding of the dependencies that exist between different pieces of the code. In this paper, we show that for Windows Server 2003 dependency data can predict the defect-proneness of software elements. Since most dependencies of a component are already known in the design phase, our prediction models can support design decisions.
Appendix to Information Needs in Bug Reports: Improving Cooperation Between Developers and Users
This technical report contains all data that is needed to replicate the paper “Information Needs in Bug Reports: Improving Cooperation Between Developers and Users” to be published at CSCW 2010 in Savannah, Georgia, USA. The accompanying zip file contains bug reports, cards with categorization, and R scripts that were used in the study.
Changes and bugs—Mining and predicting development activities
Software development results in a huge amount of data: changes to source code are recorded in version archives, bugs are reported to issue tracking systems, and communications are archived in e-mails and newsgroups. We present techniques for mining version archives and bug databases to understand and support software development. First, we introduce the concept of co-addition of method calls, which we use to identify patterns that describe how methods should be called. We use dynamic analysis to validate these patterns and identify violations. The co-addition of method calls can also detect cross-cutting changes, which are an indicator for concerns that could have been realized as aspects in aspect-oriented programming. Second, we present techniques to build models that can successfully predict the most defect-prone parts of large-scale industrial software, in our experiments Windows Server 2003. This helps managers to allocate resources for quality assurance to those parts of a system that are expected to have most defects. The proposed measures on dependency graphs outperformed traditional complexity metrics. In addition, we found empirical evidence for a domino effect, i.e., depending on defect-prone binaries increases the chances of having defects.
Expert recommendation with usage expertise
Global and distributed software development increases the need to find and connect developers with relevant expertise. Existing recommendation systems typically model expertise based on file changes (implementation expertise). While these approaches have shown success, they require a substantial recorded history of development for a project. Previously, we have proposed the concept of usage expertise, i.e., expertise manifested through the act of calling (using) a method. In this paper, we assess the viability of this concept by evaluating expert recommendations for the ASPECTJ and ECLIPSE projects. We find that both usage and implementation expertise have comparable levels of accuracy, which suggests that usage expertise may be used as a substitute measure. We also find a notable overlap of method calls across both projects, which suggests that usage expertise can be leveraged to recommend experts from different projects and thus for projects with little or no history.
Improving code review by predicting reviewers and acceptance of patches
Code reviews are an important part of software development because they help to increase code quality and reliability. For this paper, we observed the review processes of two open-source projects, Firefox and Mozilla Core. We noticed that code reviews are mostly organized manually. In particular, finding appropriate reviewers is a complex and time-consuming task and, surprisingly, impacts the review outcome: review requests without an initial reviewer assignment have lower chances to be accepted (and take longer). Based on our observations we propose two improvements: (1) predict whether a given patch is acceptable and (2) suggest reviewers for a patch. We implemented and tested both approaches for the Firefox and Mozilla Core projects. In our experiments, the prediction accuracy was 73% for the the review outcome and 51–80% for the reviewer recommendation. The values for accuracy are higher than those of comparable approaches and are high enough to be useful and applicable in practice.
Improving bug triage with bug tossing graphs
bug report is typically assigned to a single developer who is then responsible for fixing the bug. In Mozilla and Eclipse, between 37%-44% of bug reports are "tossed" (reassigned) to other developers, for example because the bug has been assigned by accident or another developer with additional expertise is needed. In any case, tossing increases the time-to-correction for a bug. In this paper, we introduce a graph model based on Markov chains, which captures bug tossing history. This model has several desirable qualities. First, it reveals developer networks which can be used to discover team structures and to find suitable experts for a new task. Second, it helps to better assign developers to bug reports. In our experiments with 445,000 bug reports, our model reduced tossing events, by up to 72%. In addition, the model increased the prediction accuracy by up to 23 percentage points compared to traditional bug triaging approaches.
Cross-project defect prediction: a large scale experiment on data vs. domain vs. process
Prediction of software defects works well within projects as long as there is a sufficient amount of data available to train any models. However, this is rarely the case for new software projects and for many companies. So far, only a few have studies focused on transferring prediction models from one project to another. In this paper, we study cross-project defect prediction models on a large scale. For 12 real-world applications, we ran 622 cross-project predictions. Our results indicate that cross-project prediction is a serious challenge, i.e., simply using models from projects in the same domain or with the same process does not lead to accurate predictions. To help software engineers choose models wisely, we identified factors that do influence the success of cross-project predictions. We also derived decision trees that can provide early estimates for precision, recall, and accuracy before a prediction is attempted.
The Beauty and the Beast: Vulnerabilities in Red Hat's Packages.
In an empirical study of 3241 Red Hat packages, we show that software vulnerabilities correlate with dependencies between packages. With formal concept analysis and statistical hypothesis testing, we identify dependencies that decrease the risk of vulnerabilities (“beauties”) or increase the risk (“beasts”). Using support vector machines on dependency data, our prediction models successfully and consistently catch about two thirds of vulnerable packages (median recall of 0.65). When our models predict a package as vulnerable, it is correct more than eight times out of ten (median precision of 0.83). Our findings help developers to choose new dependencies wisely and make them aware of risky dependencies.
Predicting defects in SAP Java code: An experience report
Which components of a large software system are the most defect-prone? In a study on a large SAP Java system, we evaluated and compared a number of defect predictors, based on code features such as complexity metrics, static error detectors, change frequency, or component imports, thus replicating a number of earlier case studies in an industrial context. We found the overall predictive power to be lower than expected; still, the resulting regression models successfully predicted 50–60% of the 20% most defect-prone components.
Improving bug tracking systems
It is important that information provided in bug reports is relevant and complete in order to help resolve bugs quickly. However, often such information trickles to developers after several iterations of communication between developers and reporters. Poorly designed bug tracking systems are partly to blame for this exchange of information being stretched over time. Our paper addresses the concerns of bug tracking systems by proposing four broad directions for enhancements. As a proof-of-concept, we also demonstrate a prototype interactive bug tracking system that gathers relevant information from the user and identifies files that need to be fixed to resolve the bug.
Frequently asked questions in bug reports
Bug tracking systems play a central role in software development since they allow users and developers to submit and discuss bugs and new features. To better understand information and communication needs in bug tracking, we analysed what questions are asked in bug reports. We sampled 600 bug reports from the MOZILLA and ECLIPSE projects and located 947 questions in the reports. Next, we used an open card sort and identified eight categories of questions, which can further be broken down into forty groups. We show the value of this catalogue of frequently asked questions with a large quantitative and qualitative study on when questions are asked and how they are answered. A consequence of our results is that constant user involvement is crucial for successful bug reports and that better tools are needed to support this.
Mining Software Archives introduction
Corporations invest more than 300 billion US dollars annually in software production. Although new people are constantly entering the fi eld, some of them aren’t suffi ciently trained and therefore aren’t prepared to draw on the experience others have accumulated. This creates a situation in which every problem is perceived as new and unique, even though there’s plenty of experience to learn from. Studying and collecting such experience is the goal of empirical software engineering, and its evidence fi nds its way into textbooks and magazines. Empirical studies tell us, for example, that the later a problem is discovered, the more effort it takes to fi x it, and that 80 percent of the defects come from 20 percent of the code. Such fi ndings have long been common knowledge, but the consequences are very unspecifi c. How do we know where the most effort is spent? How do we know where the defects are? Which properties of the software or its development contribute to effort and quality? And, most important, how do we know whether some empirical or textbook fi nding applies to the project at hand? To answer such questions, we need data—about the product, people, and process. However, collecting such data manually is expensive and can interfere with the development process and cost valuable developer time. If the data is collected from humans (for example, in surveys), there’s a risk of bias, which we must estimate and deal with. Interpreting the data (agai
Proceedings of the 3rd Workshop on FAMIX and MOOSE in Software Reengineering (FAMOOSr'09)
The goal of the FAMOOSr workshop is to strengthen the community of researchers and practitioners who are working in re- and reverse engineering, by providing a forum for building future research using Moose and FAMIX as shared infrastructure. Research should be collaborative and supported by tools. The increasing amount of data available about software systems poses new challenges for reengineering research, as the proposed approaches need to scale. In this context, concerns about meta-modeling and analysis techniques need to be augmented by technical concerns about how to reuse and how to build upon the efforts of previous research. That is why Moose is an open-source software for researchers to build and share their analysis, meta-models, and data. Both FAMIX and Moose started in the context of FAMOOS, a European research project on object-oriented frameworks. Back in 1997 Moose was as a simple implementation of the FAMIX meta-model, which was a language independent meta-model for object-oriented systems. However over the past decade, Moose has been used in a growing number of research projects and has evolved to be a generic environment for various reverse and reengineering activities. In the same time, FAMIX was extended to support emerging research interest such as dynamic analysis, evolution analysis, identifier analysis, bug tracking analysis, or visualization. Recent work includes analysis of software architecture and semantic annotations. Currently, several research groups are using Moose as a platform, or FAMIX as a meta-model, and other groups announced interest in using them in the future.
Guest Editors' Introduction: Mining Software Archives
Modern programming environments automatically collect lots of data on software development, notably changes and defects. The field of mining software archives is concerned with the automated extraction, collection, and abstraction of information from this data. This is the introduction to a special issue of IEEE Software presenting a selection of the exciting research that is taking place in the field.
What makes a good bug report?
In software development, bug reports provide crucial information to developers. However, these reports widely differ in their quality. We conducted a survey among developers and users of APACHE, ECLIPSE, and MOZILLA to find out what makes a good bug report. The analysis of the 466 responses revealed an information mismatch between what developers need and what users supply. Most developers consider steps to reproduce, stack traces, and test cases as helpful, which are at the same time most difficult to provide for users. Such insight is helpful to design new bug tracking tools that guide users at collecting and providing more helpful information. Our CUEZILLA prototype is such a tool and measures the quality of new bug reports; it also recommends which elements should be added to improve the quality. We trained CUEZILLA on a sample of 289 bug reports, rated by developers as part of the survey. In our experiments, CUEZILLA was able to predict the quality of 31--48% of bug reports accurately.
Duplicate bug reports considered harmful… really?
In a survey we found that most developers have experienced duplicated bug reports, however, only few considered them as a serious problem. This contradicts popular wisdom that considers bug duplicates as a serious problem for open source projects. In the survey, developers also pointed out that the additional information provided by duplicates helps to resolve bugs quicker. In this paper, we therefore propose to merge bug duplicates, rather than treating them separately. We quantify the amount of information that is added for developers and show that automatic triaging can be improved as well. In addition, we discuss the different reasons why users submit duplicate bug reports in the first place.
Towards the next generation of bug tracking systems
Developers typically rely on the information submitted by end-users to resolve bugs. We conducted a survey on information needs and commonly faced problems with bug reporting among several hundred developers and users of the APACHE, ECLIPSE and MOZILLA projects. In this paper, we present the results of a card sort on the 175 comments sent back to us by the responders of the survey. The card sort revealed several hurdles involved in reporting and resolving bugs, which we present in a collection of recommendations for the design of new bug tracking systems. Such systems could provide contextual assistance, reminders to add information, and most important, assistance to collect and report crucial information to developers.
DEFECTS 2008: international workshop on defects in large software systems
Bugs are everywhere in today's software and because of the huge economic damage they are actively studied. The goal of this one-day workshop is to connect the different research communities with each other and with industry.
Explaining Product Release Planning Results Using Concept Analysis.
Objective: This paper aims to generate explanations from a series of data points obtained from a decision support system called ReleasePlanner® for supporting product release planning and considered to be a black box. Method: Concept analysis is applied to 1085 data points received from running 10 scenarios of a real world product release planning project with 35 candidate solutions generated by ReleasePlanner®. Results: Three main results are obtained: (1) patterns between inputs and outputs; (2) impact of individual input parameters on outputs; and (3) sensitivity level of outputs in dependence of inputs. Conclusion: Concept analysis is shown to be a feasible technique for gaining more insight into the structure of results obtained from a black box input-outp
Do crosscutting concerns cause defects?
There is a growing consensus that crosscutting concerns harm code quality. An example of a crosscutting concern is a functional requirement whose implementation is distributed across multiple software modules. We asked the question, "How much does the amount that a concern is crosscutting affect the number of defects in a program?" We conducted three extensive case studies to help answer this question. All three studies revealed a moderate to strong statistically significant correlation between the degree of scattering and the number of defects. This paper describes the experimental framework we developed to conduct the studies, the metrics we adopted and developed to measure the degree of scattering, the studies we performed, the efforts we undertook to remove experimental and other biases, and the results we obtained. In the process, we have formulated a theory that explains why increased scattering might lead to increased defects.
Predicting software metrics at design time
How do problem domains impact software features? We mine software code bases to relate problem domains (characterized by imports) to code features such as complexity, size, or quality. The resulting predictors take the specific imports of a component and predict its size, complexity, and quality metrics. In an experiment involving 89 plug-ins of the ECLIPSE project, we found good prediction accuracy for most metrics. Since the predictors rely only on import relationships, and since these are available at design time, our approach allows for early estimation of crucial software metrics.
Mining usage expertise from version archives
In software development, there is an increasing need to find and connect developers with relevant expertise. Existing expertise recommendation systems are mostly based on variations of the Line 10 Rule: developers who changed a file most often have the most implementation expertise. In this paper, we introduce the concept of usage expertise, which manifests itself whenever developers are using functionality, e.g., by calling API methods. We present preliminary results for the ECLIPSE project that demonstrate that our technique allows to recommend experts for files with no or little history, identify developers with similar expertise, and measure the usage of API methods.
Extracting structural information from bug reports
traces, source code, and patches. Neglecting such structural elements is a loss of valuable information; structure usually leads to a better performance of machine learning approaches. In this paper, we present a tool called infoZilla that detects structural elements from bug reports with near perfect accuracy and allows us to extract them. We anticipate that infoZilla can be used to leverage data from bug reports at a different granularity level that can facilitate interesting research in the future.
Predicting defects using network analysis on dependency graphs
In software development, resources for quality assurance are limited by time and by cost. In order to allocate resources effectively, managers need to rely on their experience backed by code complexity metrics. But often dependencies exist between various pieces of code over which managers may have little knowledge. These dependencies can be construed as a low level graph of the entire system. In this paper, we propose to use network analysis on these dependency graphs. This allows managers to identify central program units that are more likely to face defects. In our evaluation on Windows Server 2003, we found that the recall for models built from network measures is by 10% points higher than for models built from complexity metrics. In addition, network measures could identify 60% of the binaries that the Windows developers considered as critical-twice as many as identified by complexity metrics.
Predicting Defects in SAP Products: A Replicated Study
Given a large body of code, how do we know where to focus our quality assurance effort? By mining the software’s defect history, we can automatically learn which code features correlated with defects in the past—and leverage these correlations for new predictions: “In the past, high inheritance depth was an indicator of a high number of defects. Since this new component also has a high inheritance depth, let us test it thoroughly”. Such history-based approaches work best if the new component is similar to the components learned from. But how does learning from history perform for projects with high variability between components? We ran a study on two SAP products involving a wide spectrum of functionality. We found that learning and predicting was accurate at package level, but not at product level. These results suggest that to learn from past defects, one should separate the product into component clusters with similar functionality, and make separate predictions for each cluster. Our initial approaches to form such clusters automatically, based on similarity of metrics, showed promising accuracy.
Predicting bugs from history
Given a large body of code, how do we know where to focus our quality assurance effort? By mining the software’s defect history, we can automatically learn which code features correlated with defects in the past—and leverage these correlations for new predictions: “In the past, high inheritance depth meant high number of defects. Since this new component also has a high inheritance depth, let us test it thoroughly”. Such history-based approaches work best if the new component is similar to the components learned from. But how does learning from history perform for projects with high variability between components? We ran a study on two SAP products involving a wide spectrum of functionality. We found that learning and predicting was accurate at package level, but not at product level. These results suggest that to learn from past defects, one should separate the product into component clusters with similar functionality, and make separate predictions for each cluster. Initial approaches to form such clusters automatically, based on similarity of metrics, showed promising accuracy
